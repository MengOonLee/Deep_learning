{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXg03MZZDABsesCh1+6iVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/PyTorch/Transformer/TabTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install -qU torch lightning"
      ],
      "metadata": {
        "id": "RGebV16U7pHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import lightning as L\n",
        "\n",
        "class TabularDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X_num, X_cat, y):\n",
        "        self.X_num = torch.tensor(data=X_num, dtype=torch.float32)\n",
        "        self.X_cat = torch.tensor(data=X_cat, dtype=torch.long)\n",
        "        self.y = torch.tensor(data=y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
        "\n",
        "class CensusDataModule(L.LightningDataModule):\n",
        "    def __init__(self, train_url, test_url, batch_size=256):\n",
        "        super().__init__()\n",
        "        self.train_url = train_url\n",
        "        self.test_url = test_url\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def _load_data(self, df):\n",
        "        df = df.copy().drop(columns=['fnlwgt', 'education_num']).dropna()\n",
        "        df['income_bracket'] = df['income_bracket'].astype(str)\\\n",
        "            .str.replace('.', '').str.strip()\n",
        "        y = df['income_bracket'].map({'<=50K': 0, '>50K': 1}).astype('float32')\n",
        "\n",
        "        NUMERIC_FEATURES = ['capital_gain', 'capital_loss', 'hours_per_week']\n",
        "        X_num = df[NUMERIC_FEATURES].astype('float32')\n",
        "        X_cat = df.drop(columns=NUMERIC_FEATURES + ['income_bracket'])\\\n",
        "            .astype(str).apply(lambda s: s.str.strip())\n",
        "        return X_num.values, X_cat, y.values\n",
        "\n",
        "    def _build_category_maps(self, train_cat):\n",
        "        maps = {}\n",
        "        for col in train_cat.columns:\n",
        "            unique_vals = sorted(train_cat[col].unique())\n",
        "            maps[col] = {val: i+1 for i, val in enumerate(unique_vals)}\n",
        "            maps[col]['__UNK__'] = 0\n",
        "        return maps\n",
        "\n",
        "    def _apply_category_maps(self, X_cat, maps):\n",
        "        X_cat = X_cat.copy().apply(\n",
        "            lambda c: c.map(maps[c.name]).fillna(0).astype(int))\n",
        "        return X_cat.values\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        CSV_HEADERS = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "            'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "            'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "            'income_bracket']\n",
        "        df_train = pd.read_csv(self.train_url, header=None,\n",
        "            names=CSV_HEADERS)\n",
        "        df_test = pd.read_csv(self.test_url, header=None,\n",
        "            names=CSV_HEADERS, skiprows=1)\n",
        "\n",
        "        X_num_train, X_cat_train, y_train = self._load_data(df=df_train)\n",
        "        X_num_test, X_cat_test, y_test = self._load_data(df=df_test)\n",
        "\n",
        "        cat_maps = self._build_category_maps(train_cat=X_cat_train)\n",
        "        X_cat_train = self._apply_category_maps(X_cat=X_cat_train, maps=cat_maps)\n",
        "        X_cat_test = self._apply_category_maps(X_cat=X_cat_test, maps=cat_maps)\n",
        "\n",
        "        self.ds_train = TabularDataset(X_num=X_num_train, X_cat=X_cat_train, y=y_train)\n",
        "        ds_test = TabularDataset(X_num=X_num_test, X_cat=X_cat_test, y=y_test)\n",
        "        self.ds_val, self.ds_test = torch.utils.data.random_split(dataset=ds_test,\n",
        "            lengths=[0.5, 0.5], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(dataset=self.ds_train,\n",
        "            batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(dataset=self.ds_val,\n",
        "            batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(dataset=self.ds_test,\n",
        "            batch_size=self.batch_size)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "    test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "    dm = CensusDataModule(train_url=train_url, test_url=test_url)\n",
        "    dm.setup()"
      ],
      "metadata": {
        "id": "bwyBuSuUBzy2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import lightning as L\n",
        "\n",
        "\n",
        "\n",
        "ds_train = TabularDataset(X_num=X_num_train, X_cat=X_cat_train, y=y_train)\n",
        "ds_test = TabularDataset(X_num=X_num_test, X_cat=X_cat_test, y=y_test)\n",
        "\n",
        "dl_train =\n",
        "dl_test = torch.utils.data.DataLoader(dataset=ds_test, batch_size=256)"
      ],
      "metadata": {
        "id": "QPFSPKBFXEp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Normalization(torch.nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.register_buffer('mean', torch.tensor(data=mean, dtype=torch.float32))\n",
        "        self.register_buffer('std', torch.tensor(data=std, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.mean) / self.std"
      ],
      "metadata": {
        "id": "Fc0EfoIhSMOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "mean, std = X_num_train.mean(axis=0), X_num_train.std(axis=0)\n",
        "num_dim = X_num_train.shape[1]\n",
        "cat_cardinalities =  X_cat_train.max(axis=0) + 1\n",
        "\n",
        "class ClassifyModel(torch.nn.Module):\n",
        "    def __init__(self, cat_cardinalities):\n",
        "        super().__init__()\n",
        "        self.normalizer = Normalization(mean=mean, std=std)\n",
        "        self.emb_layers = torch.nn.ModuleList(modules=[\n",
        "            torch.nn.Embedding(num_embeddings=c, embedding_dim=min(4, (c+1)//2))\n",
        "            for c in cat_cardinalities\n",
        "        ])\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        x_num = self.normalizer(x_num)\n",
        "        x_emb = [emb(x_cat[:, i]) for i, emb in enumerate(self.emb_layers)]\n",
        "\n",
        "\n",
        "        return x_num, x_emb\n",
        "\n",
        "model = ClassifyModel(cat_cardinalities=cat_cardinalities)\n",
        "x_num = torch.tensor(X_num_train[:10], dtype=torch.float32)\n",
        "x_emb = torch.tensor(X_cat_train[:10], dtype=torch.long)\n",
        "model(x_num, x_emb)"
      ],
      "metadata": {
        "id": "X1IPpBcxrycm",
        "outputId": "00854444-68f9-4cf9-8569-849f1223975d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1485, -0.2167, -0.0354],\n",
              "         [-0.1459, -0.2167, -2.2222],\n",
              "         [-0.1459, -0.2167, -0.0354],\n",
              "         [-0.1459, -0.2167, -0.0354],\n",
              "         [-0.1459, -0.2167, -0.0354],\n",
              "         [-0.1459, -0.2167, -0.0354],\n",
              "         [-0.1459, -0.2167, -1.9792],\n",
              "         [-0.1459, -0.2167,  0.3695],\n",
              "         [ 1.7611, -0.2167,  0.7745],\n",
              "         [ 0.5552, -0.2167, -0.0354]]),\n",
              " [tensor([[ 0.1893,  0.0246,  1.2913,  0.1899],\n",
              "          [-1.2577,  0.3759, -1.3739, -0.6427],\n",
              "          [-0.6679,  1.1394,  0.0518, -1.0582],\n",
              "          [ 0.7615, -0.6128,  0.5224, -1.1209],\n",
              "          [ 1.9621,  0.8056, -0.0843, -0.5087],\n",
              "          [-0.9374,  0.6213, -1.7214, -1.5217],\n",
              "          [-0.4241,  0.7210, -1.9187, -0.0584],\n",
              "          [ 1.8087, -0.4617,  1.1644,  0.9504],\n",
              "          [ 0.6953, -0.0394, -1.3335, -0.9625],\n",
              "          [ 0.0104, -0.0883,  1.0266, -0.0743]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[-1.7141,  0.7404,  0.4529, -0.0996],\n",
              "          [ 0.6384,  0.9471, -0.2653,  0.5266],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120],\n",
              "          [ 0.6384,  0.9471, -0.2653,  0.5266],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120],\n",
              "          [-1.5983,  0.7023, -1.2764,  0.3120]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[ 0.7516,  0.8146, -1.2132, -0.0243],\n",
              "          [ 0.7516,  0.8146, -1.2132, -0.0243],\n",
              "          [-0.3399,  0.2527,  0.9661,  0.0612],\n",
              "          [-0.0878,  0.7106,  1.2921,  0.4479],\n",
              "          [ 0.7516,  0.8146, -1.2132, -0.0243],\n",
              "          [ 0.7231,  0.7592,  0.1721,  0.2389],\n",
              "          [ 0.1731,  0.6606,  0.1970, -0.3833],\n",
              "          [-0.3399,  0.2527,  0.9661,  0.0612],\n",
              "          [ 0.7231,  0.7592,  0.1721,  0.2389],\n",
              "          [ 0.7516,  0.8146, -1.2132, -0.0243]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[-0.6626, -0.8042, -0.1772, -0.4585],\n",
              "          [ 1.8152, -0.0560, -0.2685,  0.0868],\n",
              "          [ 0.7107,  1.2827,  0.9700,  0.9968],\n",
              "          [ 1.8152, -0.0560, -0.2685,  0.0868],\n",
              "          [ 1.8152, -0.0560, -0.2685,  0.0868],\n",
              "          [ 1.8152, -0.0560, -0.2685,  0.0868],\n",
              "          [ 0.8770,  0.9855, -0.0359, -0.1424],\n",
              "          [ 1.8152, -0.0560, -0.2685,  0.0868],\n",
              "          [-0.6626, -0.8042, -0.1772, -0.4585],\n",
              "          [ 1.8152, -0.0560, -0.2685,  0.0868]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[-1.1788,  0.2663, -0.2279,  2.0245],\n",
              "          [ 1.4978, -0.3601, -0.9468,  2.0652],\n",
              "          [-0.3702, -0.3012, -0.1002, -0.5669],\n",
              "          [-0.3702, -0.3012, -0.1002, -0.5669],\n",
              "          [ 0.8771, -0.7251,  0.8638,  0.5335],\n",
              "          [ 1.4978, -0.3601, -0.9468,  2.0652],\n",
              "          [-0.1263,  0.5765, -0.4419, -0.5225],\n",
              "          [ 1.4978, -0.3601, -0.9468,  2.0652],\n",
              "          [ 0.8771, -0.7251,  0.8638,  0.5335],\n",
              "          [ 1.4978, -0.3601, -0.9468,  2.0652]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[-0.6152,  1.2001, -0.6400,  1.0422],\n",
              "          [-0.1725, -1.3652, -0.3236, -0.2830],\n",
              "          [-0.6152,  1.2001, -0.6400,  1.0422],\n",
              "          [-0.1725, -1.3652, -0.3236, -0.2830],\n",
              "          [ 0.1722,  0.8745,  0.1828,  0.1210],\n",
              "          [ 0.1722,  0.8745,  0.1828,  0.1210],\n",
              "          [-0.6152,  1.2001, -0.6400,  1.0422],\n",
              "          [-0.1725, -1.3652, -0.3236, -0.2830],\n",
              "          [-0.6152,  1.2001, -0.6400,  1.0422],\n",
              "          [-0.1725, -1.3652, -0.3236, -0.2830]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[-0.2264, -1.7589,  1.9392],\n",
              "          [-0.2264, -1.7589,  1.9392],\n",
              "          [-0.2264, -1.7589,  1.9392],\n",
              "          [-2.0507, -1.4597, -2.3467],\n",
              "          [-2.0507, -1.4597, -2.3467],\n",
              "          [-0.2264, -1.7589,  1.9392],\n",
              "          [-2.0507, -1.4597, -2.3467],\n",
              "          [-0.2264, -1.7589,  1.9392],\n",
              "          [-0.2264, -1.7589,  1.9392],\n",
              "          [-0.2264, -1.7589,  1.9392]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[ 1.4652,  0.5330],\n",
              "          [ 1.4652,  0.5330],\n",
              "          [ 1.4652,  0.5330],\n",
              "          [ 1.4652,  0.5330],\n",
              "          [ 0.2676, -1.5735],\n",
              "          [ 0.2676, -1.5735],\n",
              "          [ 0.2676, -1.5735],\n",
              "          [ 1.4652,  0.5330],\n",
              "          [ 0.2676, -1.5735],\n",
              "          [ 1.4652,  0.5330]], grad_fn=<EmbeddingBackward0>),\n",
              "  tensor([[-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [ 0.5571,  0.2387, -1.7547, -1.1437],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [ 0.6189,  1.3464, -0.5349, -1.6492],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063],\n",
              "          [-0.1474,  1.5507,  0.2781, -0.1063]], grad_fn=<EmbeddingBackward0>)])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVKlnMTa50Xp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}