{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/PyTorch/Transformer/TabTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "rrU2XSfZoRM1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_HEADERS = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
        "    'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss',\n",
        "    'hours_per_week', 'native_country', 'income_bracket']\n",
        "\n",
        "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "df_train = pd.read_csv(train_url, header=None, names=CSV_HEADERS)\n",
        "\n",
        "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "df_test = pd.read_csv(test_url, header=None, names=CSV_HEADERS, skiprows=1)\n",
        "\n",
        "def load_data(df):\n",
        "    df = df.drop(columns=['fnlwgt', 'education_num']).reset_index(drop=True)\n",
        "\n",
        "    numeric_cols = ['capital_gain', 'capital_loss', 'hours_per_week']\n",
        "    X_num = df[numeric_cols].astype('float32')\n",
        "\n",
        "    categoric_cols = [c for c in df.columns if c not in numeric_cols + ['income_bracket']]\n",
        "    X_cat = df[categoric_cols].astype(str).apply(lambda s: s.str.strip())\n",
        "\n",
        "    y = df['income_bracket'].str.replace('.', '', regex=False).str.strip()\n",
        "\n",
        "    return X_num, X_cat, y\n",
        "\n",
        "X_num_train, X_cat_train, y_train = load_data(df=df_train)\n",
        "X_num_test, X_cat_test, y_test = load_data(df=df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DYgDqAoEoRM7"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "y_lb = preprocessing.LabelBinarizer()\n",
        "y_train = y_lb.fit_transform(y=y_train).astype(int).squeeze()\n",
        "y_test = y_lb.transform(y=y_test).astype(int).squeeze()\n",
        "\n",
        "num_scaler = preprocessing.StandardScaler()\n",
        "X_num_train = num_scaler.fit_transform(X=X_num_train).astype('float32')\n",
        "X_num_test = num_scaler.transform(X=X_num_test).astype('float32')\n",
        "num_features = X_num_train.shape[1]\n",
        "\n",
        "cat_encoder = preprocessing.OrdinalEncoder(unknown_value=-1,\n",
        "    handle_unknown='use_encoded_value')\n",
        "X_cat_train = cat_encoder.fit_transform(X=X_cat_train).astype(int) + 1\n",
        "X_cat_test = cat_encoder.transform(X=X_cat_test).astype(int) + 1\n",
        "cat_cardinalities = [len(c)+1 for c in cat_encoder.categories_]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HxY10ufoRM9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(seed=42)\n",
        "\n",
        "class CensusDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X_num, X_cat, y):\n",
        "        self.X_num = torch.tensor(data=X_num, dtype=torch.float32)\n",
        "        self.X_cat = torch.tensor(data=X_cat, dtype=torch.long)\n",
        "        self.y = torch.tensor(data=y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
        "\n",
        "ds_temp = CensusDataset(X_num=X_num_train, X_cat=X_cat_train, y=y_train)\n",
        "ds_train, ds_val = torch.utils.data.random_split(dataset=ds_temp, lengths=[0.9, 0.1],\n",
        "    generator=torch.Generator().manual_seed(42))\n",
        "dl_train = torch.utils.data.DataLoader(dataset=ds_train, batch_size=256, shuffle=True)\n",
        "dl_val = torch.utils.data.DataLoader(dataset=ds_val, batch_size=256, shuffle=False)\n",
        "\n",
        "ds_test = CensusDataset(X_num=X_num_test, X_cat=X_cat_test, y=y_test)\n",
        "dl_test = torch.utils.data.DataLoader(dataset=ds_test, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GEmrkPQoRNA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(seed=42)\n",
        "\n",
        "class CensusClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_features, cat_cardinalities):\n",
        "        super().__init__()\n",
        "        self.embedding_layers = torch.nn.ModuleList(modules=[\n",
        "            torch.nn.Embedding(num_embeddings=c,\n",
        "                embedding_dim=int(min(8, max(1, round(c**0.25)))),\n",
        "                padding_idx=0)\n",
        "            for c in cat_cardinalities])\n",
        "\n",
        "        in_features = num_features + sum(e.embedding_dim for e in self.embedding_layers)\n",
        "\n",
        "        layers = []\n",
        "        for h in [64, 32]:\n",
        "            layers.append(torch.nn.Linear(in_features=in_features, out_features=h))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "            layers.append(torch.nn.Dropout(p=0.3))\n",
        "            in_features = h\n",
        "        layers.append(torch.nn.Linear(in_features, 2))\n",
        "        self.fc = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, X_num, X_cat):\n",
        "        X_emb = [emb(X_cat[:, i]) for i, emb in enumerate(self.embedding_layers)]\n",
        "        X_emb = torch.cat(tensors=X_emb, dim=1)\n",
        "        X = torch.cat(tensors=[X_num, X_emb], dim=1)\n",
        "        return self.fc(X)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CensusClassifier(num_features=num_features, cat_cardinalities=cat_cardinalities)\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print('Total parameters:', total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Gqazt3JeoRNB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(seed=42)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss, train_acc = 0.0, 0.0\n",
        "    for X_num, X_cat, y in dataloader:\n",
        "        X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
        "\n",
        "        y_pred = model(X_num=X_num, X_cat=X_cat)\n",
        "        loss = loss_fn(input=y_pred, target=y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (y_pred.argmax(dim=-1)==y).sum().item()\n",
        "    train_loss /= len(dataloader)\n",
        "    train_acc /= size\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, test_acc = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_num, X_cat, y in dataloader:\n",
        "            X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
        "            y_pred = model(X_num=X_num, X_cat=X_cat)\n",
        "            test_loss += loss_fn(input=y_pred, target=y).item()\n",
        "            test_acc += (y_pred.argmax(dim=-1)==y).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    test_acc /= size\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhcaYuE3oRND"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "torch.manual_seed(seed=42)\n",
        "\n",
        "def train_model(model, dl_train, dl_val, loss_fn, optimizer,\n",
        "        epochs, patience, save_path):\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min',\n",
        "        factor=0.5, patience=patience)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc':[],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train(dataloader=dl_train, model=model, loss_fn=loss_fn,\n",
        "            optimizer=optimizer)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = test(dataloader=dl_val, model=model, loss_fn=loss_fn)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(\"Epoch: %d, LR: %.3f, val_acc: %.3f\"%(\n",
        "            epoch, optimizer.param_groups[0]['lr'], val_acc))\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    model.load_state_dict(torch.load(save_path, weights_only=True))\n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model=model, dl_train=dl_train, dl_val=dl_val, loss_fn=loss_fn,\n",
        "    optimizer=optimizer, epochs=10, patience=3, save_path='census_classifier_best.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvkmAEcfoRNF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}