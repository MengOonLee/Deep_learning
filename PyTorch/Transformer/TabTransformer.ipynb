{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_HEADERS = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
    "    'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss',\n",
    "    'hours_per_week', 'native_country', 'income_bracket']\n",
    "\n",
    "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "df_train = pd.read_csv(train_url, header=None, names=CSV_HEADERS)\n",
    "\n",
    "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "df_test = pd.read_csv(test_url, header=None, names=CSV_HEADERS, skiprows=1)\n",
    "\n",
    "def load_data(df):\n",
    "    df = df.drop(columns=['fnlwgt', 'education_num']).reset_index(drop=True)\n",
    "    \n",
    "    numeric_cols = ['capital_gain', 'capital_loss', 'hours_per_week']\n",
    "    X_num = df[numeric_cols].astype('float32')\n",
    "\n",
    "    categoric_cols = [c for c in df.columns if c not in numeric_cols + ['income_bracket']]\n",
    "    X_cat = df[categoric_cols].astype(str).apply(lambda s: s.str.strip())\n",
    "    \n",
    "    y = df['income_bracket'].str.replace('.', '', regex=False).str.strip()\n",
    "    \n",
    "    return X_num, X_cat, y\n",
    "\n",
    "X_num_train, X_cat_train, y_train = load_data(df=df_train)\n",
    "X_num_test, X_cat_test, y_test = load_data(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "y_lb = preprocessing.LabelBinarizer()\n",
    "y_train = y_lb.fit_transform(y=y_train).astype(int).squeeze()\n",
    "y_test = y_lb.transform(y=y_test).astype(int).squeeze()\n",
    "\n",
    "num_scaler = preprocessing.StandardScaler()\n",
    "X_num_train = num_scaler.fit_transform(X=X_num_train).astype('float32')\n",
    "X_num_test = num_scaler.transform(X=X_num_test).astype('float32')\n",
    "num_features = X_num_train.shape[1]\n",
    "\n",
    "cat_encoder = preprocessing.OrdinalEncoder(unknown_value=-1,\n",
    "    handle_unknown='use_encoded_value')\n",
    "X_cat_train = cat_encoder.fit_transform(X=X_cat_train).astype(int) + 1\n",
    "X_cat_test = cat_encoder.transform(X=X_cat_test).astype(int) + 1\n",
    "cat_cardinalities = [len(c)+1 for c in cat_encoder.categories_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(seed=42)\n",
    "\n",
    "class CensusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_num, X_cat, y):\n",
    "        self.X_num = torch.tensor(data=X_num, dtype=torch.float32)\n",
    "        self.X_cat = torch.tensor(data=X_cat, dtype=torch.long)\n",
    "        self.y = torch.tensor(data=y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "\n",
    "ds_temp = CensusDataset(X_num=X_num_train, X_cat=X_cat_train, y=y_train)\n",
    "ds_train, ds_val = torch.utils.data.random_split(dataset=ds_temp, lengths=[0.9, 0.1],\n",
    "    generator=torch.Generator().manual_seed(42))\n",
    "dl_train = torch.utils.data.DataLoader(dataset=ds_train, batch_size=256, shuffle=True)\n",
    "dl_val = torch.utils.data.DataLoader(dataset=ds_val, batch_size=256, shuffle=False)\n",
    "\n",
    "ds_test = CensusDataset(X_num=X_num_test, X_cat=X_cat_test, y=y_test)\n",
    "dl_test = torch.utils.data.DataLoader(dataset=ds_test, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(seed=42)\n",
    "\n",
    "class CensusClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_features, cat_cardinalities):\n",
    "        super().__init__()\n",
    "        self.embedding_layers = torch.nn.ModuleList(modules=[\n",
    "            torch.nn.Embedding(num_embeddings=c,\n",
    "                embedding_dim=int(min(8, max(1, round(c**0.25)))),\n",
    "                padding_idx=0)\n",
    "            for c in cat_cardinalities])\n",
    "        \n",
    "        in_features = num_features + sum(e.embedding_dim for e in self.embedding_layers)\n",
    "\n",
    "        layers = []\n",
    "        for h in [64, 32]:\n",
    "            layers.append(torch.nn.Linear(in_features=in_features, out_features=h))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=0.3))\n",
    "            in_features = h\n",
    "        layers.append(torch.nn.Linear(in_features, 2))\n",
    "        self.fc = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X_num, X_cat):\n",
    "        X_emb = [emb(X_cat[:, i]) for i, emb in enumerate(self.embedding_layers)]\n",
    "        X_emb = torch.cat(tensors=X_emb, dim=1)\n",
    "        X = torch.cat(tensors=[X_num, X_emb], dim=1)\n",
    "        return self.fc(X)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CensusClassifier(num_features=num_features, cat_cardinalities=cat_cardinalities)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total parameters:', total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(seed=42)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for X_num, X_cat, y in dataloader:\n",
    "        X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X_num=X_num, X_cat=X_cat)\n",
    "        loss = loss_fn(input=y_pred, target=y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (y_pred.argmax(dim=-1)==y).sum().item()\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= size\n",
    "    return train_loss, train_acc\n",
    "        \n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, test_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_num, X_cat, y in dataloader:\n",
    "            X_num, X_cat, y = X_num.to(device), X_cat.to(device), y.to(device)\n",
    "            y_pred = model(X_num=X_num, X_cat=X_cat)\n",
    "            test_loss += loss_fn(input=y_pred, target=y).item()\n",
    "            test_acc += (y_pred.argmax(dim=-1)==y).sum().item()\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= size\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.6501721750135007, 0.62780783539233, 0.6113847535589467],\n",
       " 'train_acc': [0.7253028493431155, 0.7556048455894898, 0.7591537280327589],\n",
       " 'val_loss': [0.636003512602586, 0.6175819589541509, 0.6030017871123093],\n",
       " 'val_acc': [0.754914004914005, 0.754914004914005, 0.754914004914005]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc':[],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "for t in range(3):\n",
    "    train_loss, train_acc = train(dataloader=dl_train, model=model, loss_fn=loss_fn,\n",
    "        optimizer=optimizer)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    \n",
    "    val_loss, val_acc = test(dataloader=dl_val, model=model, loss_fn=loss_fn)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbMvq4d9pTIow2y0AY9YRq",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
