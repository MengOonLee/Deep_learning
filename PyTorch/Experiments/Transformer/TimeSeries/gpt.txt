# ===========================================================
# 1. Generate synthetic data (same as your original)
# ===========================================================
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(42)

def trend(time):
    slope = np.zeros_like(time)
    slope[time < 60] = 0.2
    slope[(time>=60) & (time<96)] = -0.4
    slope[time>=96] = 1.0
    return slope * time

def seasonal_pattern(x):
    return np.where(x < 0.4, np.cos(2*np.pi*x), 1/np.exp(3*x))

def seasonality(time, period, amplitude):
    x = (time % period) / period
    return amplitude * seasonal_pattern(x)

def noise(time, level=5):
    return np.random.randn(len(time)) * level

period = 12 ; amplitude = 40
time_hist = np.arange(10*12)
ts_hist = 10 + trend(time_hist) + seasonality(time_hist, 12, amplitude) + noise(time_hist)

time_true = np.arange(15*12)
ts_true = 10 + trend(time_true) + seasonality(time_true, 12, amplitude) + noise(time_true)


# ===========================================================
# 2. Dataset: sliding windows
# ===========================================================
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn

src_len, pred_len = 24, 12

scaler = MinMaxScaler((-1,1))
ts_scaled = scaler.fit_transform(ts_hist.reshape(-1,1)).flatten()

class WindowDataset(torch.utils.data.Dataset):
    def __init__(self, x, src_len, tgt_len):
        self.x = torch.tensor(x, dtype=torch.float32)
        self.src_len = src_len
        self.tgt_len = tgt_len
        self.idxs = list(range(len(x)-src_len-tgt_len))
    def __len__(self):
        return len(self.idxs)
    def __getitem__(self, i):
        j = self.idxs[i]
        src = self.x[j:j+src_len]
        tgt = self.x[j+src_len:j+src_len+pred_len]
        time_src = torch.arange(j, j+src_len)
        time_tgt = torch.arange(j+src_len, j+src_len+pred_len)
        month_src = (time_src % 12).long()
        month_tgt = (time_tgt % 12).long()
        return (src.unsqueeze(-1), tgt.unsqueeze(-1),
                month_src, month_tgt,
                time_src, time_tgt)

ds = WindowDataset(ts_scaled, src_len, pred_len)
dl = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True)


# ===========================================================
# 3. Time2Vec: learn trend + periodic encodings
# ===========================================================
class Time2Vec(nn.Module):
    def __init__(self, out_dim):
        super().__init__()
        self.w0 = nn.Linear(1, 1)
        self.wk = nn.Linear(1, out_dim-1)
    def forward(self, t):
        t = t.float().unsqueeze(-1)
        trend = self.w0(t)
        periodic = torch.sin(self.wk(t))
        return torch.cat([trend, periodic], dim=-1)


# ===========================================================
# 4. Decoder-only Transformer for forecasting
# ===========================================================
class ForecastTransformer(nn.Module):
    def __init__(self, d_model=128):
        super().__init__()
        self.input_proj = nn.Linear(1, d_model)

        self.month_emb = nn.Embedding(12, 8)
        self.time2vec = Time2Vec(out_dim=8)

        self.temporal_proj = nn.Linear(8+8, d_model)

        # sinusoidal positional encoding
        self.register_buffer("pos", self.sinusoidal_encoding(500, d_model))

        layer = nn.TransformerDecoderLayer(
            d_model=d_model, nhead=8, batch_first=True
        )
        self.decoder = nn.TransformerDecoder(layer, num_layers=4)

        self.out = nn.Linear(d_model, 1)

    def sinusoidal_encoding(self, max_len, d_model):
        pos = np.arange(max_len)[:,None]
        div = np.exp(np.arange(0,d_model,2)*(-np.log(10000)/d_model))
        pe = np.zeros((max_len,d_model))
        pe[:,0::2] = np.sin(pos*div)
        pe[:,1::2] = np.cos(pos*div)
        return torch.tensor(pe, dtype=torch.float32).unsqueeze(0)

    def forward(self, src, tgt, month_src, month_tgt, t_src, t_tgt):
        # project values
        src_val = self.input_proj(src)
        tgt_val = self.input_proj(tgt)

        # time embeddings
        src_time = torch.cat([self.month_emb(month_src), self.time2vec(t_src)], -1)
        tgt_time = torch.cat([self.month_emb(month_tgt), self.time2vec(t_tgt)], -1)
        src_time = self.temporal_proj(src_time)
        tgt_time = self.temporal_proj(tgt_time)

        src_emb = src_val + src_time + self.pos[:, :src_val.size(1)]
        tgt_emb = tgt_val + tgt_time + self.pos[:, :tgt_val.size(1)]

        mask = nn.Transformer.generate_square_subsequent_mask(tgt_emb.size(1)).to(src.device)
        out = self.decoder(tgt_emb, src_emb, tgt_mask=mask)
        return self.out(out)


device = "cuda" if torch.cuda.is_available() else "cpu"
model = ForecastTransformer().to(device)

opt = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()


# ===========================================================
# 5. Training loop
# ===========================================================
def train_epoch():
    model.train()
    losses = []
    for batch in dl:
        src, tgt, msrc, mtgt, tsrc, ttgt = [x.to(device) for x in batch]
        # teacher forcing input
        tgt_in = torch.cat([src[:,-1:], tgt[:,:-1]], dim=1)
        out = model(src, tgt_in, msrc, mtgt, tsrc, ttgt)
        loss = loss_fn(out, tgt)
        opt.zero_grad()
        loss.backward()
        opt.step()
        losses.append(loss.item())
    return np.mean(losses)

for epoch in range(80):
    l = train_epoch()
    print(f"Epoch {epoch:03d} loss={l:.4f}")


# ===========================================================
# 6. Forecast 60 months autoregressively
# ===========================================================
def forecast_next(model, ts_hist, horizon=60):
    model.eval()
    x = scaler.transform(ts_hist.reshape(-1,1)).flatten()
    window = torch.tensor(x[-src_len:], dtype=torch.float32).to(device).view(1,src_len,1)

    preds = []
    for h in range(horizon):
        start = len(ts_hist)-src_len + h
        month_src = torch.tensor([(start+i)%12 for i in range(src_len)]).long().to(device).unsqueeze(0)
        time_src  = torch.tensor([start+i for i in range(src_len)]).long().to(device).unsqueeze(0)

        month_tgt = torch.tensor([(start+src_len+h)%12]).long().to(device).unsqueeze(0).unsqueeze(1)
        time_tgt  = torch.tensor([start+src_len+h]).long().to(device).unsqueeze(0).unsqueeze(1)

        tgt_in = window[:, -1:, :]

        out = model(window, tgt_in, month_src, month_tgt, time_src, time_tgt)
        y = out[0, -1, 0].item()
        preds.append(y)

        window = torch.cat([window[:,1:], out.unsqueeze(1)], dim=1)

    preds = scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()
    return preds


ts_forecast = forecast_next(model, ts_hist, horizon=60)


# ===========================================================
# 7. Plot final result
# ===========================================================
plt.figure(figsize=(10,4))
sns.lineplot(x=time_hist, y=ts_hist, label="history")
sns.lineplot(x=time_true, y=ts_true, label="true future", linestyle="--")
sns.lineplot(x=np.arange(len(ts_hist), len(ts_hist)+60),
             y=ts_forecast, label="forecast", linestyle="-.")
plt.axvline(x=len(ts_hist), linestyle="--", color="red")
plt.legend()
plt.tight_layout()
plt.show()
