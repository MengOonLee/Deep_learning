import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt

# ============================================================
# 1. SYNTHETIC DATA
# ============================================================

seed = 42
np.random.seed(seed)

def trend(time):
    slope = np.zeros_like(time)
    slope[time < (5*12)] = 0.2
    slope[(time >= (5*12)) & (time < (8*12))] = -0.4
    slope[time >= (8*12)] = 1.0
    return slope * time

def seasonal_pattern(season_time):
    return np.where(season_time < 0.4, np.cos(season_time * 2 * np.pi),
        1 / np.exp(3 * season_time))

def seasonality(time, period, amplitude=1, phase=0):
    season_time = ((time + phase) % period) / period
    return amplitude * seasonal_pattern(season_time)

def noise(time, noise_level=5):
    rnd = np.random.RandomState(seed)
    return rnd.randn(len(time)) * noise_level

period = 12 ; baseline = 10 ; amplitude = 40 ; noise_level = 5

time_history = np.arange(10*12)
ts_history = baseline + trend(time_history) + seasonality(time_history, period, amplitude)
ts_history += noise(time_history)
ts_history = ts_history.astype(np.float32)

# ============================================================
# 2. NORMALIZATION
# ============================================================

scaler = StandardScaler()
ts_scaled = scaler.fit_transform(ts_history.reshape(-1, 1)).flatten()

# ============================================================
# 3. DATASET (ENCODER–DECODER)
# ============================================================

class TimeSeriesDataset(Dataset):
    def __init__(self, ts, src_len=24, tgt_len=12):
        self.ts = torch.tensor(ts, dtype=torch.float32).unsqueeze(-1)
        self.src_len = src_len
        self.tgt_len = tgt_len
        self.indices = list(range(len(ts) - (src_len + tgt_len)))

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        i = self.indices[idx]

        src = self.ts[i:i+self.src_len]           # (24,1)
        tgt_full = self.ts[i+self.src_len:i+self.src_len+self.tgt_len]  # (12,1)

        # For teacher forcing: decoder input is shifted right
        bos = torch.zeros(1,1)  # beginning of sequence
        tgt_in = torch.cat([bos, tgt_full[:-1]], dim=0)  # (12,1)
        tgt_out = tgt_full  # (12,1)

        # Time features
        months_src = torch.tensor((np.arange(i, i+self.src_len) % 12), dtype=torch.long)
        years_src  = torch.tensor((np.arange(i, i+self.src_len) // 12), dtype=torch.long)
        months_tgt = torch.tensor((np.arange(i+self.src_len, i+self.src_len+self.tgt_len) % 12),
                                  dtype=torch.long)
        years_tgt  = torch.tensor((np.arange(i+self.src_len, i+self.src_len+self.tgt_len) // 12),
                                  dtype=torch.long)

        return (src, tgt_in, months_src, years_src, months_tgt, years_tgt), tgt_out


src_len = 24
tgt_len = 12

dataset = TimeSeriesDataset(ts_scaled, src_len, tgt_len)
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)

# ============================================================
# 4. TRANSFORMER MODEL (ENCODER–DECODER)
# ============================================================

class TransformerForecast(nn.Module):
    def __init__(self, d_model=64, src_len=24, tgt_len=12):
        super().__init__()

        self.d_model = d_model
        self.src_len = src_len
        self.tgt_len = tgt_len

        self.input_proj = nn.Linear(1, d_model)

        self.month_emb = nn.Embedding(12, 8)
        self.year_emb  = nn.Embedding(100, 8)
        self.time_proj = nn.Linear(16, d_model)

        # Learnable positional embeddings
        self.pos_src = nn.Parameter(torch.randn(1, src_len, d_model))
        self.pos_tgt = nn.Parameter(torch.randn(1, tgt_len, d_model))

        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=4, dim_feedforward=128, dropout=0.1, batch_first=True)
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=2)

        dec_layer = nn.TransformerDecoderLayer(
            d_model=d_model, nhead=4, dim_feedforward=128, dropout=0.1, batch_first=True)
        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=2)

        self.output_proj = nn.Linear(d_model, 1)

    def forward(self, src, tgt_in, months_src, years_src, months_tgt, years_tgt):

        # ---------------- ENCODER ----------------
        xs = self.input_proj(src)  # (B,24,d_model)

        time_src = torch.cat([self.month_emb(months_src),
                              self.year_emb(years_src)], dim=-1)
        time_src = self.time_proj(time_src)

        xs = xs + self.pos_src + time_src
        memory = self.encoder(xs)  # (B,24,d_model)

        # ---------------- DECODER ----------------
        xt = self.input_proj(tgt_in)  # (B,12,d_model)

        time_tgt = torch.cat([self.month_emb(months_tgt),
                              self.year_emb(years_tgt)], dim=-1)
        time_tgt = self.time_proj(time_tgt)

        xt = xt + self.pos_tgt + time_tgt

        # Autoregressive mask
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(self.tgt_len).to(src.device)

        out = self.decoder(xt, memory, tgt_mask=tgt_mask)

        return self.output_proj(out)  # (B,12,1)


# ============================================================
# 5. TRAINING LOOP
# ============================================================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = TransformerForecast(d_model=64, src_len=24, tgt_len=12).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

def train_epoch():
    model.train()
    losses = []
    for (src, tgt_in, m_s, y_s, m_t, y_t), tgt_out in train_loader:
        src = src.to(device)
        tgt_in = tgt_in.to(device)
        tgt_out = tgt_out.to(device)

        m_s, y_s = m_s.to(device), y_s.to(device)
        m_t, y_t = m_t.to(device), y_t.to(device)

        optimizer.zero_grad()

        pred = model(src, tgt_in, m_s, y_s, m_t, y_t)
        loss = criterion(pred, tgt_out)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        losses.append(loss.item())
    return np.mean(losses)

# Train for 300 epochs (fast on GPU/CPU)
for epoch in range(300):
    loss = train_epoch()
    if epoch % 50 == 0:
        print(f"Epoch {epoch} — loss: {loss:.4f}")


# ============================================================
# 6. INFERENCE (AUTOREGRESSIVE 12-MONTH FORECAST)
# ============================================================

model.eval()

src = ts_scaled[-src_len:]            # last 24 months
src = torch.tensor(src, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)

# Decoder input sequence filled with zeros initially
tgt_in = torch.zeros(1, tgt_len, 1).to(device)

# Time features
start_idx = len(ts_scaled) - src_len
months_src = torch.tensor((np.arange(start_idx, start_idx+src_len) % 12)).long().to(device)
years_src  = torch.tensor((np.arange(start_idx, start_idx+src_len) // 12)).long().to(device)

months_tgt = torch.tensor((np.arange(start_idx+src_len, start_idx+src_len+tgt_len) % 12)).long().to(device)
years_tgt  = torch.tensor((np.arange(start_idx+src_len, start_idx+src_len+tgt_len) // 12)).long().to(device)

with torch.no_grad():
    pred_scaled = model(src, tgt_in, months_src, years_src, months_tgt, years_tgt)
    pred_scaled = pred_scaled.cpu().numpy().squeeze()

# Inverse scale
forecast = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()

# ============================================================
# 7. PLOT
# ============================================================

plt.figure(figsize=(12,5))
plt.plot(time_history, ts_history, label="True")
plt.plot(np.arange(len(ts_history), len(ts_history)+tgt_len),
         forecast, "--", label="Forecast")
plt.axvline(len(ts_history), color="red", linestyle="--")
plt.legend()
plt.show()
