{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/Getting_started_TensorFlow2/Sequential_model_API/Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No broken requirements found.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --no-cache-dir -qU pip wheel\n",
    "pip install --no-cache-dir -qU numpy pandas matplotlib seaborn scikit-learn\n",
    "pip install --no-cache-dir -qU tensorflow-gpu\n",
    "pip check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gK55Pq_iSySn"
   },
   "source": [
    "# Metrics in Keras\n",
    "\n",
    "In this reading we will be exploring the different metrics in Keras that may be used to judge the performance of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKbVIdeqSySp",
    "outputId": "4eb2ca58-a606-42d9-b512-f8c87aa9b951"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font=\"DejaVu Sans\")\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot_history():\n",
    "    plt_nrows = 1\n",
    "    plt_ncols = 2\n",
    "    plt_figsize = (15, 5)\n",
    "    \n",
    "    def __init__(self, history, metrics=['loss']):\n",
    "        super().__init__()\n",
    "        self.history = history\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    @classmethod\n",
    "    def _create_plot(cls):\n",
    "        return plt.subplots(nrows=cls.plt_nrows,\n",
    "            ncols=cls.plt_ncols, figsize=cls.plt_figsize)\n",
    "    \n",
    "    def plot(self):\n",
    "        df_history = pd.DataFrame(self.history.history,\n",
    "            index=self.history.epoch)\n",
    "        fig, axes = self._create_plot()\n",
    "        for ax, metric in zip(axes, self.metrics):\n",
    "            sns.lineplot(ax=ax, data=df_history,\n",
    "                x=df_history.index+1, y=metric, label='train')\n",
    "            try:\n",
    "                sns.lineplot(ax=ax, data=df_history,\n",
    "                    x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
    "            except Exception:\n",
    "                pass\n",
    "            ax.legend(loc='best')\n",
    "            ax.set_xlabel('epoch')\n",
    "            ax.set_ylabel(f'{metric}')\n",
    "            ax.set_title(f'{metric} vs. epoch')\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMb7d1sPSyS0"
   },
   "source": [
    "One of the most common metrics used for classification problems in Keras is `accuracy`.\n",
    "\n",
    "We will begin with a simple example of a model that uses accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7fbLvBRdSyS3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 13:16:22.944821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:22.969131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:22.969345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:22.969837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-12 13:16:22.970544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:22.970723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:22.970887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:23.432935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:23.433171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:23.433355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 13:16:23.433504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4364 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660, pci bus id: 0000:07:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='tanh'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8I8cSJqSyS9"
   },
   "source": [
    "We now have a model that uses accuracy as a metric to judge its performance.\n",
    "\n",
    "But how is this metric actually calculated? We will break our discussion into two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgblunXkSyTA"
   },
   "source": [
    "### Case 1 - Binary Classification with sigmoid activation function\n",
    "Suppose we are training a model for a binary classification problem with a sigmoid activation function (softmax activation functions are covered in the next case).\n",
    "\n",
    "Given a training example with input $x^{(i)}$, the model will output a float between 0 and 1. Based on whether this float is less than or greater than our \"threshold\" (which by default is set at 0.5), we round the float to get the predicted classification $y_{pred}$ from the model.\n",
    "\n",
    "The accuray metric compares the value of $y_{pred}$ on each training example with the true output, the one-hot coded vector $y_{true}^{(i)}$ from our training data.\n",
    "\n",
    "Let $$\\delta(y_{pred}^{(i)}, y_{true}^{(i)}) =\n",
    "\\begin{cases} 1 & y_{pred}=y_{true} \\\\\n",
    "0 & y_{pred} \\neq y_{true} \\end{cases}$$\n",
    "\n",
    "The accuracy metric computes the mean of\n",
    "$\\delta(y_{pred}^{(i)}, y_{true}^{(i)})$ over all training examples.\n",
    "\n",
    "$$accuracy = \\frac{1}{N} \\sum_{i=1}^N\n",
    "\\delta(y_{pred}^{(i)}, y_{true}^{(i)})$$\n",
    "\n",
    "This is implemented in the backend of Keras as follows.\n",
    "Note: We have set $y_{true}$ and $y_{pred}$ ourselves for the purposes of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVctmRYYSyTA",
    "outputId": "fd5bacfc-6b2f-43d5-e3bb-ac2cad7778ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Sigmoid activation function\n",
    "y_true = tf.constant([0.0, 1.0, 1.0])\n",
    "y_pred = tf.constant([0.4, 0.8, 0.3])\n",
    "accuracy = tf.keras.backend.mean(\n",
    "    tf.keras.backend.equal(y_true, tf.keras.backend.round(y_pred)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfTJQzDfSyTG"
   },
   "source": [
    "### Case 2 - Categorical Classification\n",
    "Now suppose we are training a model for a classification problem which should sort data into $m > 2$ different classes using a softmax activation function in the last layer.\n",
    "\n",
    "Given a training example with input $x^{(i)}$, the model will output a tensor of probabilities $p_1, p_2, \\dots p_m$, giving the likelihood (according to the model) that $x^{(i)}$ falls into each class.\n",
    "\n",
    "The accuracy metric works by determining the largest argument in the $y_{pred}^{(i)}$ tensor, and compares its index to the index of the maximum value of $y_{true}^{(i)}$ to determine $\\delta(y_{pred}^{(i)}, y_{true}^{(i)})$. It then computes thte accuracy in the same way as for the binary classification case.\n",
    "\n",
    "$$accuracy = \\frac{1}{N} \\sum_{i=1}^N\n",
    "\\delta(y_{pred}^{(i)}, y_{true}^{(i)})$$\n",
    "\n",
    "In the backend of Keras, the accuracy metric is implemented slightly differently depending on whether we have a binary classification problem ($m = 2$) or a categorical classification problem. Note that the accuracy for binary classification problems is the same, no matter if we use a sigmoid or softmax activation function to obtain the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-X56yzp_SyTI",
    "outputId": "5ce7f280-9eea-4de3-c170-4d29056da677"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification with softmax\n",
    "y_true = tf.constant([[0.0,1.0],[1.0,0.0],[1.0,0.0],[0.0,1.0]])\n",
    "y_pred = tf.constant([[0.4,0.6],[0.3,0.7],[0.05,0.95],[0.33,0.67]])\n",
    "accuracy = K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbjF9KMRSyTN",
    "outputId": "8ef2726f-f783-46ed-e464-53678de6321c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6666667>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical classification with m>2\n",
    "y_true = tf.constant([[0.0,1.0,0.0,0.0],[1.0,0.0,0.0,0.0],[0.0,0.0,1.0,0.0]])\n",
    "y_pred = tf.constant([[0.4,0.6,0.0,0.0],[0.3,0.2,0.1,0.4],[0.05,0.35,0.5,0.1]])\n",
    "accuracy = K.mean(K.equal(\n",
    "    K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Lmq-0lKSyTT"
   },
   "source": [
    "## Other examples of metrics\n",
    "We will now look at some other metrics in Keras. A full list is\n",
    "available at <https://keras.io/metrics/>\n",
    "\n",
    "### Binary accuracy and categorical accuracy\n",
    "The `binary_accuracy` and `categorical_accuracy` metrics are, by default, identical to the Case 1 and 2 respectively of the `accuracy` metric explained above.\n",
    "\n",
    "However, using `binary_accuracy` allows you to use thte optional `threshold` argument, which sets the minimum value of $y_{pred}$ which will be rounded to 1. As mentioned above, it is set as \n",
    "`threshold = 0.5` by default.\n",
    "\n",
    "Below we give some examples of how to compile a model with `binary_accuracy` with and without a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HF7aZjg4SyTV"
   },
   "outputs": [],
   "source": [
    "# Compile the model with default threshold (=0.5)\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['binary_accuracy'])\n",
    "\n",
    "# The threshold can be specified as follows\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvROzGaeSyTZ"
   },
   "source": [
    "### Sparse categorical accuracy\n",
    "\n",
    "This is a very similar metric to categorical accuracy with one major difference - the label $y_{true}$ of each training example is not expected to be a one-hot encoded vector, but to be a tensor consisting of a single integer. This integer is then compared to the index of the maximum argument of $y_{pred}$ to determine \n",
    "$\\delta(y_{pred}^{(i)}, y_{true}^{(i)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2wCOw2FKSyTa"
   },
   "outputs": [],
   "source": [
    "# Two examples of compiling a model with a \n",
    "# sparse categorical accuracy metric\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amzk_Ef9SyTe"
   },
   "source": [
    "### (Sparse) Top $k$-categorical accuracy\n",
    "In top $k$-categorical accuracy, instead of computing how often the model correctly predicts the label of a training example, the metric computes how often the model has $y_{true}$ in the top $k$ of its predictions. By default, $k=5$.\n",
    "\n",
    "As before, the main difference between top $k$-categorical accuracy and its sparse version is that the former assumes $y_{true}$ is a one-hot encoded vector, whereas the sparse version assumes $y_{true}$ is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "polAIeO_SyTg"
   },
   "outputs": [],
   "source": [
    "# Compile a model with a top-k categorical accuracy metric with default k(=5)\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['top_k_categorical_accuracy'])\n",
    "\n",
    "# Specify k instead with the sparse top-k categorical accuracy\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXmRE6NDSyTl"
   },
   "source": [
    "## Custom metrics\n",
    "It is also possible to define your own custom metric in Keras.\n",
    "You will need to make sure that your metric takes in (at least) two arguments called `y_true` and `y_pred` and then output a single tensor value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OH4fnX1WSyTm"
   },
   "outputs": [],
   "source": [
    "# Define a custom metric\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTQMlBy-SyTq"
   },
   "source": [
    "We can then use this metric when we compile our model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Cg2VBUPJSyTr"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6c1CLMsSyTw"
   },
   "source": [
    "## Multiple metrics\n",
    "Finally, it is possible to use multiple metrics to judge the performance of your model.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1tFdvA8SSyTy"
   },
   "outputs": [],
   "source": [
    "# Compile the model with multiple metrics\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[mean_pred, 'accuracy', \n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SnzeLDCSyT3"
   },
   "source": [
    "### Sources and Further Reading\n",
    "* The metrics page on the Keras website: https://keras.io/metrics/\n",
    "* The source code for the metrics: https://github.com/keras-team/keras/blob/master/keras/metrics.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
