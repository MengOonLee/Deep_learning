{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8EuPVDK0oXX"
   },
   "source": [
    "# Device placement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMepbBP-0oXb"
   },
   "source": [
    "In this reading, we are going to be looking at device placement. We will see how to access the device associated to a given tensor, and compare the use of GPUs and CPUs.\n",
    "\n",
    "When running this notebook, ensure that the GPU runtime type is selected (Runtime -> Change runtime type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --no-cache-dir -qU pip wheel\n",
    "pip install --no-cache-dir -qU numpy==1.23.0 pandas matplotlib seaborn scikit-learn\n",
    "pip install --no-cache-dir -qU tensorflow pydot\n",
    "pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGJhiWHQ0oXd",
    "outputId": "2443cbda-66ae-48de-d556-781fe9dc99a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font='DejaVu Sans')\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o07Lk8BE27-6"
   },
   "source": [
    "## Get the physical devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw5wPXyy2Zwp"
   },
   "source": [
    "First, we can list the physical devices available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rB-00SsB2Z8v",
    "outputId": "a3c95774-8334-40ce-fdf3-fa793c4d7690",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all physical devices\n",
    "\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBfa_PEw0oXk"
   },
   "source": [
    "If you have enabled the GPU runtime, then you should see the GPU device in the above list.\n",
    "\n",
    "We can also check specifically for the GPU or CPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FgViTqb0oXn",
    "outputId": "629d82a6-b0d6-46a5-edb9-001681bf4b77",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU devices\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N857-C_B2yMa",
    "outputId": "96acac70-f674-43d9-b6a1-1dc3b48d0f0a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for CPU devices\n",
    "\n",
    "tf.config.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GStCHnkx0oXs"
   },
   "source": [
    "We can get the GPU device name as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LlT6kf810oXu",
    "outputId": "fd5f7516-453f-4658-e47e-33af03fad25e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 08:11:51.809942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-31 08:11:51.811681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 08:11:51.812303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 08:11:51.812713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 08:11:52.270009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 08:11:52.270228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 08:11:52.270396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 08:11:52.270530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 4390 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660, pci bus id: 0000:29:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the GPU device name\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FNvARDH3KlC"
   },
   "source": [
    "## Placement of Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB7qxgNL0oYA"
   },
   "source": [
    "TensorFlow will automatically allocate Tensor operations to a physical device, and will handle the copying between CPU and GPU memory if necessary. \n",
    "\n",
    "Let's define a random Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FQrsztEl0oYB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.68789124, 0.48447883, 0.9309944 ],\n",
       "       [0.252187  , 0.73115396, 0.89256823],\n",
       "       [0.94674826, 0.7493341 , 0.34925628]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Tensor\n",
    "\n",
    "x = tf.random.uniform([3, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTni7OPe0oYF"
   },
   "source": [
    "We can see which device this Tensor is placed on using its `device` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iLSbukXa0oYH",
    "outputId": "ed13b08b-b1fd-4ac0-8f49-2e33cba88098",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:GPU:0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Tensor device\n",
    "\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uVDrVK60oYM"
   },
   "source": [
    "The above string will end with `'GPU:K'` if the Tensor is placed on the `K`-th GPU device. We can also check if a tensor is placed on a specific device by using `device_endswith`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n0ceL5Qy0oYN",
    "outputId": "790dc6ae-6a11-4c89-a1a3-98c55bfeb821",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the Tensor on CPU #0:  \n",
      "False\n",
      "\n",
      "Is the Tensor on GPU #0:  \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test for device allocation\n",
    "\n",
    "print(\"Is the Tensor on CPU #0:  \"),\n",
    "print(x.device.endswith('CPU:0'))\n",
    "print('')\n",
    "print(\"Is the Tensor on GPU #0:  \"),\n",
    "print(x.device.endswith('GPU:0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtEi2qnK4LyO"
   },
   "source": [
    "## Specifying device placement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33SlBwIA0oYQ"
   },
   "source": [
    "As mentioned previously, TensorFlow will automatically allocate Tensor operations to specific devices. However, it is possible to force placement on specific devices, if they are available. \n",
    "\n",
    "We can view the benefits of GPU acceleration by running some tests and placing the operations on the CPU or GPU respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wEfz4RY90oYR"
   },
   "outputs": [],
   "source": [
    "# Define simple tests to time computation speed\n",
    "\n",
    "import time\n",
    "\n",
    "def time_matadd(x):\n",
    "    start = time.time()\n",
    "    for loop in range(10):\n",
    "        tf.add(x, x)\n",
    "    result = time.time()-start\n",
    "    print(\"Matrix addition (10 loops): {:0.2f} ms\".format(1000*result))\n",
    "\n",
    "def time_matmul(x):\n",
    "    start = time.time()\n",
    "    for loop in range(10):\n",
    "        tf.matmul(x, x)\n",
    "    result = time.time()-start\n",
    "    print(\"Matrix multiplication (10 loops): {:0.2f} ms\".format(1000*result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEd85pdw5N6c"
   },
   "source": [
    "In the following cell, we run the above tests inside the context `with tf.device(\"CPU:0\")`, which forces the operations to be run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rB5Z1iUT0oYU",
    "outputId": "755092bd-f056-4c3e-dc45-3ce67e9634e8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On CPU:\n",
      "Matrix addition (10 loops): 12.74 ms\n",
      "Matrix multiplication (10 loops): 137.07 ms\n"
     ]
    }
   ],
   "source": [
    "# Force execution on CPU\n",
    "\n",
    "print(\"On CPU:\")\n",
    "with tf.device(\"CPU:0\"):\n",
    "    x = tf.random.uniform([1000, 1000])\n",
    "    assert x.device.endswith(\"CPU:0\")\n",
    "    time_matadd(x)\n",
    "    time_matmul(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN3ZHIMD50kB"
   },
   "source": [
    "And now run the same operations on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7AryQq60oYY",
    "outputId": "b9d42db3-42fb-4e66-cabb-cee538fdd281",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On GPU:\n",
      "Matrix addition (10 loops): 1.77 ms\n",
      "Matrix multiplication (10 loops): 2.92 ms\n"
     ]
    }
   ],
   "source": [
    "# Force execution on GPU #0 if available\n",
    "\n",
    "if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "    print(\"On GPU:\")\n",
    "    with tf.device(\"GPU:0\"): \n",
    "        x = tf.random.uniform([1000, 1000])\n",
    "        assert x.device.endswith(\"GPU:0\")\n",
    "        time_matadd(x)\n",
    "        time_matmul(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqJR83G354Zh"
   },
   "source": [
    "Note the significant time difference between running these operations on different devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFOJkxMk-HL_"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gBgorhv-HWE"
   },
   "source": [
    "Finally, we will demonstrate that GPU device placement offers speedup benefits for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Qa7Vvbne6QS8"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kJgFyw0567Dn"
   },
   "outputs": [],
   "source": [
    "# Reduce the dataset size to speed up the test\n",
    "\n",
    "x_train, y_train = x_train[:1000], y_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2vD2d7al4_VZ"
   },
   "outputs": [],
   "source": [
    "# Define a function to build the model\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
    "            activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),\n",
    "            activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),\n",
    "            activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxEcTFQb7hRt",
    "outputId": "e83558ce-a9e3-46fa-c7b3-d8e8aa422547",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU training time: 2902.69ms\n"
     ]
    }
   ],
   "source": [
    "# Time a training run on the CPU\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    model = get_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=1e-3), \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        x=x_train[..., np.newaxis], y=y_train,\n",
    "        epochs=5, verbose=0)\n",
    "    result = time.time() - start\n",
    "\n",
    "print(\"CPU training time: {:0.2f}ms\".format(1000 * result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9_G4sak5dHB",
    "outputId": "317fcd0b-5d4e-42be-cbee-555930e52f12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training time: 1922.78ms\n"
     ]
    }
   ],
   "source": [
    "# Time a training run on the GPU\n",
    "\n",
    "with tf.device(\"GPU:0\"):\n",
    "    model = get_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=1e-3), \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    start = time.time()\n",
    "    model.fit(x=x_train[..., np.newaxis], y=y_train,\n",
    "        epochs=5, verbose=0)\n",
    "    result = time.time() - start\n",
    "\n",
    "print(\"GPU training time: {:0.2f}ms\".format(1000 * result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_O9D-XI0oYa"
   },
   "source": [
    "## Further reading and resources \n",
    "* https://www.tensorflow.org/tutorials/customization/basics#gpu_acceleration"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Device placement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
