{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow2/Customise/DataPipeline/TFDataset_pandas_dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5f8-rto_3Qf"
   },
   "source": [
    "# Creating Datasets from different sources\n",
    "\n",
    "In this reading notebook, we will explore a few of the ways in which we can load data into a `tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5qH-T29wFPD9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No broken requirements found.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --no-cache-dir -qU pip wheel\n",
    "pip install --no-cache-dir -qU numpy pandas matplotlib seaborn scikit-learn\n",
    "pip install --no-cache-dir -qU tensorflow\n",
    "pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZCKvIeJ_3Qh",
    "outputId": "ca5060e0-59c6-4ca0-f4a3-f7021bc32d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TP_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROrVHlBg_3Qr"
   },
   "source": [
    "## The `from_tensor_slices` and `from_tensors` methods\n",
    "\n",
    "We will start by looking at the `from_tensor_slices` and the `from_tensors` methods.\n",
    "\n",
    "Both static methods are used to create datasets from Tensors or Tensor-like objects, such as numpy arrays or python lists. We can also pass in tuples and dicts of arrays or lists. The main distinction between the `from_tensor_slices` function and the `from_tensors` function is that the `from_tensor_slices` method will interpret the first dimension of the input data as the number of elements in the dataset, whereas the `from_tensors` method always results in a Dataset with a single element, containing the Tensor or tuple of Tensors passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e0L-kIg_3Qv",
    "outputId": "4b7776d0-ca7e-4c52-82f1-de23fdef050a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor with shape (3, 2)\n",
    "\n",
    "example_tensor = tf.random.uniform(shape=[3, 2])\n",
    "print(example_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rofMqf3B_3Q1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(2,), dtype=tf.float32, name=None)\n",
      "tf.Tensor([0.68789124 0.48447883], shape=(2,), dtype=float32)\n",
      "tf.Tensor([0.9309944 0.252187 ], shape=(2,), dtype=float32)\n",
      "tf.Tensor([0.73115396 0.89256823], shape=(2,), dtype=float32)\n",
      "TensorSpec(shape=(3, 2), dtype=tf.float32, name=None)\n",
      "tf.Tensor(\n",
      "[[0.68789124 0.48447883]\n",
      " [0.9309944  0.252187  ]\n",
      " [0.73115396 0.89256823]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create two Datasets, using each static method\n",
    "# Print the element_spec for each\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
    "    tensors=example_tensor)\n",
    "print(dataset1.element_spec)\n",
    "for elem in dataset1:\n",
    "    print(elem)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensors(\n",
    "    tensors=example_tensor)\n",
    "print(dataset2.element_spec)\n",
    "for elem in dataset2:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu-GIIvC_3RE"
   },
   "source": [
    "As seen above, creating the Dataset using the `from_tensor_slices` method slices the given array or Tensor along the first dimension to produce a set of elements for the Dataset.\n",
    "\n",
    "This means that although we could pass any Tensor - or tuple of Tensors - to the `from_tensors` method, the same cannot be said of the `from_tensor_slices` method, which has the additional requirement that each Tensor in the list has the same size in the zeroth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A4VOx4yX_3RG"
   },
   "outputs": [],
   "source": [
    "# Create three Tensors with different shapes\n",
    "\n",
    "tensor1 = tf.random.uniform(shape=[10, 2, 2])\n",
    "tensor2 = tf.random.uniform(shape=[10, 1])\n",
    "tensor3 = tf.random.uniform(shape=[9, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-zKj2KI_3RP"
   },
   "source": [
    "We cannot create a Dataset using the `from_tensor_slices` method from a list of `tensor1` and `tensor3` since they do not have the same size in the first dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "yHx00YMc_3RR",
    "outputId": "91aa704c-ad85-4de4-9ed6-5daa06f0c4f4",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 10 and 9 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try to create a Dataset from tensor1 and tensor3 using from_tensor_slices - this will raise an error\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:818\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:45\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     42\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mDimension(\n\u001b[1;32m     43\u001b[0m     tensor_shape\u001b[38;5;241m.\u001b[39mdimension_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_shape()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 45\u001b[0m   \u001b[43mbatch_dim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_is_compatible_with\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimension_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mtensor_slice_dataset(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors,\n\u001b[1;32m     51\u001b[0m     output_shapes\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_shapes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure),\n\u001b[1;32m     52\u001b[0m     is_files\u001b[38;5;241m=\u001b[39mis_files,\n\u001b[1;32m     53\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mSerializeToString())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m(TensorSliceDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[0;32m~/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:297\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    is_compatible_with).\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are not compatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    298\u001b[0m                    (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 10 and 9 are not compatible"
     ]
    }
   ],
   "source": [
    "# Try to create a Dataset from tensor1 and tensor3 using from_tensor_slices - this will raise an error\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    tensors=(tensor1, tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q779VFd_3RW"
   },
   "source": [
    "However, we can of course create a Dataset from this tuple using the `from_tensors` method, which interprets the tuple as a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C26468e_3RX",
    "outputId": "5721acac-5185-43aa-cc24-365d759e5c0b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(10, 2, 2), dtype=tf.float32, name=None), TensorSpec(shape=(9, 2, 2), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Create a Dataset from tensor1 and tensor3 using from_tensors\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors(\n",
    "    tensors=(tensor1, tensor3))\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG031o_Q_3Rc"
   },
   "source": [
    "Although `tensor1` and `tensor2` do not have the same shape, or even same rank (number of dimensions), we can still use the `from_tensor_slices` method to form a dataset from a list of these tensors, since they have the same size in the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cocsbny6_3Rc",
    "outputId": "de2cabdb-b133-4608-99de-461fca73cb7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(2, 2), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "# Create a Dataset from tensor1 and tensor2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    tensors=(tensor1, tensor2))\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmS2-pWM_3Ri"
   },
   "source": [
    "In the above, the first dimension was interpreted as the number of elements in the Dataset, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE2tys74_3Rj"
   },
   "source": [
    "### Creating Datasets from numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLgmebmD_3Rk"
   },
   "source": [
    "We can also use the `from_tensor_slices` and `from_tensors` methods to create Datasets from numpy arrays. In fact, behind the scenes, the numpy array is converted to a set of `tf.constant` operations to populate the Tensor in the TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d5blPTp_3Rl",
    "outputId": "f91ae62c-35e9-4a60-cd23-a64e3b502aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array dataset\n",
    "\n",
    "numpy_array = np.array([\n",
    "    [[1, 2], [3, 4]],\n",
    "    [[5, 6], [7, 8]],\n",
    "    [[9, 10], [11, 12]]\n",
    "])\n",
    "print(numpy_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPrEMlsd_3Rq",
    "outputId": "ec00b9ea-f316-45d3-c404-1baeb17283ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(2, 2), dtype=tf.int64, name=None)\n",
      "TensorSpec(shape=(3, 2, 2), dtype=tf.int64, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Create two Datasets, using each static method\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
    "    tensors=numpy_array)\n",
    "print(dataset1.element_spec)\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensors(\n",
    "    tensors=numpy_array)\n",
    "print(dataset2.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPD2u3fu_3Rw"
   },
   "source": [
    "As before, `from_tensors` interprets the entire array as a single element, whereas `from_tensor_slices` slices the array along the first dimension to form the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3xlPlXd_3Rw"
   },
   "source": [
    "### Creating Datasets from pandas DataFrames\n",
    "A pandas DataFrame can be easily converted to a Dataset using the `from_tensor_slices` method. \n",
    "#### The Balloons dataset\n",
    "A pandas DataFrame can be loaded from a CSV file. We will use the [Balloons dataset](https://archive.ics.uci.edu/ml/datasets/Balloons) to demonstrate. This dataset is stored in a CSV file, and contains a list of attributes describing instances of a balloon inflation experiment, such as the colour and size of the balloon, the age of the person who performed the attempted inflation, and the way in which they did it. Finally, there is the target column \"Inflated\", which is either `T` for True, or `F` for False, indicating whether or not the person managed to inflate the balloon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q1abbQ2KEwzn"
   },
   "outputs": [],
   "source": [
    "os.makedirs(name=\"./data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IhjqVibO_3Rx"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file into a Dataframe\n",
    "\n",
    "pandas_dataframe = pd.read_csv(\"./data/balloon_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0Cr6E1CP_3R4",
    "outputId": "45463014-974a-4484-c500-232d1bf54d70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Colour</th>\n",
       "      <th>Size</th>\n",
       "      <th>Act</th>\n",
       "      <th>Age</th>\n",
       "      <th>Inflated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YELLOW</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>STRETCH</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YELLOW</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>STRETCH</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YELLOW</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>STRETCH</td>\n",
       "      <td>CHILD</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YELLOW</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>DIP</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YELLOW</td>\n",
       "      <td>SMALL</td>\n",
       "      <td>DIP</td>\n",
       "      <td>CHILD</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Colour   Size      Act    Age Inflated\n",
       "0  YELLOW  SMALL  STRETCH  ADULT        T\n",
       "1  YELLOW  SMALL  STRETCH  ADULT        T\n",
       "2  YELLOW  SMALL  STRETCH  CHILD        F\n",
       "3  YELLOW  SMALL      DIP  ADULT        F\n",
       "4  YELLOW  SMALL      DIP  CHILD        F"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "\n",
    "pandas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ref4oYh_3R8"
   },
   "source": [
    "To convert the DataFrame to a Dataset, we first convert the DataFrame to a dictionary. By doing this, we preserve the column names as the dictionary labels.\n",
    "\n",
    "**Note**: A Dataset can be formed from either a tuple or a dict of Tensors. We saw above a number of Datasets being formed from a tuple. The only distinction for a Dataset formed from a dict is that the Dataset items will be dicts accessed by key, rather than tuples accessed by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3AEisUD_3R9",
    "outputId": "f1849798-7a84-4f47-fe21-97c08b24df34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Colour', 'Size', 'Act', 'Age', 'Inflated'])\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a dict\n",
    "\n",
    "dataframe_dict = dict(pandas_dataframe)\n",
    "print(dataframe_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjAE6O05_3SC"
   },
   "source": [
    "We can now run the `from_tensor_slices` method on this `dict` and print the resulting Dataset `element_spec`, as well as an example element. Note that since we formed the Dataset from a `dict`, we see the column (dictionary) names in the `element_spec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dswbxybY_3SD"
   },
   "outputs": [],
   "source": [
    "# Create the Dataset\n",
    "\n",
    "pandas_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    tensors=dataframe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTJW4N36_3SH",
    "outputId": "f71cd794-c099-43e8-8ffb-27d9dbe58dcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Colour': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'Size': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'Act': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'Age': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'Inflated': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the Dataset element_spec\n",
    "\n",
    "pandas_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZ2P_PSE_3SL",
    "outputId": "5db24642-3747-4ca9-aa38-937adabdf3bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Colour': <tf.Tensor: shape=(), dtype=string, numpy=b'YELLOW'>,\n",
       " 'Size': <tf.Tensor: shape=(), dtype=string, numpy=b'SMALL'>,\n",
       " 'Act': <tf.Tensor: shape=(), dtype=string, numpy=b'STRETCH'>,\n",
       " 'Age': <tf.Tensor: shape=(), dtype=string, numpy=b'ADULT'>,\n",
       " 'Inflated': <tf.Tensor: shape=(), dtype=string, numpy=b'T'>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate the Dataset\n",
    "\n",
    "next(iter(pandas_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB40lKzN_3SQ"
   },
   "source": [
    "## Creating Datasets directly from CSV Files\n",
    "\n",
    "The TensorFlow experimental library contains a variety of functions and classes contributed by the community that may not be ready for release into the main TensorFlow library in their immediate form, but which may be included in TensorFlow in the future. One such useful experimental function is the `tf.data.experimental.make_csv_dataset` function. This allows us to read CSV data from the disk directly into a Dataset object.\n",
    "\n",
    "We will run the function on the example CSV file from disk, and specify the batch size and the name of the target column, which is used to structure the Dataset into an `(input, target)` tuple.\n",
    "\n",
    "**Note:** Because of the ephemeral nature of the `experimental` package, you may well get warnings printed in the console when using a function or class contained in the package for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hd1Pnu0C_3SR"
   },
   "outputs": [],
   "source": [
    "# Create the Dataset from the CSV file\n",
    "\n",
    "csv_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='./data/balloon_dataset.csv',\n",
    "    batch_size=1, label_name='Inflated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS5gTcja_3SV"
   },
   "source": [
    "To check that we've loaded our Dataset correctly, let's print the `element_spec`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSNITKfJ_3SW",
    "outputId": "1353b1b5-4ed7-4ce2-e2a9-567347539c91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('Colour', TensorSpec(shape=(1,), dtype=tf.string, name=None)),\n",
       "              ('Size', TensorSpec(shape=(1,), dtype=tf.string, name=None)),\n",
       "              ('Act', TensorSpec(shape=(1,), dtype=tf.string, name=None)),\n",
       "              ('Age', TensorSpec(shape=(1,), dtype=tf.string, name=None))]),\n",
       " TensorSpec(shape=(1,), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the Dataset element_spec\n",
    "\n",
    "csv_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YLC1Hyt_3Sb",
    "outputId": "1b9cb60b-10fa-479f-ea2a-2b1565e4b64f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('Colour',\n",
       "               <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'YELLOW'], dtype=object)>),\n",
       "              ('Size',\n",
       "               <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'LARGE'], dtype=object)>),\n",
       "              ('Act',\n",
       "               <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'DIP'], dtype=object)>),\n",
       "              ('Age',\n",
       "               <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'CHILD'], dtype=object)>)]),\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'F'], dtype=object)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate the Dataset\n",
    "\n",
    "next(iter(csv_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox-YVNZf_3Sf"
   },
   "source": [
    "Note that in the above Dataset, the target column `Inflated` does not have a key, since it is uniquely accessible as the second element of the tuple, whereas the attributes which reside as a dictionary of Tensors in the first element retain their labels so we can distinguish them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2sHapSj_3Sg"
   },
   "source": [
    "## Further reading and resources\n",
    "\n",
    "* https://www.tensorflow.org/guide/data\n",
    "* https://www.tensorflow.org/tutorials/load_data/csv\n",
    "* https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
    "* https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "TFDataset_pandas_dataframe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
