{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow2/Customise/DataPipeline/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwp-WlWbhZ__"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cBJq_RLhaAB"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Keras datasets](#coding_tutorial_1)\n",
        " #### [2. Dataset generators](#coding_tutorial_2)\n",
        " #### [3. Keras image data augmentation](#coding_tutorial_3)\n",
        " #### [4. The Dataset class](#coding_tutorial_4)\n",
        " #### [5. Training with Datasets](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWJ8FsVQhaAE"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Keras datasets\n",
        "\n",
        "For a list of Keras datasets and documentation on recommended usage, see [this link](https://keras.io/datasets/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgnvqE4bhaAM"
      },
      "source": [
        "#### Load the CIFAR-100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UvgpO_ithaAS",
        "tags": [],
        "outputId": "d86541ba-d3cb-42c1-a214-9068e3c3e478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1,), dtype=tf.int64, name=None))\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the CIFAR-100 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = \\\n",
        "    tf.keras.datasets.cifar100.load_data(label_mode=\"fine\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(train_images, train_labels))\n",
        "\n",
        "# Inspect the dataset.\n",
        "print(dataset.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf61wNi8haAg"
      },
      "source": [
        "#### Examine the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "638jOdbUhaAu"
      },
      "source": [
        "The list of labels for the CIFAR-100 dataset are available [here](https://www.cs.toronto.edu/~kriz/cifar.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CaZS2ZqRhaAp",
        "outputId": "85984a93-3f44-4ec3-9f21-0ebd41dadd6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x300 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "with zipfile.ZipFile(file=\"Files.zip\") as file:\n",
        "     file.extractall(path=\"data\")\n",
        "# Load the list of labels from a JSON file\n",
        "with open(\"./data/Files/cifar100_fine_labels.json\", \"r\") as fine_labels:\n",
        "    cifar100_fine_labels = json.load(fine_labels)\n",
        "\n",
        "# Display a few examples\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(9, 3))\n",
        "for i, elem in enumerate(dataset.take(count=3)):\n",
        "    image, label = elem[0], elem[1].numpy().item()\n",
        "    ax[i].imshow(X=image)\n",
        "    ax[i].set_title(label=cifar100_fine_labels[label])\n",
        "    ax[i].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTVd9oefhaA4"
      },
      "source": [
        "#### Load the data using different label modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuXemjRYhaA-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Reload the data using the 'coarse' label mode\n",
        "(train_images, train_labels), (test_images, test_labels) = \\\n",
        "    tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "print(f\"train images shape: {train_images.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9cNRdPYhaBL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "# Load the list of coarse labels from a JSON file\n",
        "with open('./data/cifar100_coarse_labels.json', 'r') as coarse_labels:\n",
        "    cifar100_coarse_labels = json.load(coarse_labels)\n",
        "\n",
        "# Print a few of the labels\n",
        "cifar100_coarse_labels[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIp_vgFShaBG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display three images from the dataset with the label (index 5)\n",
        "examples = train_images[(train_labels.T==5)[0]][:3]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
        "for i, img in enumerate(examples):\n",
        "    ax[i].imshow(X=img)\n",
        "    # Print the corresponding label for the example above\n",
        "    ax[i].set_title(label=cifar100_coarse_labels[5])\n",
        "    ax[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sa08-SVhaBY"
      },
      "source": [
        "#### Load the IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm1b7aw6haBb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data()\n",
        "\n",
        "# Print the shape of the training dataset, along with its corresponding label\n",
        "print(f\"train data shape: {train_data.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")\n",
        "\n",
        "# Get the lengths of the input sequences\n",
        "sequence_lengths = [len(seq) for seq in train_data]\n",
        "\n",
        "# Determine the maximum and minimum sequence length\n",
        "print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {np.min(sequence_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q76xgLEHhaBt"
      },
      "source": [
        "#### Using Keyword Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXXALxPhaBu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the data ignoring the 50 most frequent words,\n",
        "# use oov_char=2 (this is the default)\n",
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data(skip_top=50, oov_char=2)\n",
        "\n",
        "print(f\"train data shape: {train_data.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")\n",
        "\n",
        "# Get the lengths of the input sequences\n",
        "sequence_lengths = [len(seq) for seq in train_data]\n",
        "\n",
        "# Determine the maximum and minimum sequence length\n",
        "print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {np.min(sequence_lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVAxwvpNhaB5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define functions for filtering the sequences\n",
        "def remove_oov_char(element):\n",
        "    \"\"\" Filter function for removing the oov_char. \"\"\"\n",
        "    return [word for word in element if word!=2]\n",
        "\n",
        "def filter_list(lst):\n",
        "    \"\"\" Run remove_oov_char on elements in a list. \"\"\"\n",
        "    return [remove_oov_char(element) for element in lst]\n",
        "\n",
        "# Remove the oov_char from the sequences using the filter_list function\n",
        "train_data = filter_list(train_data)\n",
        "\n",
        "# Get the lengths of the input sequences\n",
        "sequence_lengths = [len(seq) for seq in train_data]\n",
        "\n",
        "# Determine the maximum and minimum sequence length\n",
        "print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {np.min(sequence_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jdrQ2I6haCG"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Dataset generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li0tJhkQhaCH"
      },
      "outputs": [],
      "source": [
        "def text_file_reader(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for row in f:\n",
        "            yield row\n",
        "\n",
        "text_datagen = text_file_reader('data_file.txt')\n",
        "\n",
        "next(text_datagen) # 'A line of text\\n'\n",
        "next(text_datagen) # 'Another line of text\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFE86mgluqYX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_data(batch_size):\n",
        "    while True:\n",
        "        y_train = np.random.choice([0, 1], (batch_size, 1))\n",
        "        x_train = np.random.randn(batch_size, 1) + (2 * y_train - 1)\n",
        "        yield x_train, y_train\n",
        "\n",
        "datagen = get_data(32)\n",
        "\n",
        "x, y = next(datagen)\n",
        "print(f\"x train shape: {x.shape}\")\n",
        "print(f\"y train shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEHNIf43VrMY",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.SGD()\n",
        ")\n",
        "history = model.fit(datagen,\n",
        "    steps_per_epoch=1000, epochs=10,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(history.epoch, history.history['loss'], label='Train')\n",
        "ax.set_title('Loss vs. Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "datagen_eval = get_data(32)\n",
        "model.evaluate(datagen_eval, steps=100, verbose=2)\n",
        "\n",
        "datagen_test = get_data(32)\n",
        "model.predict(datagen_test, steps=100, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fKVUVRGc_my"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.SGD()\n",
        ")\n",
        "\n",
        "losses = []\n",
        "for _ in range(10000):\n",
        "    x_train, y_train = next(datagen)\n",
        "    loss = model.train_on_batch(x=x_train, y=y_train)\n",
        "    losses.append(loss)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(losses, label='Train')\n",
        "ax.set_title('Loss vs. Batch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Batch')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GVfDxrvhaCK"
      },
      "source": [
        "#### Load the UCI Fertility Dataset\n",
        "\n",
        "We will be using a dataset available at https://archive.ics.uci.edu/ml/datasets/Fertility from UC Irvine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH5sTyYUbfic"
      },
      "outputs": [],
      "source": [
        "os.makedirs('./data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIlGWFwohaCL"
      },
      "outputs": [],
      "source": [
        "# Load the fertility dataset\n",
        "\n",
        "headers = ['Season', 'Age', 'Diseases', 'Trauma', 'Surgery', 'Fever',\n",
        "    'Alcohol', 'Smoking', 'Sitting', 'Output']\n",
        "fertility = pd.read_csv('./data/fertility_diagnosis.txt', delimiter=',',\n",
        "    header=None, names=headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ynOs8o8haCO"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the DataFrame\n",
        "\n",
        "print(fertility.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9xM_0JhhaCR"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpewphL7haCX"
      },
      "source": [
        "#### Process the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaKHnUrfhaCY"
      },
      "outputs": [],
      "source": [
        "# Map the 'Output' feature from 'N' to 0 and from 'O' to 1\n",
        "\n",
        "fertility['Output'] = fertility['Output'].map(lambda x : 0.0 if x=='N' else 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2wRf1ufhaCc"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXVN-S0NhaCh"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame so that the features are mapped to floats\n",
        "\n",
        "fertility = fertility.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyqlEHm8haCj"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame\n",
        "\n",
        "fertility = fertility.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwQxZk0thaCl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRJDa_FNhaCp"
      },
      "outputs": [],
      "source": [
        "# Convert the field Season to a one-hot encoded vector\n",
        "\n",
        "fertility = pd.get_dummies(fertility, prefix='Season', columns=['Season'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69RqZRfhhaCr"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zB6FM2EhaCt"
      },
      "outputs": [],
      "source": [
        "# Move the Output column such that it is the last column in the DataFrame\n",
        "\n",
        "fertility.columns = [col for col in fertility.columns if col != 'Output'] + ['Output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMivX3lGhaCw"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhZ1-N2ihaCz"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame to a numpy array.\n",
        "\n",
        "fertility = fertility.to_numpy()\n",
        "fertility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qaw2BjYhaC1"
      },
      "source": [
        "#### Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e612AYyahaC3"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation set\n",
        "\n",
        "training = fertility[0:70]\n",
        "validation = fertility[70:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8vBQHLqhaC7"
      },
      "outputs": [],
      "source": [
        "# Verify the shape of the training data\n",
        "\n",
        "print(training.shape)\n",
        "print(validation.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GlYDYVohaC9"
      },
      "outputs": [],
      "source": [
        "# Separate the features and labels for the validation and training data\n",
        "\n",
        "training_features = training[:,0:-1]\n",
        "training_labels = training[:,-1]\n",
        "validation_features = validation[:,0:-1]\n",
        "validation_labels = validation[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttEpRpcwhaC_"
      },
      "source": [
        "#### Create the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSjVXicihaC_"
      },
      "outputs": [],
      "source": [
        "# Create a function that returns a generator producing inputs and labels\n",
        "\n",
        "def get_generator(features, labels, batch_size=1):\n",
        "    for n in range(int(len(features)/batch_size)):\n",
        "        yield (\n",
        "            features[n*batch_size: (n+1)*batch_size],\n",
        "            labels[n*batch_size: (n+1)*batch_size]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMiEgHFuhaDC"
      },
      "outputs": [],
      "source": [
        "# Apply the function to our training features and labels with a batch size of 10\n",
        "\n",
        "train_generator = get_generator(training_features, training_labels, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qqfSAXWhaDG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test the generator using the next() function\n",
        "\n",
        "x, y = next(train_generator)\n",
        "print(f\"x train shape: {x.shape}\")\n",
        "print(f\"y train shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2ffnGVhaDJ"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr6Du-4KhaDJ"
      },
      "outputs": [],
      "source": [
        "# Create a model using Keras with 3 layers\n",
        "\n",
        "input_shape = (12,)\n",
        "output_shape = (1,)\n",
        "\n",
        "def get_model(\n",
        "    input_shape=input_shape,\n",
        "    output_shape=output_shape\n",
        "):\n",
        "    model_input = tf.keras.Input(shape=input_shape)\n",
        "    batch_1 = tf.keras.layers.BatchNormalization(momentum=0.8)(model_input)\n",
        "    dense_1 = tf.keras.layers.Dense(units=100, activation='relu')(batch_1)\n",
        "    batch_2 = tf.keras.layers.BatchNormalization(momentum=0.8)(dense_1)\n",
        "    output = tf.keras.layers.Dense(units=1, activation='sigmoid')(batch_2)\n",
        "\n",
        "    model = tf.keras.Model(inputs=model_input, outputs=output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfgcKE8UhaDN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Display the model summary to show the resultant structure\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUYrDLaAhaDY"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Fn78YchaDb"
      },
      "outputs": [],
      "source": [
        "# Compile the model with optimizer object, loss function and metric\n",
        "\n",
        "def compile_model(model):\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09uEqTpEhaDg"
      },
      "source": [
        "#### Train and evaluate the model using the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXwGCGeIhaDh"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of training steps per epoch for the given batch size.\n",
        "\n",
        "batch_size = 5\n",
        "train_steps = len(training) // batch_size\n",
        "print(f\"train steps: {train_steps:0d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszYL-_EhaDj"
      },
      "outputs": [],
      "source": [
        "# Set the epochs to 3\n",
        "\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0iYqScZhaDm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "model = get_model()\n",
        "compile_model(model)\n",
        "\n",
        "dfs_history = []\n",
        "for epoch in range(epochs):\n",
        "    train_generator = get_generator(training_features,\n",
        "        training_labels, batch_size=batch_size)\n",
        "    validation_generator = get_generator(validation_features,\n",
        "        validation_labels, batch_size=30)\n",
        "    history = model.fit(train_generator, steps_per_epoch=train_steps,\n",
        "        validation_data=validation_generator, validation_steps=1,\n",
        "        verbose=2)\n",
        "    dfs_history.append(pd.DataFrame(history.history))\n",
        "\n",
        "df_history = pd.concat(dfs_history)\\\n",
        "    .reset_index().drop(labels='index', axis=1)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'binary_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    try:\n",
        "        sns.lineplot(ax=ax, data=df_history,\n",
        "            x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
        "    except Exception:\n",
        "        pass\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0kQUErOhaDn",
        "scrolled": true,
        "tags": [],
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Try to run the fit_generator function once more; observe what happens\n",
        "\n",
        "model = get_model()\n",
        "compile_model(model)\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=train_steps,\n",
        "    validation_data=validation_generator, validation_steps=1,\n",
        "    verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-fqtd11haDq"
      },
      "source": [
        "#### Make an infinitely looping generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMVW0cn6haDr"
      },
      "outputs": [],
      "source": [
        "# Create a function that returns an infinitely looping generator\n",
        "\n",
        "def get_generator_cyclic(features, labels, batch_size=1):\n",
        "    while True:\n",
        "        for n in range(int(len(features)/batch_size)):\n",
        "            yield (\n",
        "                features[n*batch_size: (n+1)*batch_size],\n",
        "                labels[n*batch_size: (n+1)*batch_size]\n",
        "            )\n",
        "        permuted = np.random.permutation(len(features))\n",
        "        features = features[permuted]\n",
        "        labels = labels[permuted]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlWgTbiMhaDv"
      },
      "outputs": [],
      "source": [
        "# Create a generator using this function.\n",
        "\n",
        "train_generator_cyclic = get_generator_cyclic(\n",
        "    training_features, training_labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLH7308FhaDz",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Assert that the new cyclic generator does not raise a StopIteration\n",
        "\n",
        "for i in range(2*train_steps):\n",
        "    x, y = next(train_generator_cyclic)\n",
        "    print(f\"{i}, x shape: {x.shape}, y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcu6MbjwhaD2"
      },
      "outputs": [],
      "source": [
        "# Generate a cyclic validation generator\n",
        "\n",
        "validation_generator_cyclic = get_generator_cyclic(\n",
        "    validation_features, validation_labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d-hqzGThaD4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "model = get_model()\n",
        "compile_model(model)\n",
        "\n",
        "history = model.fit(train_generator_cyclic, steps_per_epoch=train_steps,\n",
        "    validation_data=validation_generator_cyclic, validation_steps=1,\n",
        "    epochs=3, verbose=2)\n",
        "\n",
        "df_history = pd.DataFrame(history.history,\n",
        "    index=history.epoch)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'binary_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    try:\n",
        "        sns.lineplot(ax=ax, data=df_history,\n",
        "            x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
        "    except Exception:\n",
        "        pass\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX3wHyf-haD7"
      },
      "source": [
        "#### Evaluate the model and get predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZPnBbc4haD7"
      },
      "outputs": [],
      "source": [
        "# Let's obtain a validation data generator.\n",
        "\n",
        "validation_generator_cyclic = get_generator_cyclic(\n",
        "    validation_features, validation_labels, batch_size=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH7FnT8nhaD9"
      },
      "outputs": [],
      "source": [
        "# Get predictions on the validation data\n",
        "\n",
        "predictions = model.predict(validation_generator_cyclic,\n",
        "    steps=1, verbose=0)\n",
        "print(np.round(predictions.T[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfDxaO80haD_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Print the corresponding validation labels\n",
        "\n",
        "print(validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaJi2CvghaEE"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "evaluation = model.evaluate(\n",
        "    validation_generator_cyclic, steps=1, verbose=0)\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MulkCc1FhaEN"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Keras image data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8G8N_88haEQ"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "image_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255., horizontal_flip=True, height_shift_range=0.2,\n",
        "    fill_mode='nearest',featurewise_center=True) # aaaa|abcd|dddd\n",
        "\n",
        "image_data_gen.fit(x_train)\n",
        "\n",
        "train_datagen = image_data_gen.flow(\n",
        "    x_train, y_train, batch_size=16)\n",
        "\n",
        "model.fit(train_datagen, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awp0hKRDhaER"
      },
      "source": [
        "#### Load the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOd1YbsJhaEU"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "\n",
        "(training_features, training_labels), (test_features, test_labels) = \\\n",
        "    tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsAjqqnjhaEV"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to a one-hot encoding\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "training_labels = tf.keras.utils.to_categorical(\n",
        "    y=training_labels, num_classes=num_classes)\n",
        "test_labels = tf.keras.utils.to_categorical(\n",
        "    y=test_labels, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNrlxnRlhaEX"
      },
      "source": [
        "#### Create a generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieho8OrohaEY"
      },
      "outputs": [],
      "source": [
        "# Create a function that returns a data generator\n",
        "\n",
        "def get_generator(features, labels, batch_size=1):\n",
        "    for n in range(int(len(features)/batch_size)):\n",
        "        yield (\n",
        "            features[n*batch_size: (n+1)*batch_size],\n",
        "            labels[n*batch_size: (n+1)*batch_size]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kk4Z87MhaEa"
      },
      "outputs": [],
      "source": [
        "# Use the function we created to get a training data generator with a batch size of 1\n",
        "\n",
        "training_generator = get_generator(\n",
        "    training_features, training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUgGGmjOhaEb"
      },
      "outputs": [],
      "source": [
        "# Assess the shape of the items generated by training_generator using the\n",
        "# `next` function to yield an item.\n",
        "\n",
        "image, label = next(training_generator)\n",
        "print(f\"image shape: {image.shape}\")\n",
        "print(f\"label shape: {label.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEqA1uDAhaEd"
      },
      "outputs": [],
      "source": [
        "# Test the training generator by obtaining an image using the\n",
        "# `next` generator function, and then using imshow to plot it.\n",
        "# Print the corresponding label\n",
        "\n",
        "image, label = next(training_generator)\n",
        "image_unbatched = np.squeeze(image)\n",
        "plt.imshow(image_unbatched)\n",
        "plt.title(str(label))\n",
        "plt.grid(visible=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhNS0N5khaEf"
      },
      "outputs": [],
      "source": [
        "# Reset the generator by re-running the `get_generator` function.\n",
        "\n",
        "train_generator = get_generator(\n",
        "    training_features, training_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh41TDl2haEh"
      },
      "source": [
        "#### Create a data augmention generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKyZQms7haEk"
      },
      "outputs": [],
      "source": [
        "# Create a function to convert an image to monochrome\n",
        "\n",
        "def monochrome(x):\n",
        "    def func_bw(a):\n",
        "        average_colour = np.mean(a)\n",
        "        return [average_colour, average_colour, average_colour]\n",
        "    x = np.apply_along_axis(func_bw, -1, x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxuo_9aAhaEn"
      },
      "outputs": [],
      "source": [
        "# Create an ImageDataGenerator object\n",
        "\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=monochrome,\n",
        "    rotation_range=180, rescale=1/255.0)\n",
        "\n",
        "image_generator.fit(training_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRwjDb69haEq"
      },
      "source": [
        "Check [the documentation](https://keras.io/preprocessing/image/) for the full list of image data augmentation options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRnwugLChaEr"
      },
      "outputs": [],
      "source": [
        "# Create an iterable generator using the `flow` function\n",
        "\n",
        "image_generator_iterable = image_generator.flow(\n",
        "    training_features, training_labels,\n",
        "    batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2bR1hpHhaEs"
      },
      "outputs": [],
      "source": [
        "# Show a sample from the generator and compare with the original\n",
        "\n",
        "image, label = next(image_generator_iterable)\n",
        "image_orig, label_orig = next(train_generator)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(np.squeeze(image))\n",
        "ax[0].set_title('Transformed')\n",
        "ax[0].grid(visible=None)\n",
        "ax[1].imshow(np.squeeze(image_orig))\n",
        "ax[1].set_title('Original')\n",
        "ax[1].grid(visible=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZN3SD04haEt"
      },
      "source": [
        "#### Flow from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW7NZ379eczt"
      },
      "outputs": [],
      "source": [
        "os.makedirs('./data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS8Aj9o7haEu"
      },
      "outputs": [],
      "source": [
        "# Inspect the directory structure\n",
        "\n",
        "train_path = 'data/flowers-recognition-split/train'\n",
        "val_path = 'data/flowers-recognition-split/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0teFy5XhaEw"
      },
      "outputs": [],
      "source": [
        "# Create an ImageDataGenerator object\n",
        "\n",
        "datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8F4Hv6OhaEy"
      },
      "outputs": [],
      "source": [
        "classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43SCIXOohaEz"
      },
      "outputs": [],
      "source": [
        "# Create a training data generator\n",
        "\n",
        "train_generator = datagenerator.flow_from_directory(\n",
        "    directory=train_path, batch_size=64, classes=classes,\n",
        "    target_size=(16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNduitYAhaE1"
      },
      "outputs": [],
      "source": [
        "# Create a validation data generator\n",
        "\n",
        "val_generator = datagenerator.flow_from_directory(\n",
        "    directory=val_path, batch_size=64, classes=classes,\n",
        "    target_size=(16,16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzYyqoVhaE8"
      },
      "outputs": [],
      "source": [
        "# Get and display an image and label from the training generator\n",
        "\n",
        "x = next(train_generator)\n",
        "plt.imshow(x[0][8])\n",
        "plt.title(str(x[1][8]))\n",
        "plt.grid(visible=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hawvnO6-haE9"
      },
      "outputs": [],
      "source": [
        "# Reset the training generator\n",
        "\n",
        "train_generator = datagenerator.flow_from_directory(\n",
        "    directory=train_path, batch_size=64, classes=classes,\n",
        "    target_size=(16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSWrrJRGhaE_"
      },
      "source": [
        "#### Create a model to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLteymJqhaE_"
      },
      "outputs": [],
      "source": [
        "# Build a CNN model\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(16, 16, 3)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(8, 8),\n",
        "    padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(8, 8),\n",
        "    padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=4, kernel_size=(4, 4),\n",
        "    padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ScUILYhaFB"
      },
      "outputs": [],
      "source": [
        "# Create an optimizer object\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxqLi4jrhaFC"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KogBiFshaFE"
      },
      "outputs": [],
      "source": [
        "# Print the model summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqwr_ORZhaFG"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDyE8_XchaFH"
      },
      "outputs": [],
      "source": [
        "# Calculate the training generator and test generator steps per epoch\n",
        "\n",
        "train_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "val_steps = val_generator.n // val_generator.batch_size\n",
        "print(f\"train steps per epoch: {train_steps_per_epoch}\\\n",
        "    \\nvalid steps: {val_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRmd7ZDDhaFI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch, epochs=5,\n",
        "    verbose=2)\n",
        "\n",
        "df_history = pd.DataFrame(history.history, index=history.epoch)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'categorical_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da9gQT5BhaFL"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWkUOT4fhaFM"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "loss, accuracy = model.evaluate(\n",
        "    val_generator, steps=val_steps, verbose=0)\n",
        "print(f\"loss: {loss:.2f}, accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRD2u5UhaFN"
      },
      "source": [
        "#### Predict using the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de5vOjAjhaFN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Predict labels with the model\n",
        "\n",
        "predictions = model.predict(val_generator, steps=1, verbose=0)\n",
        "print([classes[i] for i in np.argmax(predictions, axis=1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7sr3v1haFP"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tensors=(\n",
        "    tf.random.uniform(shape=(256, 4), minval=1, maxval=10, dtype=tf.int32),\n",
        "    tf.random.normal(shape=(256,))\n",
        "))\n",
        "print(dataset.element_spec)\n",
        "\n",
        "for x, y in dataset.take(count=2):\n",
        "    print(x.numpy(), y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yj9PF9WFCu2",
        "outputId": "706fbc27-df50-49ae-d27d-f18e7c1bea43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(4,), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))\n",
            "[8 6 9 4] 0.08422458\n",
            "[8 3 8 8] -0.86090374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM8-2uhNeXcc",
        "outputId": "e594a476-acf9-4445-bacc-8b95a155cee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1,), dtype=tf.uint8, name=None))\n",
            "(TensorSpec(shape=(32, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.int32, name=None))\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = \\\n",
        "    tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(x_train, y_train))\n",
        "print(dataset.element_spec)\n",
        "\n",
        "img_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    width_shift_range=0.2, horizontal_flip=True)\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    generator=img_datagen.flow, args=[x_train, y_train],\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=((32, 32, 32, 3), (32, 1))\n",
        ")\n",
        "print(dataset.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_eojOchaFu"
      },
      "source": [
        "#### Create a zipped dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6tuzoM-ehaFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c153a024-c4f8-482b-9aec-5557532ab372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None), (TensorSpec(shape=(2, 2), dtype=tf.float64, name=None), TensorSpec(shape=(1,), dtype=tf.float64, name=None)))\n",
            "Number of batches: 10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Create a dataset from the tensor x\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=np.zeros(shape=(100, 10, 2, 2)))\n",
        "\n",
        "# Create another dataset from the tensor x2\n",
        "# and inspect the Dataset object\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(tensors=(\n",
        "    np.zeros(shape=(10, 2, 2)), np.zeros(shape=(10, 1))\n",
        "))\n",
        "\n",
        "# Combine the two datasets into one larger dataset\n",
        "dataset_zipped = tf.data.Dataset.zip(\n",
        "    datasets=(dataset1, dataset2))\n",
        "# Inspect the dataset object\n",
        "print(dataset_zipped.element_spec)\n",
        "\n",
        "# Define a function to find the number of batches in a dataset\n",
        "def get_batches(dataset):\n",
        "    iter_dataset = iter(dataset)\n",
        "    i = 0\n",
        "    try:\n",
        "        while next(iter_dataset):\n",
        "            i = i+1\n",
        "    except:\n",
        "        return i\n",
        "\n",
        "# Find the number of batches in the zipped Dataset\n",
        "print(f\"Number of batches: {get_batches(dataset_zipped)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBmSO5lJhaF1"
      },
      "source": [
        "#### Create a dataset from numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za9ynOokhaF5",
        "outputId": "e48748e8-5851-4d64-e510-f649e8698b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n",
            "Length of an element: 2\n",
            "x train shape: (28, 28)\n",
            "y train shape: ()\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_features, train_labels), (test_features, test_labels) = \\\n",
        "    tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Create a Dataset from the MNIST data\n",
        "mnist_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(train_features, train_labels))\n",
        "# Inspect the Dataset object\n",
        "print(mnist_dataset.element_spec)\n",
        "\n",
        "# Inspect the length of an element using the take method\n",
        "element = next(iter(mnist_dataset.take(count=1)))\n",
        "print(f\"Length of an element: {len(element)}\")\n",
        "\n",
        "# Examine the shapes of the data\n",
        "print(f\"x train shape: {element[0].shape}\")\n",
        "print(f\"y train shape: {element[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzJe1CIvhaGB"
      },
      "source": [
        "#### Create a dataset from text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zSGZUSrh1Y5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(name=\"./data/shakespeare\", exist_ok=True)\n",
        "\n",
        "# Print the list of text files\n",
        "text_files = sorted([f.path for f in\n",
        "    os.scandir(path=\"./data/shakespeare\")])\n",
        "\n",
        "# Load the first file using python and print the first 5 lines.\n",
        "with open(text_files[0], 'r') as f:\n",
        "    contents = [f.readline() for _ in range(5)]\n",
        "    for line in contents:\n",
        "        print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZkNIUYnhaGE"
      },
      "outputs": [],
      "source": [
        "# Load the lines from the files into a dataset using TextLineDataset\n",
        "\n",
        "shakespeare_dataset = tf.data.TextLineDataset(\n",
        "    filenames=text_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbK2AV4WhaGG"
      },
      "outputs": [],
      "source": [
        "# Use the take method to get and print the first 5 lines of the dataset\n",
        "\n",
        "first_5_lines_dataset = iter(shakespeare_dataset.take(5))\n",
        "lines = [line for line in first_5_lines_dataset]\n",
        "for line in lines:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxr_d64jhaGH"
      },
      "outputs": [],
      "source": [
        "# Compute the number of lines in the first file\n",
        "\n",
        "lines = []\n",
        "with open(text_files[0], 'r') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        lines.append(line)\n",
        "        line = f.readline()\n",
        "    print(f\"Number of lines: {len(lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xj3mOiUhaGJ"
      },
      "outputs": [],
      "source": [
        "# Compute the number of lines in the shakespeare dataset we created\n",
        "\n",
        "shakespeare_dataset_iterator = iter(shakespeare_dataset)\n",
        "lines = [line for line in shakespeare_dataset_iterator]\n",
        "print(f\"Number of lines: {len(lines)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrXEQqa3haGK"
      },
      "source": [
        "#### Interleave lines from the text data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv7HQG9_haGK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a dataset of the text file strings\n",
        "\n",
        "text_files_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=text_files)\n",
        "files = [file for file in text_files_dataset]\n",
        "for file in files:\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXEK2VNQhaGM"
      },
      "outputs": [],
      "source": [
        "# Interleave the lines from the text files\n",
        "\n",
        "interleaved_shakespeare_dataset = text_files_dataset.interleave(\n",
        "    map_func=tf.data.TextLineDataset, cycle_length=9)\n",
        "print(interleaved_shakespeare_dataset.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywdbkvVJhaGO"
      },
      "outputs": [],
      "source": [
        "# Print the first 10 elements of the interleaved dataset\n",
        "\n",
        "lines = [line for line in\n",
        "    iter(interleaved_shakespeare_dataset.take(10))]\n",
        "for line in lines:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5CRpW_whaGQ"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## Training with Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGGtQCM2wxc4"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(x_train, y_train))\n",
        "dataset = dataset.batch(batch_size=16, drop_remainder=True)\n",
        "print(dataset.element_spec)\n",
        "dataset = dataset.repeat(count=10)\n",
        "\n",
        "history = model.fit(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDN2uIRWwxc5"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(x_train, y_train))\n",
        "\n",
        "def rescale(image, label):\n",
        "    return image/255., label\n",
        "\n",
        "dataset = dataset.map(map_func=rescale)\n",
        "\n",
        "def label_filter(image, label):\n",
        "    return tf.squeeze(label)!=9\n",
        "\n",
        "dataset = dataset.filter(predicate=label_filter)\n",
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "dataset = dataset.batch(batch_size=16, drop_remainder=True)\n",
        "print(dataset.element_spec)\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "history = model.fit(dataset, steps_per_epoch=x_train.shape[0]//16,\n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sImBVH74haGR"
      },
      "source": [
        "#### Load the UCI Bank Marketing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jwLM9ijlgYY"
      },
      "outputs": [],
      "source": [
        "os.makedirs(name=\"./data/bank\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRt2GoekhaGS"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "\n",
        "bank_dataframe = pd.read_csv('./data/bank/bank-full.csv', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKUSID04haGT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "bank_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNobGyshaGW"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the DataFrame\n",
        "\n",
        "print(f\"data shape: {bank_dataframe.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV-zDyNdhaGX"
      },
      "outputs": [],
      "source": [
        "# Select features from the DataFrame\n",
        "\n",
        "features = ['age', 'job', 'marital', 'education', 'default', 'balance',\n",
        "    'housing', 'loan', 'contact', 'campaign', 'pdays', 'poutcome']\n",
        "labels = ['y']\n",
        "\n",
        "bank_dataframe = bank_dataframe.filter(features + labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qxNd34thaGY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "print(f\"data shape: {bank_dataframe.shape}\")\n",
        "bank_dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMOyO0aZhaGa"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIwMmt9KhaGa"
      },
      "outputs": [],
      "source": [
        "# Convert the categorical features in the DataFrame to one-hot encodings\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "categorical_features = ['default', 'housing', 'job', 'loan',\n",
        "    'education', 'contact', 'poutcome']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    bank_dataframe[feature] = tuple(encoder.fit_transform(\n",
        "        bank_dataframe[feature]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbwWaUKEhaGb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "print(bank_dataframe.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXIuiANFhaGc"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame\n",
        "\n",
        "bank_dataframe = bank_dataframe.sample(frac=1)\\\n",
        "    .reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrI8QPAghaGe"
      },
      "source": [
        "#### Create the Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyWS07bghaGf"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame to a Dataset\n",
        "\n",
        "dict_bank = dict(bank_dataframe)\n",
        "bank_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors={k: dict_bank[k].tolist() for k in dict_bank})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ykb1yAlhaGn"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OULr3jShaGq"
      },
      "source": [
        "#### Filter the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJwAeSuwhaGr"
      },
      "outputs": [],
      "source": [
        "# First check that there are records in the dataset for non-married individuals\n",
        "\n",
        "def check_divorced():\n",
        "    bank_dataset_iterable = iter(bank_dataset)\n",
        "    for x in bank_dataset_iterable:\n",
        "        if x['marital'] != 'divorced':\n",
        "            print('Found a person with marital status: {}'.format(x['marital']))\n",
        "            return\n",
        "    print('No non-divorced people were found!')\n",
        "\n",
        "check_divorced()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixTiUgcShaGt"
      },
      "outputs": [],
      "source": [
        "# Filter the Dataset to retain only entries with a 'divorced' marital status\n",
        "\n",
        "bank_dataset = bank_dataset.filter(predicate=lambda x:\n",
        "    tf.equal(x['marital'], tf.constant([b'divorced']))[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATDI5_73haGu"
      },
      "outputs": [],
      "source": [
        "# Check the records in the dataset again\n",
        "\n",
        "check_divorced()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZQjW6MGhaGv"
      },
      "source": [
        "#### Map a function over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__3Jjfl7haGw"
      },
      "outputs": [],
      "source": [
        "# Convert the label ('y') to an integer instead of 'yes' or 'no'\n",
        "\n",
        "def map_label(x):\n",
        "    x['y'] = 0 if (x['y']==tf.constant([b'no'], dtype=tf.string)) else 1\n",
        "    return x\n",
        "\n",
        "bank_dataset = bank_dataset.map(map_func=map_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60G0f9YGhaGx"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGpolkUJhaGy"
      },
      "outputs": [],
      "source": [
        "# Remove the 'marital' column\n",
        "\n",
        "bank_dataset = bank_dataset.map(map_func=lambda x:\n",
        "    {key: val for key, val in x.items() if key!='marital'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTbrVE7chaG0"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlYUoVylhaG1"
      },
      "source": [
        "#### Create input and output data tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIsxi_u0haG1"
      },
      "outputs": [],
      "source": [
        "# Create an input and output tuple for the dataset\n",
        "\n",
        "def map_feature_label(x):\n",
        "    features = [[x['age']], [x['balance']], [x['campaign']], x['contact'],\n",
        "        x['default'], x['education'], x['housing'], x['job'], x['loan'],\n",
        "        [x['pdays']], x['poutcome']]\n",
        "    return (tf.concat(features, axis=0), x['y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIzfxziMhaG3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Map this function over the dataset\n",
        "\n",
        "bank_dataset = bank_dataset.map(map_func=map_feature_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JKhkqaEhaHB"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTg7cxYVhaHC"
      },
      "source": [
        "#### Split into a training and a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpa2OGmxhaHC"
      },
      "outputs": [],
      "source": [
        "# Determine the length of the Dataset\n",
        "\n",
        "dataset_length = 0\n",
        "for _ in bank_dataset:\n",
        "    dataset_length += 1\n",
        "print(f\"dataset length: {dataset_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJ5z3chwxc9"
      },
      "outputs": [],
      "source": [
        "bank_dataset_iterator = iter(bank_dataset)\n",
        "dataset_length = 0\n",
        "try:\n",
        "    while next(bank_dataset_iterator):\n",
        "        dataset_length += 1\n",
        "except Exception:\n",
        "    pass\n",
        "print(f\"dataset length: {dataset_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiBen_r1haHD"
      },
      "outputs": [],
      "source": [
        "# Make training and validation sets from the dataset\n",
        "\n",
        "training_elements = int(dataset_length * 0.7)\n",
        "train_dataset = bank_dataset.take(count=training_elements)\n",
        "valid_dataset = bank_dataset.skip(count=training_elements)\n",
        "\n",
        "print(f\"train dataset element spec: {train_dataset.element_spec}\")\n",
        "print(f\"valid dataset element spec: {valid_dataset.element_spec}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj7Zs8yhhaHG"
      },
      "source": [
        "#### Build a classification model\n",
        "\n",
        "Now let's build a model to classify the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFJN8PaUhaHG"
      },
      "outputs": [],
      "source": [
        "# Build a classifier model\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(30,)))\n",
        "model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "model.add(tf.keras.layers.Dense(units=400, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "model.add(tf.keras.layers.Dense(units=400, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY92bQT-haHJ"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_p0gJvVhaHK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the model summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aALufpk6haHL"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIJanyhDhaHL"
      },
      "outputs": [],
      "source": [
        "# Create batched training and validation datasets\n",
        "\n",
        "train_dataset = train_dataset.batch(\n",
        "    batch_size=20, drop_remainder=True)\n",
        "print(f\"train dataset element spec: {train_dataset.element_spec}\")\n",
        "\n",
        "valid_dataset = valid_dataset.batch(\n",
        "    batch_size=100)\n",
        "print(f\"valid dataset element spec: {valid_dataset.element_spec}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xNHeIlthaHN"
      },
      "outputs": [],
      "source": [
        "# Shuffle the training data\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7daJ6eOhaHO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=valid_dataset,\n",
        "    epochs=5, verbose=0)\n",
        "df_history = pd.DataFrame(history.history, index=history.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE7zoUwLhaHQ"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'binary_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    try:\n",
        "        sns.lineplot(ax=ax, data=df_history,\n",
        "            x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
        "    except Exception:\n",
        "        pass\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data_pipeline.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}