{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow2/Customise/DataPipeline/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwp-WlWbhZ__"
      },
      "source": [
        "# Data Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cBJq_RLhaAB"
      },
      "source": [
        " ## Coding tutorials\n",
        " #### [1. Keras datasets](#coding_tutorial_1)\n",
        " #### [2. Dataset generators](#coding_tutorial_2)\n",
        " #### [3. Keras image data augmentation](#coding_tutorial_3)\n",
        " #### [4. The Dataset class](#coding_tutorial_4)\n",
        " #### [5. Training with Datasets](#coding_tutorial_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWJ8FsVQhaAE"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## Keras datasets\n",
        "\n",
        "For a list of Keras datasets and documentation on recommended usage, see [this link](https://keras.io/datasets/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgnvqE4bhaAM"
      },
      "source": [
        "#### Load the CIFAR-100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvgpO_ithaAS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the CIFAR-100 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = \\\n",
        "    tf.keras.datasets.cifar100.load_data(label_mode=\"fine\")\n",
        "\n",
        "# Examine the shape of the data.\n",
        "print(f\"train images shape: {train_images.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")\n",
        "print(f\"test images shape: {test_images.shape}\")\n",
        "print(f\"test labels shape: {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf61wNi8haAg"
      },
      "source": [
        "#### Examine the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "638jOdbUhaAu"
      },
      "source": [
        "The list of labels for the CIFAR-100 dataset are available [here](https://www.cs.toronto.edu/~kriz/cifar.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaZS2ZqRhaAp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "# Load the list of labels from a JSON file\n",
        "with open(\"./data/cifar100_fine_labels.json\", \"r\") as fine_labels:\n",
        "    cifar100_fine_labels = json.load(fine_labels)\n",
        "\n",
        "# Print a few of the labels\n",
        "cifar100_fine_labels[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGJe--0AhaAk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Examine one of the images and its corresponding label\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.imshow(X=train_images[500])\n",
        "# Print the corresponding label for the example above\n",
        "plt.title(label=cifar100_fine_labels[train_labels[500].item()])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMfLUaBKhaA4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display a few examples from category (index 86) and the list of labels\n",
        "examples = train_images[(train_labels.T==86)[0]][:3]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
        "for i, img in enumerate(examples):\n",
        "    ax[i].imshow(X=img)\n",
        "    ax[i].set_title(label=cifar100_fine_labels[86])\n",
        "    ax[i].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTVd9oefhaA4"
      },
      "source": [
        "#### Load the data using different label modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuXemjRYhaA-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Reload the data using the 'coarse' label mode\n",
        "(train_images, train_labels), (test_images, test_labels) = \\\n",
        "    tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "print(f\"train images shape: {train_images.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9cNRdPYhaBL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "# Load the list of coarse labels from a JSON file\n",
        "with open('./data/cifar100_coarse_labels.json', 'r') as coarse_labels:\n",
        "    cifar100_coarse_labels = json.load(coarse_labels)\n",
        "\n",
        "# Print a few of the labels\n",
        "cifar100_coarse_labels[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIp_vgFShaBG",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display three images from the dataset with the label (index 5)\n",
        "examples = train_images[(train_labels.T==5)[0]][:3]\n",
        "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
        "for i, img in enumerate(examples):\n",
        "    ax[i].imshow(X=img)\n",
        "    # Print the corresponding label for the example above\n",
        "    ax[i].set_title(label=cifar100_coarse_labels[5])\n",
        "    ax[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sa08-SVhaBY"
      },
      "source": [
        "#### Load the IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm1b7aw6haBb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data()\n",
        "\n",
        "# Print the shape of the training dataset, along with its corresponding label\n",
        "print(f\"train data shape: {train_data.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")\n",
        "\n",
        "# Get the lengths of the input sequences\n",
        "sequence_lengths = [len(seq) for seq in train_data]\n",
        "\n",
        "# Determine the maximum and minimum sequence length\n",
        "print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {np.min(sequence_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q76xgLEHhaBt"
      },
      "source": [
        "#### Using Keyword Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXXALxPhaBu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the data ignoring the 50 most frequent words,\n",
        "# use oov_char=2 (this is the default)\n",
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data(skip_top=50, oov_char=2)\n",
        "\n",
        "print(f\"train data shape: {train_data.shape}\")\n",
        "print(f\"train labels shape: {train_labels.shape}\")\n",
        "\n",
        "# Get the lengths of the input sequences\n",
        "sequence_lengths = [len(seq) for seq in train_data]\n",
        "\n",
        "# Determine the maximum and minimum sequence length\n",
        "print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {np.min(sequence_lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVAxwvpNhaB5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define functions for filtering the sequences\n",
        "def remove_oov_char(element):\n",
        "    \"\"\" Filter function for removing the oov_char. \"\"\"\n",
        "    return [word for word in element if word!=2]\n",
        "\n",
        "def filter_list(lst):\n",
        "    \"\"\" Run remove_oov_char on elements in a list. \"\"\"\n",
        "    return [remove_oov_char(element) for element in lst]\n",
        "\n",
        "# Remove the oov_char from the sequences using the filter_list function\n",
        "train_data = filter_list(train_data)\n",
        "\n",
        "# Get the lengths of the input sequences\n",
        "sequence_lengths = [len(seq) for seq in train_data]\n",
        "\n",
        "# Determine the maximum and minimum sequence length\n",
        "print(f\"Max sequence length: {np.max(sequence_lengths)}\")\n",
        "print(f\"Min sequence length: {np.min(sequence_lengths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jdrQ2I6haCG"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Dataset generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li0tJhkQhaCH"
      },
      "outputs": [],
      "source": [
        "def text_file_reader(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        for row in f:\n",
        "            yield row\n",
        "\n",
        "text_datagen = text_file_reader('data_file.txt')\n",
        "\n",
        "next(text_datagen) # 'A line of text\\n'\n",
        "next(text_datagen) # 'Another line of text\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFE86mgluqYX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_data(batch_size):\n",
        "    while True:\n",
        "        y_train = np.random.choice([0, 1], (batch_size, 1))\n",
        "        x_train = np.random.randn(batch_size, 1) + (2 * y_train - 1)\n",
        "        yield x_train, y_train\n",
        "\n",
        "datagen = get_data(32)\n",
        "\n",
        "x, y = next(datagen)\n",
        "print(f\"x train shape: {x.shape}\")\n",
        "print(f\"y train shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEHNIf43VrMY",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.SGD()\n",
        ")\n",
        "history = model.fit(datagen,\n",
        "    steps_per_epoch=1000, epochs=10,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(history.epoch, history.history['loss'], label='Train')\n",
        "ax.set_title('Loss vs. Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "datagen_eval = get_data(32)\n",
        "model.evaluate(datagen_eval, steps=100, verbose=2)\n",
        "\n",
        "datagen_test = get_data(32)\n",
        "model.predict(datagen_test, steps=100, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fKVUVRGc_my"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        ")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.SGD()\n",
        ")\n",
        "\n",
        "losses = []\n",
        "for _ in range(10000):\n",
        "    x_train, y_train = next(datagen)\n",
        "    loss = model.train_on_batch(x=x_train, y=y_train)\n",
        "    losses.append(loss)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(losses, label='Train')\n",
        "ax.set_title('Loss vs. Batch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_xlabel('Batch')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GVfDxrvhaCK"
      },
      "source": [
        "#### Load the UCI Fertility Dataset\n",
        "\n",
        "We will be using a dataset available at https://archive.ics.uci.edu/ml/datasets/Fertility from UC Irvine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH5sTyYUbfic"
      },
      "outputs": [],
      "source": [
        "os.makedirs('./data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIlGWFwohaCL"
      },
      "outputs": [],
      "source": [
        "# Load the fertility dataset\n",
        "\n",
        "headers = ['Season', 'Age', 'Diseases', 'Trauma', 'Surgery', 'Fever',\n",
        "    'Alcohol', 'Smoking', 'Sitting', 'Output']\n",
        "fertility = pd.read_csv('./data/fertility_diagnosis.txt', delimiter=',',\n",
        "    header=None, names=headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ynOs8o8haCO"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the DataFrame\n",
        "\n",
        "print(fertility.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9xM_0JhhaCR"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpewphL7haCX"
      },
      "source": [
        "#### Process the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaKHnUrfhaCY"
      },
      "outputs": [],
      "source": [
        "# Map the 'Output' feature from 'N' to 0 and from 'O' to 1\n",
        "\n",
        "fertility['Output'] = fertility['Output'].map(lambda x : 0.0 if x=='N' else 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2wRf1ufhaCc"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXVN-S0NhaCh"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame so that the features are mapped to floats\n",
        "\n",
        "fertility = fertility.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyqlEHm8haCj"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame\n",
        "\n",
        "fertility = fertility.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwQxZk0thaCl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRJDa_FNhaCp"
      },
      "outputs": [],
      "source": [
        "# Convert the field Season to a one-hot encoded vector\n",
        "\n",
        "fertility = pd.get_dummies(fertility, prefix='Season', columns=['Season'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69RqZRfhhaCr"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zB6FM2EhaCt"
      },
      "outputs": [],
      "source": [
        "# Move the Output column such that it is the last column in the DataFrame\n",
        "\n",
        "fertility.columns = [col for col in fertility.columns if col != 'Output'] + ['Output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMivX3lGhaCw"
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "fertility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhZ1-N2ihaCz"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame to a numpy array.\n",
        "\n",
        "fertility = fertility.to_numpy()\n",
        "fertility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qaw2BjYhaC1"
      },
      "source": [
        "#### Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e612AYyahaC3"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation set\n",
        "\n",
        "training = fertility[0:70]\n",
        "validation = fertility[70:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8vBQHLqhaC7"
      },
      "outputs": [],
      "source": [
        "# Verify the shape of the training data\n",
        "\n",
        "print(training.shape)\n",
        "print(validation.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GlYDYVohaC9"
      },
      "outputs": [],
      "source": [
        "# Separate the features and labels for the validation and training data\n",
        "\n",
        "training_features = training[:,0:-1]\n",
        "training_labels = training[:,-1]\n",
        "validation_features = validation[:,0:-1]\n",
        "validation_labels = validation[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttEpRpcwhaC_"
      },
      "source": [
        "#### Create the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSjVXicihaC_"
      },
      "outputs": [],
      "source": [
        "# Create a function that returns a generator producing inputs and labels\n",
        "\n",
        "def get_generator(features, labels, batch_size=1):\n",
        "    for n in range(int(len(features)/batch_size)):\n",
        "        yield (\n",
        "            features[n*batch_size: (n+1)*batch_size],\n",
        "            labels[n*batch_size: (n+1)*batch_size]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMiEgHFuhaDC"
      },
      "outputs": [],
      "source": [
        "# Apply the function to our training features and labels with a batch size of 10\n",
        "\n",
        "train_generator = get_generator(training_features, training_labels, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qqfSAXWhaDG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test the generator using the next() function\n",
        "\n",
        "x, y = next(train_generator)\n",
        "print(f\"x train shape: {x.shape}\")\n",
        "print(f\"y train shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2ffnGVhaDJ"
      },
      "source": [
        "#### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr6Du-4KhaDJ"
      },
      "outputs": [],
      "source": [
        "# Create a model using Keras with 3 layers\n",
        "\n",
        "input_shape = (12,)\n",
        "output_shape = (1,)\n",
        "\n",
        "def get_model(\n",
        "    input_shape=input_shape,\n",
        "    output_shape=output_shape\n",
        "):\n",
        "    model_input = tf.keras.Input(shape=input_shape)\n",
        "    batch_1 = tf.keras.layers.BatchNormalization(momentum=0.8)(model_input)\n",
        "    dense_1 = tf.keras.layers.Dense(units=100, activation='relu')(batch_1)\n",
        "    batch_2 = tf.keras.layers.BatchNormalization(momentum=0.8)(dense_1)\n",
        "    output = tf.keras.layers.Dense(units=1, activation='sigmoid')(batch_2)\n",
        "\n",
        "    model = tf.keras.Model(inputs=model_input, outputs=output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfgcKE8UhaDN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Display the model summary to show the resultant structure\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUYrDLaAhaDY"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Fn78YchaDb"
      },
      "outputs": [],
      "source": [
        "# Compile the model with optimizer object, loss function and metric\n",
        "\n",
        "def compile_model(model):\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09uEqTpEhaDg"
      },
      "source": [
        "#### Train and evaluate the model using the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXwGCGeIhaDh"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of training steps per epoch for the given batch size.\n",
        "\n",
        "batch_size = 5\n",
        "train_steps = len(training) // batch_size\n",
        "print(f\"train steps: {train_steps:0d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszYL-_EhaDj"
      },
      "outputs": [],
      "source": [
        "# Set the epochs to 3\n",
        "\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0iYqScZhaDm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "model = get_model()\n",
        "compile_model(model)\n",
        "\n",
        "dfs_history = []\n",
        "for epoch in range(epochs):\n",
        "    train_generator = get_generator(training_features,\n",
        "        training_labels, batch_size=batch_size)\n",
        "    validation_generator = get_generator(validation_features,\n",
        "        validation_labels, batch_size=30)\n",
        "    history = model.fit(train_generator, steps_per_epoch=train_steps,\n",
        "        validation_data=validation_generator, validation_steps=1,\n",
        "        verbose=2)\n",
        "    dfs_history.append(pd.DataFrame(history.history))\n",
        "\n",
        "df_history = pd.concat(dfs_history)\\\n",
        "    .reset_index().drop(labels='index', axis=1)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'binary_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    try:\n",
        "        sns.lineplot(ax=ax, data=df_history,\n",
        "            x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
        "    except Exception:\n",
        "        pass\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0kQUErOhaDn",
        "scrolled": true,
        "tags": [],
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Try to run the fit_generator function once more; observe what happens\n",
        "\n",
        "model = get_model()\n",
        "compile_model(model)\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=train_steps,\n",
        "    validation_data=validation_generator, validation_steps=1,\n",
        "    verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-fqtd11haDq"
      },
      "source": [
        "#### Make an infinitely looping generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMVW0cn6haDr"
      },
      "outputs": [],
      "source": [
        "# Create a function that returns an infinitely looping generator\n",
        "\n",
        "def get_generator_cyclic(features, labels, batch_size=1):\n",
        "    while True:\n",
        "        for n in range(int(len(features)/batch_size)):\n",
        "            yield (\n",
        "                features[n*batch_size: (n+1)*batch_size],\n",
        "                labels[n*batch_size: (n+1)*batch_size]\n",
        "            )\n",
        "        permuted = np.random.permutation(len(features))\n",
        "        features = features[permuted]\n",
        "        labels = labels[permuted]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlWgTbiMhaDv"
      },
      "outputs": [],
      "source": [
        "# Create a generator using this function.\n",
        "\n",
        "train_generator_cyclic = get_generator_cyclic(\n",
        "    training_features, training_labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLH7308FhaDz",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Assert that the new cyclic generator does not raise a StopIteration\n",
        "\n",
        "for i in range(2*train_steps):\n",
        "    x, y = next(train_generator_cyclic)\n",
        "    print(f\"{i}, x shape: {x.shape}, y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcu6MbjwhaD2"
      },
      "outputs": [],
      "source": [
        "# Generate a cyclic validation generator\n",
        "\n",
        "validation_generator_cyclic = get_generator_cyclic(\n",
        "    validation_features, validation_labels, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d-hqzGThaD4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "model = get_model()\n",
        "compile_model(model)\n",
        "\n",
        "history = model.fit(train_generator_cyclic, steps_per_epoch=train_steps,\n",
        "    validation_data=validation_generator_cyclic, validation_steps=1,\n",
        "    epochs=3, verbose=2)\n",
        "\n",
        "df_history = pd.DataFrame(history.history,\n",
        "    index=history.epoch)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'binary_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    try:\n",
        "        sns.lineplot(ax=ax, data=df_history,\n",
        "            x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
        "    except Exception:\n",
        "        pass\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX3wHyf-haD7"
      },
      "source": [
        "#### Evaluate the model and get predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZPnBbc4haD7"
      },
      "outputs": [],
      "source": [
        "# Let's obtain a validation data generator.\n",
        "\n",
        "validation_generator_cyclic = get_generator_cyclic(\n",
        "    validation_features, validation_labels, batch_size=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH7FnT8nhaD9"
      },
      "outputs": [],
      "source": [
        "# Get predictions on the validation data\n",
        "\n",
        "predictions = model.predict(validation_generator_cyclic,\n",
        "    steps=1, verbose=0)\n",
        "print(np.round(predictions.T[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfDxaO80haD_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Print the corresponding validation labels\n",
        "\n",
        "print(validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaJi2CvghaEE"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "evaluation = model.evaluate(\n",
        "    validation_generator_cyclic, steps=1, verbose=0)\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MulkCc1FhaEN"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## Keras image data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8G8N_88haEQ"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "image_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255., horizontal_flip=True, height_shift_range=0.2,\n",
        "    fill_mode='nearest',featurewise_center=True) # aaaa|abcd|dddd\n",
        "\n",
        "image_data_gen.fit(x_train)\n",
        "\n",
        "train_datagen = image_data_gen.flow(\n",
        "    x_train, y_train, batch_size=16)\n",
        "\n",
        "model.fit(train_datagen, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awp0hKRDhaER"
      },
      "source": [
        "#### Load the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOd1YbsJhaEU"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "\n",
        "(training_features, training_labels), (test_features, test_labels) = \\\n",
        "    tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsAjqqnjhaEV"
      },
      "outputs": [],
      "source": [
        "# Convert the labels to a one-hot encoding\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "training_labels = tf.keras.utils.to_categorical(\n",
        "    y=training_labels, num_classes=num_classes)\n",
        "test_labels = tf.keras.utils.to_categorical(\n",
        "    y=test_labels, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNrlxnRlhaEX"
      },
      "source": [
        "#### Create a generator function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ieho8OrohaEY"
      },
      "outputs": [],
      "source": [
        "# Create a function that returns a data generator\n",
        "\n",
        "def get_generator(features, labels, batch_size=1):\n",
        "    for n in range(int(len(features)/batch_size)):\n",
        "        yield (\n",
        "            features[n*batch_size: (n+1)*batch_size],\n",
        "            labels[n*batch_size: (n+1)*batch_size]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kk4Z87MhaEa"
      },
      "outputs": [],
      "source": [
        "# Use the function we created to get a training data generator with a batch size of 1\n",
        "\n",
        "training_generator = get_generator(\n",
        "    training_features, training_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUgGGmjOhaEb"
      },
      "outputs": [],
      "source": [
        "# Assess the shape of the items generated by training_generator using the\n",
        "# `next` function to yield an item.\n",
        "\n",
        "image, label = next(training_generator)\n",
        "print(f\"image shape: {image.shape}\")\n",
        "print(f\"label shape: {label.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEqA1uDAhaEd"
      },
      "outputs": [],
      "source": [
        "# Test the training generator by obtaining an image using the\n",
        "# `next` generator function, and then using imshow to plot it.\n",
        "# Print the corresponding label\n",
        "\n",
        "image, label = next(training_generator)\n",
        "image_unbatched = np.squeeze(image)\n",
        "plt.imshow(image_unbatched)\n",
        "plt.title(str(label))\n",
        "plt.grid(visible=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhNS0N5khaEf"
      },
      "outputs": [],
      "source": [
        "# Reset the generator by re-running the `get_generator` function.\n",
        "\n",
        "train_generator = get_generator(\n",
        "    training_features, training_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh41TDl2haEh"
      },
      "source": [
        "#### Create a data augmention generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKyZQms7haEk"
      },
      "outputs": [],
      "source": [
        "# Create a function to convert an image to monochrome\n",
        "\n",
        "def monochrome(x):\n",
        "    def func_bw(a):\n",
        "        average_colour = np.mean(a)\n",
        "        return [average_colour, average_colour, average_colour]\n",
        "    x = np.apply_along_axis(func_bw, -1, x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxuo_9aAhaEn"
      },
      "outputs": [],
      "source": [
        "# Create an ImageDataGenerator object\n",
        "\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=monochrome,\n",
        "    rotation_range=180, rescale=1/255.0)\n",
        "\n",
        "image_generator.fit(training_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRwjDb69haEq"
      },
      "source": [
        "Check [the documentation](https://keras.io/preprocessing/image/) for the full list of image data augmentation options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRnwugLChaEr"
      },
      "outputs": [],
      "source": [
        "# Create an iterable generator using the `flow` function\n",
        "\n",
        "image_generator_iterable = image_generator.flow(\n",
        "    training_features, training_labels,\n",
        "    batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2bR1hpHhaEs"
      },
      "outputs": [],
      "source": [
        "# Show a sample from the generator and compare with the original\n",
        "\n",
        "image, label = next(image_generator_iterable)\n",
        "image_orig, label_orig = next(train_generator)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(np.squeeze(image))\n",
        "ax[0].set_title('Transformed')\n",
        "ax[0].grid(visible=None)\n",
        "ax[1].imshow(np.squeeze(image_orig))\n",
        "ax[1].set_title('Original')\n",
        "ax[1].grid(visible=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZN3SD04haEt"
      },
      "source": [
        "#### Flow from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW7NZ379eczt"
      },
      "outputs": [],
      "source": [
        "os.makedirs('./data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS8Aj9o7haEu"
      },
      "outputs": [],
      "source": [
        "# Inspect the directory structure\n",
        "\n",
        "train_path = 'data/flowers-recognition-split/train'\n",
        "val_path = 'data/flowers-recognition-split/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0teFy5XhaEw"
      },
      "outputs": [],
      "source": [
        "# Create an ImageDataGenerator object\n",
        "\n",
        "datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1/255.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8F4Hv6OhaEy"
      },
      "outputs": [],
      "source": [
        "classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43SCIXOohaEz"
      },
      "outputs": [],
      "source": [
        "# Create a training data generator\n",
        "\n",
        "train_generator = datagenerator.flow_from_directory(\n",
        "    directory=train_path, batch_size=64, classes=classes,\n",
        "    target_size=(16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNduitYAhaE1"
      },
      "outputs": [],
      "source": [
        "# Create a validation data generator\n",
        "\n",
        "val_generator = datagenerator.flow_from_directory(\n",
        "    directory=val_path, batch_size=64, classes=classes,\n",
        "    target_size=(16,16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzYyqoVhaE8"
      },
      "outputs": [],
      "source": [
        "# Get and display an image and label from the training generator\n",
        "\n",
        "x = next(train_generator)\n",
        "plt.imshow(x[0][8])\n",
        "plt.title(str(x[1][8]))\n",
        "plt.grid(visible=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hawvnO6-haE9"
      },
      "outputs": [],
      "source": [
        "# Reset the training generator\n",
        "\n",
        "train_generator = datagenerator.flow_from_directory(\n",
        "    directory=train_path, batch_size=64, classes=classes,\n",
        "    target_size=(16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSWrrJRGhaE_"
      },
      "source": [
        "#### Create a model to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLteymJqhaE_"
      },
      "outputs": [],
      "source": [
        "# Build a CNN model\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(16, 16, 3)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(8, 8),\n",
        "    padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(8, 8),\n",
        "    padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=4, kernel_size=(4, 4),\n",
        "    padding='same', activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ScUILYhaFB"
      },
      "outputs": [],
      "source": [
        "# Create an optimizer object\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxqLi4jrhaFC"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KogBiFshaFE"
      },
      "outputs": [],
      "source": [
        "# Print the model summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqwr_ORZhaFG"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDyE8_XchaFH"
      },
      "outputs": [],
      "source": [
        "# Calculate the training generator and test generator steps per epoch\n",
        "\n",
        "train_steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "val_steps = val_generator.n // val_generator.batch_size\n",
        "print(f\"train steps per epoch: {train_steps_per_epoch}\\\n",
        "    \\nvalid steps: {val_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRmd7ZDDhaFI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch, epochs=5,\n",
        "    verbose=2)\n",
        "\n",
        "df_history = pd.DataFrame(history.history, index=history.epoch)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'categorical_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da9gQT5BhaFL"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWkUOT4fhaFM"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "loss, accuracy = model.evaluate(\n",
        "    val_generator, steps=val_steps, verbose=0)\n",
        "print(f\"loss: {loss:.2f}, accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQRD2u5UhaFN"
      },
      "source": [
        "#### Predict using the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de5vOjAjhaFN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Predict labels with the model\n",
        "\n",
        "predictions = model.predict(val_generator, steps=1, verbose=0)\n",
        "print([classes[i] for i in np.argmax(predictions, axis=1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7sr3v1haFP"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tensors=(\n",
        "    tf.random.uniform(shape=(256, 4), minval=1, maxval=10, dtype=tf.int32),\n",
        "    tf.random.normal(shape=(256,))\n",
        "))\n",
        "print(dataset.element_spec)\n",
        "\n",
        "for x, y in dataset.take(count=2):\n",
        "    print(x.numpy(), y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yj9PF9WFCu2",
        "outputId": "706fbc27-df50-49ae-d27d-f18e7c1bea43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(4,), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))\n",
            "[8 6 9 4] 0.08422458\n",
            "[8 3 8 8] -0.86090374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM8-2uhNeXcc",
        "outputId": "e594a476-acf9-4445-bacc-8b95a155cee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1,), dtype=tf.uint8, name=None))\n",
            "(TensorSpec(shape=(32, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.int32, name=None))\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = \\\n",
        "    tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(x_train, y_train))\n",
        "print(dataset.element_spec)\n",
        "\n",
        "img_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    width_shift_range=0.2, horizontal_flip=True)\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    generator=img_datagen.flow, args=[x_train, y_train],\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=((32, 32, 32, 3), (32, 1))\n",
        ")\n",
        "print(dataset.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhejCLoBhaFR"
      },
      "source": [
        "#### Create a simple dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6tuzoM-ehaFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80c4d3e-d1a2-4289-b3bf-05b97ffa1048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None)\n",
            "(TensorSpec(shape=(2, 2), dtype=tf.float64, name=None), TensorSpec(shape=(1,), dtype=tf.float64, name=None))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Create a dataset from the tensor x\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=np.zeros(shape=(100, 10, 2, 2)))\n",
        "\n",
        "# Inspect the Dataset object\n",
        "print(dataset1.element_spec)\n",
        "\n",
        "# Create another dataset from the tensor x2\n",
        "# and inspect the Dataset object\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(tensors=(\n",
        "    np.zeros(shape=(10, 2, 2)), np.zeros(shape=(10, 1))\n",
        "))\n",
        "# Print the element spec\n",
        "print(dataset2.element_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_eojOchaFu"
      },
      "source": [
        "#### Create a zipped dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oSu4cNAXhaFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136c9078-fea4-45d0-8eeb-9b9fc116193b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None), (TensorSpec(shape=(2, 2), dtype=tf.float64, name=None), TensorSpec(shape=(1,), dtype=tf.float64, name=None)))\n",
            "Number of batches: 10\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Combine the two datasets into one larger dataset\n",
        "dataset_zipped = tf.data.Dataset.zip(\n",
        "    datasets=(dataset1, dataset2))\n",
        "\n",
        "# Print the element_spec\n",
        "print(dataset_zipped.element_spec)\n",
        "\n",
        "# Define a function to find the number of batches in a dataset\n",
        "def get_batches(dataset):\n",
        "    iter_dataset = iter(dataset)\n",
        "    i = 0\n",
        "    try:\n",
        "        while next(iter_dataset):\n",
        "            i = i+1\n",
        "    except:\n",
        "        return i\n",
        "\n",
        "# Find the number of batches in the zipped Dataset\n",
        "print(f\"Number of batches: {get_batches(dataset_zipped)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBmSO5lJhaF1"
      },
      "source": [
        "#### Create a dataset from numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za9ynOokhaF5",
        "outputId": "90f3ff64-4e55-464b-b6b0-428f58768db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n",
            "Length of an element: 2\n",
            "x train shape: (28, 28)\n",
            "y train shape: ()\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_features, train_labels), (test_features, test_labels) = \\\n",
        "    tf.keras.datasets.mnist.load_data()\n",
        "print(type(train_features), type(train_labels))\n",
        "\n",
        "# Create a Dataset from the MNIST data\n",
        "mnist_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(train_features, train_labels))\n",
        "\n",
        "# Inspect the Dataset object\n",
        "print(mnist_dataset.element_spec)\n",
        "\n",
        "# Inspect the length of an element using the take method\n",
        "element = next(iter(mnist_dataset.take(count=1)))\n",
        "print(f\"Length of an element: {len(element)}\")\n",
        "\n",
        "# Examine the shapes of the data\n",
        "print(f\"x train shape: {element[0].shape}\")\n",
        "print(f\"y train shape: {element[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzJe1CIvhaGB"
      },
      "source": [
        "#### Create a dataset from text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zSGZUSrh1Y5"
      },
      "outputs": [],
      "source": [
        "os.makedirs(name=\"./data/shakespeare\",\n",
        "    exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKK73IjhhaGB"
      },
      "outputs": [],
      "source": [
        "# Print the list of text files\n",
        "\n",
        "text_files = sorted([f.path for f in\n",
        "    os.scandir(path=\"./data/shakespeare\")])\n",
        "\n",
        "print(text_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Jwc9dEhaGD"
      },
      "outputs": [],
      "source": [
        "# Load the first file using python and print the first 5 lines.\n",
        "\n",
        "with open(text_files[0], 'r') as f:\n",
        "    contents = [f.readline() for i in range(5)]\n",
        "    for line in contents:\n",
        "        print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZkNIUYnhaGE"
      },
      "outputs": [],
      "source": [
        "# Load the lines from the files into a dataset using TextLineDataset\n",
        "\n",
        "shakespeare_dataset = tf.data.TextLineDataset(\n",
        "    filenames=text_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbK2AV4WhaGG"
      },
      "outputs": [],
      "source": [
        "# Use the take method to get and print the first 5 lines of the dataset\n",
        "\n",
        "first_5_lines_dataset = iter(shakespeare_dataset.take(5))\n",
        "lines = [line for line in first_5_lines_dataset]\n",
        "for line in lines:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxr_d64jhaGH"
      },
      "outputs": [],
      "source": [
        "# Compute the number of lines in the first file\n",
        "\n",
        "lines = []\n",
        "with open(text_files[0], 'r') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        lines.append(line)\n",
        "        line = f.readline()\n",
        "    print(f\"Number of lines: {len(lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xj3mOiUhaGJ"
      },
      "outputs": [],
      "source": [
        "# Compute the number of lines in the shakespeare dataset we created\n",
        "\n",
        "shakespeare_dataset_iterator = iter(shakespeare_dataset)\n",
        "lines = [line for line in shakespeare_dataset_iterator]\n",
        "print(f\"Number of lines: {len(lines)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrXEQqa3haGK"
      },
      "source": [
        "#### Interleave lines from the text data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv7HQG9_haGK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create a dataset of the text file strings\n",
        "\n",
        "text_files_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=text_files)\n",
        "files = [file for file in text_files_dataset]\n",
        "for file in files:\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXEK2VNQhaGM"
      },
      "outputs": [],
      "source": [
        "# Interleave the lines from the text files\n",
        "\n",
        "interleaved_shakespeare_dataset = text_files_dataset.interleave(\n",
        "    map_func=tf.data.TextLineDataset, cycle_length=9)\n",
        "print(interleaved_shakespeare_dataset.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywdbkvVJhaGO"
      },
      "outputs": [],
      "source": [
        "# Print the first 10 elements of the interleaved dataset\n",
        "\n",
        "lines = [line for line in\n",
        "    iter(interleaved_shakespeare_dataset.take(10))]\n",
        "for line in lines:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5CRpW_whaGQ"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## Training with Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGGtQCM2wxc4"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(x_train, y_train))\n",
        "dataset = dataset.batch(batch_size=16, drop_remainder=True)\n",
        "print(dataset.element_spec)\n",
        "dataset = dataset.repeat(count=10)\n",
        "\n",
        "history = model.fit(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDN2uIRWwxc5"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors=(x_train, y_train))\n",
        "\n",
        "def rescale(image, label):\n",
        "    return image/255., label\n",
        "\n",
        "dataset = dataset.map(map_func=rescale)\n",
        "\n",
        "def label_filter(image, label):\n",
        "    return tf.squeeze(label)!=9\n",
        "\n",
        "dataset = dataset.filter(predicate=label_filter)\n",
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "dataset = dataset.batch(batch_size=16, drop_remainder=True)\n",
        "print(dataset.element_spec)\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "history = model.fit(dataset, steps_per_epoch=x_train.shape[0]//16,\n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sImBVH74haGR"
      },
      "source": [
        "#### Load the UCI Bank Marketing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jwLM9ijlgYY"
      },
      "outputs": [],
      "source": [
        "os.makedirs(name=\"./data/bank\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRt2GoekhaGS"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "\n",
        "bank_dataframe = pd.read_csv('./data/bank/bank-full.csv', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKUSID04haGT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "bank_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNobGyshaGW"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the DataFrame\n",
        "\n",
        "print(f\"data shape: {bank_dataframe.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV-zDyNdhaGX"
      },
      "outputs": [],
      "source": [
        "# Select features from the DataFrame\n",
        "\n",
        "features = ['age', 'job', 'marital', 'education', 'default', 'balance',\n",
        "    'housing', 'loan', 'contact', 'campaign', 'pdays', 'poutcome']\n",
        "labels = ['y']\n",
        "\n",
        "bank_dataframe = bank_dataframe.filter(features + labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qxNd34thaGY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "print(f\"data shape: {bank_dataframe.shape}\")\n",
        "bank_dataframe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMOyO0aZhaGa"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIwMmt9KhaGa"
      },
      "outputs": [],
      "source": [
        "# Convert the categorical features in the DataFrame to one-hot encodings\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "categorical_features = ['default', 'housing', 'job', 'loan',\n",
        "    'education', 'contact', 'poutcome']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    bank_dataframe[feature] = tuple(encoder.fit_transform(\n",
        "        bank_dataframe[feature]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbwWaUKEhaGb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the head of the DataFrame\n",
        "\n",
        "print(bank_dataframe.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXIuiANFhaGc"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame\n",
        "\n",
        "bank_dataframe = bank_dataframe.sample(frac=1)\\\n",
        "    .reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrI8QPAghaGe"
      },
      "source": [
        "#### Create the Dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyWS07bghaGf"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame to a Dataset\n",
        "\n",
        "dict_bank = dict(bank_dataframe)\n",
        "bank_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    tensors={k: dict_bank[k].tolist() for k in dict_bank})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ykb1yAlhaGn"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OULr3jShaGq"
      },
      "source": [
        "#### Filter the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJwAeSuwhaGr"
      },
      "outputs": [],
      "source": [
        "# First check that there are records in the dataset for non-married individuals\n",
        "\n",
        "def check_divorced():\n",
        "    bank_dataset_iterable = iter(bank_dataset)\n",
        "    for x in bank_dataset_iterable:\n",
        "        if x['marital'] != 'divorced':\n",
        "            print('Found a person with marital status: {}'.format(x['marital']))\n",
        "            return\n",
        "    print('No non-divorced people were found!')\n",
        "\n",
        "check_divorced()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixTiUgcShaGt"
      },
      "outputs": [],
      "source": [
        "# Filter the Dataset to retain only entries with a 'divorced' marital status\n",
        "\n",
        "bank_dataset = bank_dataset.filter(predicate=lambda x:\n",
        "    tf.equal(x['marital'], tf.constant([b'divorced']))[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATDI5_73haGu"
      },
      "outputs": [],
      "source": [
        "# Check the records in the dataset again\n",
        "\n",
        "check_divorced()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZQjW6MGhaGv"
      },
      "source": [
        "#### Map a function over the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__3Jjfl7haGw"
      },
      "outputs": [],
      "source": [
        "# Convert the label ('y') to an integer instead of 'yes' or 'no'\n",
        "\n",
        "def map_label(x):\n",
        "    x['y'] = 0 if (x['y']==tf.constant([b'no'], dtype=tf.string)) else 1\n",
        "    return x\n",
        "\n",
        "bank_dataset = bank_dataset.map(map_func=map_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60G0f9YGhaGx"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGpolkUJhaGy"
      },
      "outputs": [],
      "source": [
        "# Remove the 'marital' column\n",
        "\n",
        "bank_dataset = bank_dataset.map(map_func=lambda x:\n",
        "    {key: val for key, val in x.items() if key!='marital'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTbrVE7chaG0"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlYUoVylhaG1"
      },
      "source": [
        "#### Create input and output data tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIsxi_u0haG1"
      },
      "outputs": [],
      "source": [
        "# Create an input and output tuple for the dataset\n",
        "\n",
        "def map_feature_label(x):\n",
        "    features = [[x['age']], [x['balance']], [x['campaign']], x['contact'],\n",
        "        x['default'], x['education'], x['housing'], x['job'], x['loan'],\n",
        "        [x['pdays']], x['poutcome']]\n",
        "    return (tf.concat(features, axis=0), x['y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIzfxziMhaG3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Map this function over the dataset\n",
        "\n",
        "bank_dataset = bank_dataset.map(map_func=map_feature_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JKhkqaEhaHB"
      },
      "outputs": [],
      "source": [
        "# Inspect the Dataset object\n",
        "\n",
        "bank_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTg7cxYVhaHC"
      },
      "source": [
        "#### Split into a training and a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpa2OGmxhaHC"
      },
      "outputs": [],
      "source": [
        "# Determine the length of the Dataset\n",
        "\n",
        "dataset_length = 0\n",
        "for _ in bank_dataset:\n",
        "    dataset_length += 1\n",
        "print(f\"dataset length: {dataset_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJ5z3chwxc9"
      },
      "outputs": [],
      "source": [
        "bank_dataset_iterator = iter(bank_dataset)\n",
        "dataset_length = 0\n",
        "try:\n",
        "    while next(bank_dataset_iterator):\n",
        "        dataset_length += 1\n",
        "except Exception:\n",
        "    pass\n",
        "print(f\"dataset length: {dataset_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiBen_r1haHD"
      },
      "outputs": [],
      "source": [
        "# Make training and validation sets from the dataset\n",
        "\n",
        "training_elements = int(dataset_length * 0.7)\n",
        "train_dataset = bank_dataset.take(count=training_elements)\n",
        "valid_dataset = bank_dataset.skip(count=training_elements)\n",
        "\n",
        "print(f\"train dataset element spec: {train_dataset.element_spec}\")\n",
        "print(f\"valid dataset element spec: {valid_dataset.element_spec}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj7Zs8yhhaHG"
      },
      "source": [
        "#### Build a classification model\n",
        "\n",
        "Now let's build a model to classify the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFJN8PaUhaHG"
      },
      "outputs": [],
      "source": [
        "# Build a classifier model\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(30,)))\n",
        "model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "model.add(tf.keras.layers.Dense(units=400, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "model.add(tf.keras.layers.Dense(units=400, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY92bQT-haHJ"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_p0gJvVhaHK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Show the model summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aALufpk6haHL"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIJanyhDhaHL"
      },
      "outputs": [],
      "source": [
        "# Create batched training and validation datasets\n",
        "\n",
        "train_dataset = train_dataset.batch(\n",
        "    batch_size=20, drop_remainder=True)\n",
        "print(f\"train dataset element spec: {train_dataset.element_spec}\")\n",
        "\n",
        "valid_dataset = valid_dataset.batch(\n",
        "    batch_size=100)\n",
        "print(f\"valid dataset element spec: {valid_dataset.element_spec}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xNHeIlthaHN"
      },
      "outputs": [],
      "source": [
        "# Shuffle the training data\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7daJ6eOhaHO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=valid_dataset,\n",
        "    epochs=5, verbose=0)\n",
        "df_history = pd.DataFrame(history.history, index=history.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE7zoUwLhaHQ"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "for ax, metric in zip(axes, ['loss', 'binary_accuracy']):\n",
        "    sns.lineplot(ax=ax, data=df_history,\n",
        "        x=df_history.index+1, y=metric, label='train')\n",
        "    try:\n",
        "        sns.lineplot(ax=ax, data=df_history,\n",
        "            x=df_history.index+1, y=f'val_{metric}', label='valid')\n",
        "    except Exception:\n",
        "        pass\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel(f'{metric}')\n",
        "    ax.set_title(f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data_pipeline.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}