{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/Getting_started_TensorFlow2/Validation_regularisation_callbacks/Logs_dictionary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --no-cache-dir -qU pip wheel\n",
    "pip install --no-cache-dir -qU tensorflow-gpu\n",
    "pip install --no-cache-dir -qU pandas matplotlib seaborn scikit-learn\n",
    "pip check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVVr65yyRbvK"
   },
   "source": [
    "# Using the logs dictionary\n",
    "\n",
    "In this reading, we will learn how to take advantage of the `logs` dictionary in Keras to define our own callbacks and check the progress of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font=\"DejaVu Sans\")\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `logs` dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n",
    "\n",
    "We can incorporate information from the `logs` dictionary into our own custom callbacks.\n",
    "\n",
    "Let's see this in action in the context of a model we will construct and fit to the `sklearn` diabetes dataset that we have been using in this module.\n",
    "\n",
    "Let's first import the dataset, and split it into the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lxeQ2PrJRbvd"
   },
   "outputs": [],
   "source": [
    "# Save the input and target variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and test sets\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "    data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,305\n",
      "Trainable params: 18,177\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znnmoVozRbvz"
   },
   "source": [
    "We now compile the model, with\n",
    "* Mean squared error as the loss function,\n",
    "* the Adam optimizer, and\n",
    "* Mean absolute error (`mae`) as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcA-LdiJRbv5"
   },
   "source": [
    "### Defining a custom callback\n",
    "\n",
    "Now we define our custom callback using the `logs` dictionary to access the loss and metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rveaw2FGRbv6"
   },
   "outputs": [],
   "source": [
    "# Create the custom callback\n",
    "\n",
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch%2==0:\n",
    "            print('\\n After batch {}, the loss is {:7.2f}.'.format(\n",
    "                batch, logs['loss']))\n",
    "            \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}.'.format(\n",
    "            batch, logs['loss']))\n",
    "        \n",
    "    # Print the loss and mean absolute error after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}'.format(\n",
    "            epoch, logs['loss'], logs['mae']))\n",
    "        \n",
    "    # Notify the user when prediction has finished on each batch\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        print('Finished prediction on batch {}!'.format(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJURHJUqRbwA"
   },
   "source": [
    "We now fit model to the data, and specify that we would like to use our custom callback `LossAndMetricCallback()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 31782.16.\n",
      "\n",
      " After batch 2, the loss is 30234.99.\n",
      "Epoch 0: Average loss is 28840.67, mean absolute error is  150.79\n",
      "\n",
      " After batch 0, the loss is 28245.10.\n",
      "\n",
      " After batch 2, the loss is 29880.22.\n",
      "Epoch 1: Average loss is 28746.09, mean absolute error is  150.51\n",
      "\n",
      " After batch 0, the loss is 28693.44.\n",
      "\n",
      " After batch 2, the loss is 27690.93.\n",
      "Epoch 2: Average loss is 28624.91, mean absolute error is  150.16\n",
      "\n",
      " After batch 0, the loss is 28814.40.\n",
      "\n",
      " After batch 2, the loss is 28906.96.\n",
      "Epoch 3: Average loss is 28455.99, mean absolute error is  149.66\n",
      "\n",
      " After batch 0, the loss is 26023.30.\n",
      "\n",
      " After batch 2, the loss is 27122.93.\n",
      "Epoch 4: Average loss is 28221.31, mean absolute error is  148.99\n",
      "\n",
      " After batch 0, the loss is 31825.07.\n",
      "\n",
      " After batch 2, the loss is 29075.80.\n",
      "Epoch 5: Average loss is 27915.98, mean absolute error is  148.08\n",
      "\n",
      " After batch 0, the loss is 26871.72.\n",
      "\n",
      " After batch 2, the loss is 29233.21.\n",
      "Epoch 6: Average loss is 27502.75, mean absolute error is  146.88\n",
      "\n",
      " After batch 0, the loss is 21764.23.\n",
      "\n",
      " After batch 2, the loss is 25188.17.\n",
      "Epoch 7: Average loss is 26949.99, mean absolute error is  145.29\n",
      "\n",
      " After batch 0, the loss is 24108.48.\n",
      "\n",
      " After batch 2, the loss is 26401.73.\n",
      "Epoch 8: Average loss is 26271.46, mean absolute error is  143.25\n",
      "\n",
      " After batch 0, the loss is 26142.04.\n",
      "\n",
      " After batch 2, the loss is 25029.12.\n",
      "Epoch 9: Average loss is 25404.01, mean absolute error is  140.65\n",
      "\n",
      " After batch 0, the loss is 24121.68.\n",
      "\n",
      " After batch 2, the loss is 24116.11.\n",
      "Epoch 10: Average loss is 24341.76, mean absolute error is  137.35\n",
      "\n",
      " After batch 0, the loss is 21959.91.\n",
      "\n",
      " After batch 2, the loss is 23366.02.\n",
      "Epoch 11: Average loss is 23044.95, mean absolute error is  133.26\n",
      "\n",
      " After batch 0, the loss is 23361.26.\n",
      "\n",
      " After batch 2, the loss is 22298.01.\n",
      "Epoch 12: Average loss is 21551.48, mean absolute error is  128.29\n",
      "\n",
      " After batch 0, the loss is 19329.22.\n",
      "\n",
      " After batch 2, the loss is 20231.41.\n",
      "Epoch 13: Average loss is 19791.93, mean absolute error is  122.15\n",
      "\n",
      " After batch 0, the loss is 18085.72.\n",
      "\n",
      " After batch 2, the loss is 17474.16.\n",
      "Epoch 14: Average loss is 17799.05, mean absolute error is  114.94\n",
      "\n",
      " After batch 0, the loss is 15265.91.\n",
      "\n",
      " After batch 2, the loss is 15576.54.\n",
      "Epoch 15: Average loss is 15594.87, mean absolute error is  106.24\n",
      "\n",
      " After batch 0, the loss is 15122.68.\n",
      "\n",
      " After batch 2, the loss is 14077.95.\n",
      "Epoch 16: Average loss is 13334.29, mean absolute error is   96.26\n",
      "\n",
      " After batch 0, the loss is 11042.48.\n",
      "\n",
      " After batch 2, the loss is 11232.86.\n",
      "Epoch 17: Average loss is 11085.07, mean absolute error is   85.51\n",
      "\n",
      " After batch 0, the loss is 8977.99.\n",
      "\n",
      " After batch 2, the loss is 8768.54.\n",
      "Epoch 18: Average loss is 8862.35, mean absolute error is   74.10\n",
      "\n",
      " After batch 0, the loss is 7245.22.\n",
      "\n",
      " After batch 2, the loss is 7308.90.\n",
      "Epoch 19: Average loss is 7120.63, mean absolute error is   65.60\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_data, train_targets, epochs=20, \n",
    "    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our callback in the `evaluate` function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 24499.36.\n",
      "\n",
      " After batch 1, the loss is 20883.67.\n",
      "\n",
      " After batch 2, the loss is 22232.93.\n",
      "\n",
      " After batch 3, the loss is 22456.43.\n",
      "\n",
      " After batch 4, the loss is 22349.46.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_eval = model.evaluate(test_data, test_targets, batch_size=10,\n",
    "    callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And also the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction on batch 0!\n",
      "Finished prediction on batch 1!\n",
      "Finished prediction on batch 2!\n",
      "Finished prediction on batch 3!\n",
      "Finished prediction on batch 4!\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "model_pred = model.predict(test_data, batch_size=10,\n",
    "    callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVM8pLGgRbwE"
   },
   "source": [
    "### Application - learning rate scheduler\n",
    "Let's now look at a more sophisticated custom callback.\n",
    "\n",
    "We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n",
    "\n",
    "First we define the auxillary function that returns the learning rate for each epoch based on our schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HWrZ19SXRbwF"
   },
   "outputs": [],
   "source": [
    "# Define the learning rate schedule. The tuples below are \n",
    "# (start_epoch, new_learning_rate)\n",
    "lr_schedule = [(4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    # Checks to see if the input epoch is listed in the learning rate\n",
    "    # schedule and if so, returns index in lr_schedule\n",
    "    epoch_in_sched = [i for i in range(len(lr_schedule)) \n",
    "        if lr_schedule[i][0]==int(epoch)]\n",
    "    if len(epoch_in_sched)>0:\n",
    "        # If it is, return the learning rate corresponding to the epoch\n",
    "        return lr_schedule[epoch_in_sched[0]][1]\n",
    "    else:\n",
    "        # Otherwise, return the existing learning rate\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzQ-cbVGRbwK"
   },
   "source": [
    "Let's now define the callback itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Vfq_Jt30RbwL"
   },
   "outputs": [],
   "source": [
    "# Define the custom callback\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning\n",
    "        # rate, and raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError(\"Error: Optimizer does not have a learning rate.\")\n",
    "            \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(\n",
    "            self.model.optimizer.lr))\n",
    "        \n",
    "        # Call the auxillary function to get the scheduled learning rate \n",
    "        # for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "        \n",
    "        # Set the learning rate to the scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print(\"Learning rate for epoch {} is {:7.3f}\".format(\n",
    "            epoch, scheduled_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPDmzjeWRbwP"
   },
   "source": [
    "Let's now train the model again with our new callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 128)               1408      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,305\n",
      "Trainable params: 18,177\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the same model as before\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "new_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "new_model.compile(\n",
    "    loss='mse', \n",
    "    optimizer='adam', \n",
    "    metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 0 is   0.001\n",
      "Learning rate for epoch 1 is   0.001\n",
      "Learning rate for epoch 2 is   0.001\n",
      "Learning rate for epoch 3 is   0.001\n",
      "Learning rate for epoch 4 is   0.030\n",
      "Learning rate for epoch 5 is   0.030\n",
      "Learning rate for epoch 6 is   0.030\n",
      "Learning rate for epoch 7 is   0.020\n",
      "Learning rate for epoch 8 is   0.020\n",
      "Learning rate for epoch 9 is   0.020\n",
      "Learning rate for epoch 10 is   0.020\n",
      "Learning rate for epoch 11 is   0.005\n",
      "Learning rate for epoch 12 is   0.005\n",
      "Learning rate for epoch 13 is   0.005\n",
      "Learning rate for epoch 14 is   0.005\n",
      "Learning rate for epoch 15 is   0.007\n",
      "Learning rate for epoch 16 is   0.007\n",
      "Learning rate for epoch 17 is   0.007\n",
      "Learning rate for epoch 18 is   0.007\n",
      "Learning rate for epoch 19 is   0.007\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with our learning rate scheduler callback\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "    batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)],\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfzOPQomRbwU"
   },
   "source": [
    "### Further reading and resources\n",
    "* https://www.tensorflow.org/guide/keras/custom_callback\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Logs_dictionary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
