{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Capstone Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/Customising_TensorFlow2_models/Neural_translation/Neural_translation_English_German.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsX0L1sG1iZj"
      },
      "source": [
        "# Capstone Project\n",
        "## Neural translation model\n",
        "### Instructions\n",
        "\n",
        "In this notebook, you will create a neural network that translates from English to German. You will use concepts from throughout this course, including building more flexible model architectures, freezing layers, data processing pipeline and sequence modelling.\n",
        "\n",
        "This project is peer-assessed. Within this notebook you will find instructions in each section for how to complete the project. Pay close attention to the instructions as the peer review will be carried out according to a grading rubric that checks key parts of the project instructions. Feel free to add extra cells into the notebook as required.\n",
        "\n",
        "### How to submit\n",
        "\n",
        "When you have completed the Capstone project notebook, you will submit a pdf of the notebook for peer review. First ensure that the notebook has been fully executed from beginning to end, and all of the cell outputs are visible. This is important, as the grading rubric depends on the reviewer being able to view the outputs of your notebook. Save the notebook as a pdf (you could download the notebook with File -> Download .ipynb, open the notebook locally, and then File -> Download as -> PDF via LaTeX), and then submit this pdf for review.\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "We'll start by running some imports, and loading the dataset. For this project you are free to make further imports throughout the notebook as you wish. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VyTvxPN1iZn"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Layer, Input, Masking, LSTM, \\\n",
        "    Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Mean"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuIi4lAR1iZt"
      },
      "source": [
        "![Flags overview image](https://github.com/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/image/neural_translation/germany_uk_flags.png?raw=1)\n",
        "\n",
        "For the capstone project, you will use a language dataset from http://www.manythings.org/anki/ to build a neural translation model. This dataset consists of over 200,000 pairs of sentences in English and German. In order to make the training quicker, we will restrict to our dataset to 20,000 pairs. Feel free to change this if you wish - the size of the dataset used is not part of the grading rubric.\n",
        "\n",
        "Your goal is to develop a neural translation model from English to German, making use of a pre-trained English word embedding module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi9Dq6vv3FVO"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset is available for download as a zip file at the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1KczOciG7sYY7SB9UlBeRP1T9659b121Q\n",
        "\n",
        "You should store the unzipped folder in local drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw99tEEQ3bKL"
      },
      "source": [
        "if not os.path.exists(\"./data\"):\n",
        "    os.makedirs(\"./data\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAfYIl-cERc_",
        "outputId": "84b1d069-0c7c-4dbd-a6db-b8eb39ccee1e"
      },
      "source": [
        "%%bash\n",
        "\n",
        "fileid=\"1KczOciG7sYY7SB9UlBeRP1T9659b121Q\"\n",
        "filename=\"./data/deu.txt\"\n",
        "curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\n",
        "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r100   408    0   408    0     0    333      0 --:--:--  0:00:01 --:--:--   333\r100   408    0   408    0     0    333      0 --:--:--  0:00:01 --:--:--   333\n",
            "\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "\r100 29.9M    0 29.9M    0     0  16.9M      0 --:--:--  0:00:01 --:--:-- 16.9M\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8PetPpw1iZu"
      },
      "source": [
        "# Run this cell to load the dataset\n",
        "\n",
        "NUM_EXAMPLES = 20000\n",
        "data_examples = []\n",
        "with open('data/deu.txt', 'r', encoding='utf8') as f:\n",
        "    for line in f.readlines():\n",
        "        if len(data_examples) < NUM_EXAMPLES:\n",
        "            data_examples.append(line)\n",
        "        else:\n",
        "            break"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JumLjJ631iZy"
      },
      "source": [
        "# These functions preprocess English and German sentences\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) \n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"ü\", 'ue', sentence)\n",
        "    sentence = re.sub(r\"ä\", 'ae', sentence)\n",
        "    sentence = re.sub(r\"ö\", 'oe', sentence)\n",
        "    sentence = re.sub(r'ß', 'ss', sentence)\n",
        "    \n",
        "    sentence = unicode_to_ascii(sentence)\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r\"[^a-z?.!,']+\", \" \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    \n",
        "    return sentence.strip()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFJap-TW1iZ2"
      },
      "source": [
        "#### The custom translation model\n",
        "The following is a schematic of the custom translation model architecture you will develop in this project.\n",
        "\n",
        "![Model Schematic](https://github.com/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/image/neural_translation/neural_translation_model.png?raw=1)\n",
        "\n",
        "Key:\n",
        "![Model key](https://github.com/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/image/neural_translation/neural_translation_model_key.png?raw=1)\n",
        "\n",
        "The custom model consists of an encoder RNN and a decoder RNN. The encoder takes words of an English sentence as input, and uses a pre-trained word embedding to embed the words into a 128-dimensional space. To indicate the end of the input sentence, a special end token (in the same 128-dimensional space) is passed in as an input. This token is a TensorFlow Variable that is learned in the training phase (unlike the pre-trained word embedding, which is frozen).\n",
        "\n",
        "The decoder RNN takes the internal state of the encoder network as its initial state. A `<start>` token is passed in as the first input, which is embedded using a learned German word embedding. The decoder RNN then makes a prediction for the next German word, which during inference is then passed in as the following input, and this process is repeated until the special `<end>` token is emitted from the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z70nu6_01iZ3"
      },
      "source": [
        "## 1. Text preprocessing\n",
        "* Create separate lists of English and German sentences, and preprocess them using the `preprocess_sentence` function provided for you above.\n",
        "* Add a special `\"<start>\"` and `\"<end>\"` token to the beginning and end of every German sentence.\n",
        "* Use the Tokenizer class from the `tf.keras.preprocessing.text` module to tokenize the German sentences, ensuring that no character filters are applied. _Hint: use the Tokenizer's \"filter\" keyword argument._\n",
        "* Print out at least 5 randomly chosen examples of (preprocessed) English and German sentence pairs. For the German sentence, print out the text (with start and end tokens) as well as the tokenized sequence.\n",
        "* Pad the end of the tokenized German sequences with zeros, and batch the complete set of sequences into one numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9G20C4bk1iZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcee73ff-24c9-4cbe-af3c-065b9b54d4f6"
      },
      "source": [
        "def create_sentence(data):\n",
        "    \"\"\"\n",
        "    Clean the input and return sentence pairs.\n",
        "    \"\"\"\n",
        "    sentence_pairs = [[preprocess_sentence(w) for w in l.split('\\t')][:2] \n",
        "        for l in data]\n",
        "    return zip(*sentence_pairs)\n",
        "\n",
        "# get pairs\n",
        "en_sentence, ge_sentence = create_sentence(data_examples)\n",
        "# show last 5 pairs\n",
        "for pair in zip(en_sentence[-5:], ge_sentence[-5:]):\n",
        "    print(pair)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\"i've been working .\", 'ich war arbeiten .')\n",
            "(\"i've bought a car .\", 'ich habe ein auto gekauft .')\n",
            "(\"i've come for tom .\", 'ich bin fuer tom gekommen .')\n",
            "(\"i've done nothing .\", 'ich habe nichts getan .')\n",
            "(\"i've eaten enough .\", 'ich habe genug gegessen .')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhdUE_5ukdjv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f062f884-3450-43d8-8fbf-69f3b48463a3"
      },
      "source": [
        "def add_start_end(sentences):\n",
        "    \"\"\"\n",
        "    Adding a start and end token to the sentence so that the\n",
        "    model know when to start and stop predicting.\n",
        "    \"\"\"\n",
        "    return [\"<start> \" + s + \" <end>\" for s in sentences]\n",
        "\n",
        "ge_sentence = add_start_end(ge_sentence)\n",
        "ge_sentence[-5:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> ich war arbeiten . <end>',\n",
              " '<start> ich habe ein auto gekauft . <end>',\n",
              " '<start> ich bin fuer tom gekommen . <end>',\n",
              " '<start> ich habe nichts getan . <end>',\n",
              " '<start> ich habe genug gegessen . <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UlnBdIK1iaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052e8fc2-af86-426f-f068-5d595254c8ae"
      },
      "source": [
        "def create_tokenizer(sentences):\n",
        "    \"\"\"\n",
        "    Create and return a Tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer\n",
        "\n",
        "# Build the German tokenizer vocabulary\n",
        "ge_tokenizer = create_tokenizer(ge_sentence)\n",
        "# Tokenize the German sentences\n",
        "ge_sequence = ge_tokenizer.texts_to_sequences(ge_sentence)\n",
        "ge_sequence[-5:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 4, 24, 252, 3, 2],\n",
              " [1, 4, 18, 19, 82, 280, 3, 2],\n",
              " [1, 4, 15, 77, 5, 276, 3, 2],\n",
              " [1, 4, 18, 108, 182, 3, 2],\n",
              " [1, 4, 18, 480, 217, 3, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGTaqt1t1iaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3288befa-bf53-4fe2-a2d8-ba1546cf4792"
      },
      "source": [
        "# Randomly select 5 examples of English and German sentence pairs\n",
        "inx = np.random.choice(len(en_sentence), 5, replace=False)\n",
        "\n",
        "for n, i in enumerate(inx):\n",
        "    print(n, en_sentence[i])\n",
        "    print(n, ge_sentence[i], ge_sequence[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 i love cake .\n",
            "0 <start> ich liebe kuchen . <end> [1, 4, 61, 828, 3, 2]\n",
            "1 i'm really fast .\n",
            "1 <start> ich bin sehr schnell . <end> [1, 4, 15, 56, 127, 3, 2]\n",
            "2 i'm not guilty .\n",
            "2 <start> ich bin nicht schuldig . <end> [1, 4, 15, 12, 564, 3, 2]\n",
            "3 keep paddling .\n",
            "3 <start> paddel weiter ! <end> [1, 3650, 126, 9, 2]\n",
            "4 do what you like .\n",
            "4 <start> tu , was dir beliebt . <end> [1, 266, 25, 38, 52, 831, 3, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-tjXZn6_RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fe0fc5-98f5-4a5c-9bb7-af5e0fe55e2e"
      },
      "source": [
        "def post_pad(sequences):\n",
        "    \"\"\"\n",
        "    Pad each sequence to a maximum length.\n",
        "    \"\"\"\n",
        "    return pad_sequences(sequences, padding='post')\n",
        "\n",
        "ge_padded = post_pad(ge_sequence)\n",
        "ge_padded[-5:]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   4,  24, 252,   3,   2,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0],\n",
              "       [  1,   4,  18,  19,  82, 280,   3,   2,   0,   0,   0,   0,   0,\n",
              "          0],\n",
              "       [  1,   4,  15,  77,   5, 276,   3,   2,   0,   0,   0,   0,   0,\n",
              "          0],\n",
              "       [  1,   4,  18, 108, 182,   3,   2,   0,   0,   0,   0,   0,   0,\n",
              "          0],\n",
              "       [  1,   4,  18, 480, 217,   3,   2,   0,   0,   0,   0,   0,   0,\n",
              "          0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foL7Ihs21iaP"
      },
      "source": [
        "## 2. Prepare the data with tf.data.Dataset objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9rCEE4z1iaQ"
      },
      "source": [
        "#### Load the embedding layer\n",
        "As part of the dataset preproceessing for this project, you will use a pre-trained English word embedding module from TensorFlow Hub. The URL for the module is https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1.\n",
        "\n",
        "This embedding takes a batch of text tokens in a 1-D tensor of strings as input. It then embeds the separate tokens into a 128-dimensional space. \n",
        "\n",
        "The code to load and test the embedding layer is provided for you below.\n",
        "\n",
        "**NB:** This model can also be used as a sentence embedding module. The module will process each token by removing punctuation and splitting on spaces. It then averages the word embeddings over a sentence to give a single embedding vector. However, we will use it only as a word embedding module, and will pass each word in the input sentence as a separate token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywZgobCh1iaR"
      },
      "source": [
        "# Load embedding module from Tensorflow Hub\n",
        "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\", \n",
        "    output_shape=(128), input_shape=(), dtype=tf.string)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiY8QEDp1iaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ade8a60-3340-43ef-b00e-d86a25ec4bb9"
      },
      "source": [
        "# Test the layer\n",
        "\n",
        "embedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([7, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjuzXc-z1iaY"
      },
      "source": [
        "You should now prepare the training and validation Datasets.\n",
        "\n",
        "* Create a random training and validation set split of the data, reserving e.g. 20% of the data for validation (NB: each English dataset example is a single sentence string, and each German dataset example is a sequence of padded integer tokens).\n",
        "* Load the training and validation sets into a tf.data.Dataset object, passing in a tuple of English and German data for both training and validation sets.\n",
        "* Create a function to map over the datasets that splits each English sentence at spaces. Apply this function to both Dataset objects using the map method. _Hint: look at the tf.strings.split function._\n",
        "* Create a function to map over the datasets that embeds each sequence of English words using the loaded embedding layer/model. Apply this function to both Dataset objects using the map method.\n",
        "* Create a function to filter out dataset examples where the English sentence is greater than or equal to than 13 (embedded) tokens in length. Apply this function to both Dataset objects using the filter method.\n",
        "* Create a function to map over the datasets that pads each English sequence of embeddings with some distinct padding value before the sequence, so that each sequence is length 13. Apply this function to both Dataset objects using the map method. _Hint: look at the tf.pad function. You can extract a Tensor shape using tf.shape; you might also find the tf.math.maximum function useful._\n",
        "* Batch both training and validation Datasets with a batch size of 16.\n",
        "* Print the `element_spec` property for the training and validation Datasets. \n",
        "* Using the Dataset `.take(1)` method, print the shape of the English data example from the training Dataset.\n",
        "* Using the Dataset `.take(1)` method, print the German data example Tensor from the validation Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-BUJOl_1iaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb43e0e4-0f29-4f10-9104-c63fb6a0a498"
      },
      "source": [
        "# Create a random training and validation sets using an 80-20 split\n",
        "en_train, en_valid, ge_train, ge_valid = train_test_split(\n",
        "    en_sentence, ge_padded, test_size=0.2)\n",
        "\n",
        "# Examine the shape of data\n",
        "print(\"The shape of training dataset:\")\n",
        "print(f\"English: {np.array(en_train).shape} \\t German: {ge_train.shape}\")\n",
        "print(\"The shape of validation dataset:\")\n",
        "print(f\"English: {np.array(en_valid).shape} \\t German: {ge_valid.shape}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of training dataset:\n",
            "English: (16000,) \t German: (16000, 14)\n",
            "The shape of validation dataset:\n",
            "English: (4000,) \t German: (4000, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w6rM8Bl1iad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9936ad92-5f1e-4e76-944c-ed3f724e6b0f"
      },
      "source": [
        "def create_dataset(input, target):\n",
        "    \"\"\"\n",
        "    Takes an input language, and a corresponding target language \n",
        "    to create a tf.data.Dataset object with these input and target.\n",
        "    \"\"\"\n",
        "    return tf.data.Dataset.from_tensor_slices((input, target))\n",
        "\n",
        "# Create Dataset object for both training and vailidation sets\n",
        "train_dataset = create_dataset(en_train, ge_train)\n",
        "valid_dataset = create_dataset(en_valid, ge_valid)\n",
        "\n",
        "# # Print the first 5 elements of the Dataset\n",
        "for elem in iter(train_dataset.take(5)):\n",
        "    print(elem[0].numpy(), elem[1].numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'read the article .' [   1  677   53 3076    9    2    0    0    0    0    0    0    0    0]\n",
            "b\"i'm doing ok .\" [   1   21   59   10 3429   49    3    2    0    0    0    0    0    0]\n",
            "b\"they're crazy .\" [  1  26  35   8  12 144  78   9   2   0   0   0   0   0]\n",
            "b'black suits you .' [  1 505 287  52  49   3   2   0   0   0   0   0   0   0]\n",
            "b\"you're not alone .\" [  1  13  32  12 524   3   2   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7bn3mRs1iaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fb8d6c-26e7-4e6d-f9e8-2792347605ba"
      },
      "source": [
        "def map_split(x, _):\n",
        "    \"\"\"\n",
        "    Splits each English sentence at spaces.\n",
        "    \"\"\"\n",
        "    return (tf.strings.split(x, sep=' '), _)\n",
        "\n",
        "# Map the function over the both Dataset objects. \n",
        "train_dataset = train_dataset.map(map_split)\n",
        "valid_dataset = valid_dataset.map(map_split)\n",
        "\n",
        "# # Print the first 5 elements of the Dataset\n",
        "for elem in iter(train_dataset.take(5)):\n",
        "    print(elem[0].numpy(), elem[1].numpy())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'read' b'the' b'article' b'.'] [   1  677   53 3076    9    2    0    0    0    0    0    0    0    0]\n",
            "[b\"i'm\" b'doing' b'ok' b'.'] [   1   21   59   10 3429   49    3    2    0    0    0    0    0    0]\n",
            "[b\"they're\" b'crazy' b'.'] [  1  26  35   8  12 144  78   9   2   0   0   0   0   0]\n",
            "[b'black' b'suits' b'you' b'.'] [  1 505 287  52  49   3   2   0   0   0   0   0   0   0]\n",
            "[b\"you're\" b'not' b'alone' b'.'] [  1  13  32  12 524   3   2   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Fdso381ian",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27db5e73-df8e-407f-8d2a-cbe1854c5637"
      },
      "source": [
        "def map_embed(x, _):\n",
        "    \"\"\"\n",
        "    Embeds each sequence of English words using the loaded embedding layer.\n",
        "    \"\"\"\n",
        "    return (embedding_layer(x), _)\n",
        "\n",
        "# Map the function over the both Dataset objects. \n",
        "train_dataset = train_dataset.map(map_embed)\n",
        "valid_dataset = valid_dataset.map(map_embed)\n",
        "\n",
        "# Print the element_spec property for both Datasets\n",
        "print(\"The training Dataset's element_spec property:\")\n",
        "print(train_dataset.element_spec)\n",
        "print(\"The validation Dataset's element_spec property:\")\n",
        "print(valid_dataset.element_spec)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training Dataset's element_spec property:\n",
            "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n",
            "The validation Dataset's element_spec property:\n",
            "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqlxuqZ1ias",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c797e8f-de8e-481d-e3ba-71f45e4988a2"
      },
      "source": [
        "def filter_length(x, _):\n",
        "    \"\"\"\n",
        "    Filter out dataset examples where the English sentence is \n",
        "    greater than or equal to 13 (embedded) tokens in length.\n",
        "    \"\"\"\n",
        "    return len(x)<13\n",
        "\n",
        "# Filter over the both Dataset objects.\n",
        "train_dataset = train_dataset.filter(filter_length)\n",
        "valid_dataset = valid_dataset.filter(filter_length)\n",
        "\n",
        "# Print the element_spec property for both Datasets\n",
        "print(\"The training Dataset's element_spec property:\")\n",
        "print(train_dataset.element_spec)\n",
        "print(\"The validation Dataset's element_spec property:\")\n",
        "print(valid_dataset.element_spec)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training Dataset's element_spec property:\n",
            "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n",
            "The validation Dataset's element_spec property:\n",
            "(TensorSpec(shape=(None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3Zlr2M0BBhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acaa6c5f-9eda-4e0a-b742-ec8463f30084"
      },
      "source": [
        "def map_pad_embedding(x, _):\n",
        "    \"\"\"\n",
        "    Pads each English sequence of embeddings with some distinct\n",
        "    padding value before the sequence, so that each sequence is\n",
        "    length 13.\n",
        "    \"\"\"\n",
        "    pad_size = tf.math.maximum(0, 13 - tf.shape(x)[0])\n",
        "    paddings = tf.concat(([[pad_size, 0]], [[0, 0]]), axis=0)\n",
        "    padded = tf.pad(x, paddings, \"CONSTANT\")\n",
        "    return (tf.reshape(padded, (13, 128)), _)\n",
        "\n",
        "# Map the function over the both Dataset objects. \n",
        "train_dataset = train_dataset.map(map_pad_embedding)\n",
        "valid_dataset = valid_dataset.map(map_pad_embedding)\n",
        "\n",
        "# Print the element_spec property for both Datasets\n",
        "print(\"The training Dataset's element_spec property:\")\n",
        "print(train_dataset.element_spec)\n",
        "print(\"The validation Dataset's element_spec property:\")\n",
        "print(valid_dataset.element_spec)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training Dataset's element_spec property:\n",
            "(TensorSpec(shape=(13, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n",
            "The validation Dataset's element_spec property:\n",
            "(TensorSpec(shape=(13, 128), dtype=tf.float32, name=None), TensorSpec(shape=(14,), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMslwlMYhNKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f569e1-42b2-4b19-d9f1-5b8f415fc743"
      },
      "source": [
        "def create_batch(dataset, batch_size):\n",
        "  \"\"\"\n",
        "  Batch the dataset using the batch_size argument, \n",
        "  setting drop_remainder to True.\n",
        "  \"\"\"\n",
        "  return dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# Batch the both Datasets with a batch size of 16.\n",
        "train_dataset = create_batch(train_dataset, 16)\n",
        "valid_dataset = create_batch(valid_dataset, 16)\n",
        "\n",
        "# Print the element_spec property for both Datasets\n",
        "print(\"The training Dataset's element_spec property:\")\n",
        "print(train_dataset.element_spec)\n",
        "print(\"The validation Dataset's element_spec property:\")\n",
        "print(valid_dataset.element_spec)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training Dataset's element_spec property:\n",
            "(TensorSpec(shape=(16, 13, 128), dtype=tf.float32, name=None), TensorSpec(shape=(16, 14), dtype=tf.int32, name=None))\n",
            "The validation Dataset's element_spec property:\n",
            "(TensorSpec(shape=(16, 13, 128), dtype=tf.float32, name=None), TensorSpec(shape=(16, 14), dtype=tf.int32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSocw5QhA9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83aa1d9-82cb-4bb3-d861-159dc641dd6f"
      },
      "source": [
        "# Print the shape of the English data example from the training Dataset\n",
        "en_train_batch, _ = next(iter(train_dataset.take(1)))\n",
        "print(\"The shape of English data example from the training Dataset:\", \n",
        "    en_train_batch.shape)\n",
        "\n",
        "# Print the German data example Tensor from the validation Dataset\n",
        "_, ge_valid_batch = next(iter(valid_dataset.take(1)))\n",
        "print(\"The German data example Tensor from the validation Dataset:\\n\", \n",
        "    ge_valid_batch.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of English data example from the training Dataset: (16, 13, 128)\n",
            "The German data example Tensor from the validation Dataset:\n",
            " [[   1    4  213   22   56 2575    3    2    0    0    0    0    0    0]\n",
            " [   1   30    8  112   92    7    2    0    0    0    0    0    0    0]\n",
            " [   1  673  334  780    9    2    0    0    0    0    0    0    0    0]\n",
            " [   1  134    4   21   11 5431    7    2    0    0    0    0    0    0]\n",
            " [   1    5  241  125    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    5   16 2084    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    5   16   34 4499    3    2    0    0    0    0    0    0    0]\n",
            " [   1    5   48  108  698   35    3    2    0    0    0    0    0    0]\n",
            " [   1    5 1824   65  814    3    2    0    0    0    0    0    0    0]\n",
            " [   1    5  308   65    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1    8 3090   22    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1 5460    7    2    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1    4  180  824    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1 1080  106 3971    3    2    0    0    0    0    0    0    0    0]\n",
            " [   1 1298    4  392  644    7    2    0    0    0    0    0    0    0]\n",
            " [   1    5 2920   34   12    3    2    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isIYhjq01iay"
      },
      "source": [
        "## 3. Create the custom layer\n",
        "You will now create a custom layer to add the learned end token embedding to the encoder model:\n",
        "\n",
        "![Encoder schematic](https://github.com/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/image/neural_translation/neural_translation_model_encoder.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6gLIHG81iaz"
      },
      "source": [
        "You should now build the custom layer.\n",
        "* Using layer subclassing, create a custom layer that takes a batch of English data examples from one of the Datasets, and adds a learned embedded ‘end’ token to the end of each sequence. \n",
        "* This layer should create a TensorFlow Variable (that will be learned during training) that is 128-dimensional (the size of the embedding space). _Hint: you may find it helpful in the call method to use the tf.tile function to replicate the end token embedding across every element in the batch._\n",
        "* Using the Dataset `.take(1)` method, extract a batch of English data examples from the training Dataset and print the shape. Test the custom layer by calling the layer on the English data batch Tensor and print the resulting Tensor shape (the layer should increase the sequence length by one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avionZuFghj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f2ecb6-cca7-49d7-99dc-81c5f4fad100"
      },
      "source": [
        "# Create a custom layer to add a learned end token embedding \n",
        "# to the encoder model using Layer subclassing API \n",
        "# according to the above specification.\n",
        "\n",
        "class EndToken(Layer):\n",
        "    \"\"\"\n",
        "    Takes a batch of English data examples from Datasets, and adds \n",
        "    a learned embedded 'end' token to the end of each sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(EndToken, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.end_var = self.add_weight(shape=(input_shape[-1],), \n",
        "            initializer=\"random_normal\", name=\"end\")\n",
        "        # self.end_token = tf.tile(self.end_var, (input_shape[0], 1))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.end_var\n",
        "        # return tf.concat([inputs, self.end_token], axis=1)\n",
        "\n",
        "# Instantiate the custom layer object\n",
        "end_token = EndToken()\n",
        "end_token(en_train_batch)\n",
        "end_token.trainable_variables"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'end_token_9/end:0' shape=(128,) dtype=float32, numpy=\n",
              " array([-4.22076173e-02,  2.62377802e-02, -2.49393079e-02, -5.87892942e-02,\n",
              "         4.06863913e-03,  6.80565462e-02, -1.32284939e-01, -6.91651776e-02,\n",
              "         1.00331707e-02, -5.53480946e-02,  1.75081994e-02,  5.31358086e-02,\n",
              "        -8.71938616e-02,  9.59187374e-03, -9.95273888e-02,  4.49895635e-02,\n",
              "         3.85218523e-02, -1.28584644e-02,  3.59058492e-02, -3.58383022e-02,\n",
              "        -5.62324002e-03,  6.71292767e-02,  1.00955777e-01, -9.73976031e-03,\n",
              "         1.64765790e-02,  5.14289029e-02,  6.91589341e-02, -5.68033941e-02,\n",
              "         2.17283261e-03, -5.25258295e-02,  2.23529171e-02, -2.36906111e-02,\n",
              "        -6.72826618e-02,  9.25394222e-02,  5.58651388e-02,  8.00540950e-03,\n",
              "         5.69629557e-02,  8.79428983e-02,  1.25540188e-02,  1.68147087e-02,\n",
              "        -7.35148266e-02,  3.30088213e-02, -3.78754847e-02, -7.99248070e-02,\n",
              "         4.02071849e-02, -8.28499570e-02, -1.11380359e-02,  4.60318588e-02,\n",
              "        -2.86451820e-03,  3.30726500e-03, -7.84756839e-02, -1.74477175e-02,\n",
              "        -2.22792663e-02,  8.95708799e-02, -3.19281816e-02,  6.02413975e-02,\n",
              "         2.88559143e-02, -1.34993587e-02,  1.71263646e-02,  6.24378286e-02,\n",
              "         1.28612937e-02,  1.02034509e-01, -3.81067470e-02,  1.16306022e-01,\n",
              "        -2.67834519e-03, -5.49507104e-02,  3.96050289e-02,  4.74164560e-02,\n",
              "        -4.64356281e-02,  2.70658676e-02,  2.07302477e-02,  4.99757640e-02,\n",
              "        -7.56081287e-03, -5.76177128e-02, -8.46331194e-02, -4.32938412e-02,\n",
              "         2.13838611e-02,  2.26318873e-02,  1.36465381e-03,  8.63485560e-02,\n",
              "         9.82061699e-02, -5.17597198e-02,  1.13442531e-02, -2.08828542e-02,\n",
              "        -5.74925244e-02,  2.98934337e-03, -3.97863379e-03,  7.58248493e-02,\n",
              "        -1.07641578e-01, -1.37660867e-02,  5.75756729e-02, -2.28347359e-04,\n",
              "        -1.11022346e-01,  1.79970022e-02,  5.91954105e-02, -2.97533418e-03,\n",
              "        -3.93011719e-02,  8.46366398e-04,  4.90648160e-03,  3.06340065e-02,\n",
              "         4.69970182e-02,  4.75793891e-02,  4.17816900e-02,  6.45112768e-02,\n",
              "        -8.16499591e-02,  4.38198773e-03, -3.02648749e-02,  2.50654109e-02,\n",
              "        -4.34939042e-02,  8.87611061e-02, -9.21842549e-03,  2.80408617e-02,\n",
              "         4.58162986e-02, -4.99299802e-02, -4.85888496e-03, -3.50225084e-02,\n",
              "         6.75487593e-02,  9.25059896e-03,  2.08274610e-02,  2.25170399e-03,\n",
              "         9.33203697e-02,  8.39788541e-02,  1.92327108e-02, -8.39698696e-05,\n",
              "        -6.95885494e-02,  1.05850426e-02,  1.12695746e-01, -2.20725313e-02],\n",
              "       dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm-vBbGh5Zar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc389c87-d78e-4dac-f55a-08daf1738cbb"
      },
      "source": [
        "# Extract a batch of English data examples from the training Dataset.\n",
        "en_train_batch, _ = next(iter(train_dataset.take(1)))\n",
        "# Print the shape before the custom layer\n",
        "print(\"English data batch Tensor shape before the custom end_token layer:\", \n",
        "    en_train_batch.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data batch Tensor shape before the custom end_token layer: (16, 13, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebjK8PmL1ia6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "bbf117c6-a31a-4cf3-b914-ec6148d02a5a"
      },
      "source": [
        "# Test the custom layer on the English data batch Tensor \n",
        "# and print the resulting Tensor shape\n",
        "print(\"English data batch Tensor shape after the custom end_token layer:\", \n",
        "    end_token(en_train_batch).shape)\n",
        "\n",
        "# Print the number of trainable variables\n",
        "print('Number of trainable variables in the end_token layer:', \n",
        "    len(end_token.trainable_variables))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-abe81e63acbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and print the resulting Tensor shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print(\"English data batch Tensor shape after the custom end_token layer:\", \n\u001b[0;32m----> 4\u001b[0;31m     end_token(en_train_batch).shape)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Print the number of trainable variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2710\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2711\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-ccb2d97dda1b>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         self.end_var = self.add_weight(shape=(None, input_shape[-1]), \n\u001b[0;32m---> 15\u001b[0;31m             initializer=\"random_normal\", name=\"end\")\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    640\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2619\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1583\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1710\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m               \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointInitialValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_initialize_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[1;32m    304\u001b[0m     return super(RandomNormal, self).__call__(\n\u001b[0;32m--> 305\u001b[0;31m         shape, dtype=_get_dtype(dtype), **kwargs)\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_PARTITION_SHAPE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     return self._random_generator.random_normal(shape, self.mean, self.stddev,\n\u001b[0;32m--> 420\u001b[0;31m                                                 dtype)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(self, shape, mean, stddev, dtype)\u001b[0m\n\u001b[1;32m   1071\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     return op(\n\u001b[0;32m-> 1073\u001b[0;31m         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m     88\u001b[0m   \"\"\"\n\u001b[1;32m     89\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mshape_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mmean_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mstddev_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stddev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mshape_tensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1033\u001b[0m       \u001b[0;31m# not convertible to Tensors because of mixed content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                          as_ref=False):\n\u001b[1;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAd3i4_y1ia-"
      },
      "source": [
        "## 4. Build the encoder network\n",
        "The encoder network follows the schematic diagram above. You should now build the RNN encoder model.\n",
        "* Using the functional API, build the encoder network according to the following spec:\n",
        "    * The model will take a batch of sequences of embedded English words as input, as given by the Dataset objects.\n",
        "    * The next layer in the encoder will be the custom layer you created previously, to add a learned end token embedding to the end of the English sequence.\n",
        "    * This is followed by a Masking layer, with the `mask_value` set to the distinct padding value you used when you padded the English sequences with the Dataset preprocessing above.\n",
        "    * The final layer is an LSTM layer with 512 units, which also returns the hidden and cell states.\n",
        "    * The encoder is a multi-output model. There should be two output Tensors of this model: the hidden state and cell states of the LSTM layer. The output of the LSTM layer is unused.\n",
        "* Using the Dataset `.take(1)` method, extract a batch of English data examples from the training Dataset and test the encoder model by calling it on the English data Tensor, and print the shape of the resulting Tensor outputs.\n",
        "* Print the model summary for the encoder network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MrOS1LoFyA9"
      },
      "source": [
        "# Build a RNN encoder model using the functional API\n",
        "# according to the above specification.\n",
        "\n",
        "# A batch of sequences of embedded English words from Dataset object as input.\n",
        "inputs = Input(batch_shape=train_dataset.element_spec[0].shape)\n",
        "# Add a learned end token embedding to the end of \n",
        "# the English sequence using the end_token layer.\n",
        "h = end_token(inputs)\n",
        "# A Masking layer with the mask_value=0.0.\n",
        "h = Masking(mask_value=0.0)(h)\n",
        "# A LSTM layer with 512 units returns the hidden and cell states.\n",
        "_, hidden, cell = LSTM(units=512, return_state=True)(h)\n",
        "\n",
        "# Create the RNN encoder model object\n",
        "encoder = Model(inputs=inputs, outputs=[hidden, cell])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5XW6NxL1ibC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acbb468-322c-4c79-8142-e452e41bcd7d"
      },
      "source": [
        "# Extract a batch of English data examples from the training Dataset.\n",
        "en_train_batch, _ = next(iter(train_dataset.take(1)))\n",
        "\n",
        "# Test the encoder model on the English data tensor, \n",
        "# and print the shape of the resulting Tensor outputs.\n",
        "enc_hidden, enc_cell = encoder(en_train_batch)\n",
        "print(\"The shape of Encoder hidden state output: (batch_size, units)\", \n",
        "    enc_hidden.shape)\n",
        "print(\"The shape of Encoder cell state output: (batch_size, units)\", \n",
        "    enc_cell.shape)\n",
        "\n",
        "# Print the number of trainable variables\n",
        "print('Number of trainable variables in the RNN encoder_model:', \n",
        "    len(encoder.trainable_variables))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of Encoder hidden state output: (batch_size, units) (16, 512)\n",
            "The shape of Encoder cell state output: (batch_size, units) (16, 512)\n",
            "Number of trainable variables in the RNN encoder_model: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEk9ikVh1ibL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8f5390-54d8-44f6-b0b6-58dfa8cf4e78"
      },
      "source": [
        "# Print the model summary for the encoder network\n",
        "encoder.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(16, 13, 128)]           0         \n",
            "_________________________________________________________________\n",
            "end_token (EndToken)         (16, 14, 128)             128       \n",
            "_________________________________________________________________\n",
            "masking (Masking)            (16, 14, 128)             0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(16, 512), (16, 512), (1 1312768   \n",
            "=================================================================\n",
            "Total params: 1,312,896\n",
            "Trainable params: 1,312,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9vcSXtu4hLp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "1e0c2e22-0a52-4eaa-e7ef-19c824ec7ac9"
      },
      "source": [
        "# Plot the Encoder RNN model\n",
        "plot_model(encoder, 'encoder.png', show_shapes=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvkzpCeZ1ibR"
      },
      "source": [
        "## 5. Build the decoder network\n",
        "The decoder network follows the schematic diagram below.\n",
        "\n",
        "![Decoder schematic](https://github.com/MengOonLee/Deep_learning/blob/master/TensorFlow2/Tutorial/image/neural_translation/neural_translation_model_decoder.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPBK8WGL1ibS"
      },
      "source": [
        "You should now build the RNN decoder model.\n",
        "* Using Model subclassing, build the decoder network according to the following spec:\n",
        "    * The initializer should create the following layers:\n",
        "        * An Embedding layer with vocabulary size set to the number of unique German tokens, embedding dimension 128, and set to mask zero values in the input.\n",
        "        * An LSTM layer with 512 units, that returns its hidden and cell states, and also returns sequences.\n",
        "        * A Dense layer with number of units equal to the number of unique German tokens, and no activation function.\n",
        "    * The call method should include the usual `inputs` argument, as well as the additional keyword arguments `hidden_state` and `cell_state`. The default value for these keyword arguments should be `None`.\n",
        "    * The call method should pass the inputs through the Embedding layer, and then through the LSTM layer. If the `hidden_state` and `cell_state` arguments are provided, these should be used for the initial state of the LSTM layer. _Hint: use the_ `initial_state` _keyword argument when calling the LSTM layer on its input._\n",
        "    * The call method should pass the LSTM output sequence through the Dense layer, and return the resulting Tensor, along with the hidden and cell states of the LSTM layer.\n",
        "* Using the Dataset `.take(1)` method, extract a batch of English and German data examples from the training Dataset. Test the decoder model by first calling the encoder model on the English data Tensor to get the hidden and cell states, and then call the decoder model on the German data Tensor and hidden and cell states, and print the shape of the resulting decoder Tensor outputs.\n",
        "* Print the model summary for the decoder network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l50qhnXD1ibT"
      },
      "source": [
        "# Build an RNN decoder model using Model subclassing API\n",
        "# according to the above specification.\n",
        "\n",
        "class Decoder(Model):\n",
        "    \"\"\"\n",
        "    Takes a batch of German data Tensor from the Dataset through an \n",
        "    Embedding layer with vocab_size set to number of unique German \n",
        "    token. It is followed by an LSTM layer with initial state are \n",
        "    provided by the hidden and cell states from the RNN encoder model on \n",
        "    the English data Tensor. The LSTM output sequence is passed through \n",
        "    the final Dense layer, which returns the resulting Tensor, along with \n",
        "    the hidden and cell states of the LSTM layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        # Define layers\n",
        "        self.embedding = Embedding(input_dim=vocab_size, \n",
        "            output_dim=128, mask_zero=True)\n",
        "        self.lstm = LSTM(units=512, return_state=True, \n",
        "            return_sequences=True)\n",
        "        self.dense = Dense(units=vocab_size, activation=None)\n",
        "\n",
        "    def call(self, inputs, states=None):\n",
        "        # Define forward pass\n",
        "        h = self.embedding(inputs)\n",
        "        if states is None:\n",
        "            states = self.lstm.get_initial_state(x)\n",
        "        h, _, _ = self.lstm(h, initial_state=states)\n",
        "        outputs = self.dense(h)\n",
        "        return outputs\n",
        "\n",
        "# Vocabulary size set to the number of unique German tokens\n",
        "vocab_size = len(ge_tokenizer.word_index) + 1\n",
        "# Instantiate the RNN decoder model object\n",
        "decoder = Decoder(vocab_size=vocab_size)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG5CRIXV1ibi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e546c510-24bc-4a4f-e64e-07c0fd59e812"
      },
      "source": [
        "# Extract a batch of English and German data examples \n",
        "# from the training Dataset\n",
        "en_train_batch, ge_train_batch = next(iter(train_dataset.take(1)))\n",
        "\n",
        "# First call the encoder model on the English data Tensor\n",
        "# to get the hidden and cell states\n",
        "enc_hidden, enc_cell = encoder(en_train_batch)\n",
        "\n",
        "# Then test the decoder model on the German data Tensor,\n",
        "# the hidden and cell states from the encoder model, \n",
        "# and print the shape of the resulting decoder Tensor outputs.\n",
        "dec_outputs = decoder(ge_train_batch, \n",
        "    states=[enc_hidden, enc_cell])\n",
        "print(\"The shape of Decoder outputs: (batch_size, sequence_length, units)\", \n",
        "    dec_outputs.shape)\n",
        "\n",
        "# Print the number of trainable variables\n",
        "print('Number of trainable variables in the RNN decoder_model:', \n",
        "    len(decoder.trainable_variables))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of Decoder outputs: (batch_size, sequence_length, units) (16, 14, 5744)\n",
            "Number of trainable variables in the RNN decoder_model: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvqqT_ET1ibl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d03452-1fd0-4204-8bd4-6225e3459357"
      },
      "source": [
        "# Print the model summary for the decoder network.\n",
        "decoder.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  735232    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  1312768   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  2946672   \n",
            "=================================================================\n",
            "Total params: 4,994,672\n",
            "Trainable params: 4,994,672\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pST9XGJ81ibo"
      },
      "source": [
        "## 6. Make a custom training loop\n",
        "You should now write a custom training loop to train your custom neural translation model.\n",
        "* Define a function that takes a Tensor batch of German data (as extracted from the training Dataset), and returns a tuple containing German inputs and outputs for the decoder model (refer to schematic diagram above).\n",
        "* Define a function that computes the forward and backward pass for your translation model. This function should take an English input, German input and German output as arguments, and should do the following:\n",
        "    * Pass the English input into the encoder, to get the hidden and cell states of the encoder LSTM.\n",
        "    * These hidden and cell states are then passed into the decoder, along with the German inputs, which returns a sequence of outputs (the hidden and cell state outputs of the decoder LSTM are unused in this function).\n",
        "    * The loss should then be computed between the decoder outputs and the German output function argument.\n",
        "    * The function returns the loss and gradients with respect to the encoder and decoder’s trainable variables.\n",
        "    * Decorate the function with `@tf.function`\n",
        "* Define and run a custom training loop for a number of epochs (for you to choose) that does the following:\n",
        "    * Iterates through the training dataset, and creates decoder inputs and outputs from the German sequences.\n",
        "    * Updates the parameters of the translation model using the gradients of the function above and an optimizer object.\n",
        "    * Every epoch, compute the validation loss on a number of batches from the validation and save the epoch training and validation losses.\n",
        "* Plot the learning curves for loss vs epoch for both training and validation sets.\n",
        "\n",
        "_Hint: This model is computationally demanding to train. The quality of the model or length of training is not a factor in the grading rubric. However, to obtain a better model we recommend using the GPU accelerator hardware on Colab._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJHbWqs1ibr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054563ad-3cbe-4e0b-9091-595663815bb6"
      },
      "source": [
        "def split_inputs_targets(sequences):\n",
        "    \"\"\"\n",
        "    Takes an array of token sequences, and returns \n",
        "    a tuple of decoder inputs and targets.\n",
        "    \"\"\"\n",
        "    dec_inputs = sequences[:, :-1]\n",
        "    dec_targets = sequences[:, 1:]\n",
        "    return dec_inputs, dec_targets\n",
        "\n",
        "# Extract a Tensor batch of German data from the training Dataset.\n",
        "_, ge_train_batch = next(iter(train_dataset.take(1)))\n",
        "\n",
        "# Create a tuple containing German inputs and targets \n",
        "# for the decoder model.\n",
        "dec_inputs, dec_targets = split_inputs_targets(ge_train_batch)\n",
        "\n",
        "# Print the decoder inputs.\n",
        "print(\"The decoder inputs:\\n\", dec_inputs.numpy())\n",
        "# Print the decoder targets.\n",
        "print(\"The decoder targets:\\n\", dec_targets.numpy())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The decoder inputs:\n",
            " [[   1  677   53 3076    9    2    0    0    0    0    0    0    0]\n",
            " [   1   21   59   10 3429   49    3    2    0    0    0    0    0]\n",
            " [   1   26   35    8   12  144   78    9    2    0    0    0    0]\n",
            " [   1  505  287   52   49    3    2    0    0    0    0    0    0]\n",
            " [   1   13   32   12  524    3    2    0    0    0    0    0    0]\n",
            " [   1    5  416   12    3    2    0    0    0    0    0    0    0]\n",
            " [   1   17  228   98    3    2    0    0    0    0    0    0    0]\n",
            " [   1    5   16 1817    3    2    0    0    0    0    0    0    0]\n",
            " [   1    4   30  507   12  504    3    2    0    0    0    0    0]\n",
            " [   1   11    6   12 1666    3    2    0    0    0    0    0    0]\n",
            " [   1   15    4   20  219    7    2    0    0    0    0    0    0]\n",
            " [   1    5   16  223 1309    3    2    0    0    0    0    0    0]\n",
            " [   1  140   27    5  390    7    2    0    0    0    0    0    0]\n",
            " [   1   21  353  121  415    3    2    0    0    0    0    0    0]\n",
            " [   1   14   30   10   12   86    3    2    0    0    0    0    0]\n",
            " [   1   14 1675   46   47 2829    3    2    0    0    0    0    0]]\n",
            "The decoder targets:\n",
            " [[ 677   53 3076    9    2    0    0    0    0    0    0    0    0]\n",
            " [  21   59   10 3429   49    3    2    0    0    0    0    0    0]\n",
            " [  26   35    8   12  144   78    9    2    0    0    0    0    0]\n",
            " [ 505  287   52   49    3    2    0    0    0    0    0    0    0]\n",
            " [  13   32   12  524    3    2    0    0    0    0    0    0    0]\n",
            " [   5  416   12    3    2    0    0    0    0    0    0    0    0]\n",
            " [  17  228   98    3    2    0    0    0    0    0    0    0    0]\n",
            " [   5   16 1817    3    2    0    0    0    0    0    0    0    0]\n",
            " [   4   30  507   12  504    3    2    0    0    0    0    0    0]\n",
            " [  11    6   12 1666    3    2    0    0    0    0    0    0    0]\n",
            " [  15    4   20  219    7    2    0    0    0    0    0    0    0]\n",
            " [   5   16  223 1309    3    2    0    0    0    0    0    0    0]\n",
            " [ 140   27    5  390    7    2    0    0    0    0    0    0    0]\n",
            " [  21  353  121  415    3    2    0    0    0    0    0    0    0]\n",
            " [  14   30   10   12   86    3    2    0    0    0    0    0    0]\n",
            " [  14 1675   46   47 2829    3    2    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvu4J4-u1ibu"
      },
      "source": [
        "# Define the Adam optimizer and categorical cross entropy loss \n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True)\n",
        "# Define the encoder and decoder's trainable variables\n",
        "train_variables = encoder.trainable_variables + \\\n",
        "    decoder.trainable_variables\n",
        "\n",
        "# Define a function that computes the forward and backward pass \n",
        "# for the translation model according to the above specification.\n",
        "# Use the @tf.function decorator\n",
        "@tf.function\n",
        "def get_loss_and_grad(enc_inputs, dec_inputs, dec_targets):\n",
        "    \"\"\"\n",
        "    Compute a loss and gradient of the encoder and decoder models, \n",
        "    corresponding to the encoder inputs, decoder inputs and targets.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass enc_inputs into the encoder to get \n",
        "        # enc_hidden and enc_cell states of the encoder LSTM.\n",
        "        enc_hidden, enc_cell = encoder(enc_inputs)\n",
        "        # Pass enc_hidden and enc_cell states into the decoder, \n",
        "        # along with dec_inputs to get dec_outputs\n",
        "        dec_outputs = decoder(dec_inputs, \n",
        "            states=[enc_hidden, enc_cell])\n",
        "        # Compute loss between dec_targets and dec_outputs\n",
        "        loss = loss_object(y_true=dec_targets, y_pred=dec_outputs)\n",
        "        # Compute gradient with respect to the trainable variables\n",
        "        grad = tape.gradient(loss, train_variables)\n",
        "    return loss, grad"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl5H06811ibx"
      },
      "source": [
        "# Define a custom training loop for a number of epochs\n",
        "# according to the above specification\n",
        "\n",
        "def train_step(num_epochs, dataset):\n",
        "    \"\"\"\n",
        "    Implement a custom training loop and return \n",
        "    a list of loss values per epoch.\n",
        "    \"\"\"\n",
        "    # Keep results for plotting\n",
        "    train_loss = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss_avg = Mean()\n",
        "        # Training loop\n",
        "        for x, y in dataset:\n",
        "            dec_inputs, dec_targets = split_inputs_targets(y)\n",
        "            # Optimize the model\n",
        "            loss, grad = get_loss_and_grad(x, dec_inputs, dec_targets)\n",
        "            optimizer.apply_gradients(zip(grad, train_variables))\n",
        "            # Compute current avg loss\n",
        "            epoch_loss_avg(loss)\n",
        "\n",
        "        # End epoch\n",
        "        train_loss.append(epoch_loss_avg.result())\n",
        "        print(\"Epoch {:03d}: Loss: {.3f}\".format(epoch, \n",
        "            epoch_loss_avg.result()))\n",
        "        \n",
        "    return train_loss"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWOYJ3l1ib1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "eb6d684a-90e9-4faf-c10c-79415156e4fd"
      },
      "source": [
        "# Execute the training process\n",
        "train_loss = train_step(1, train_dataset)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['end_token/end:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-72a34c03c481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-dedb8f4ae51b>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(num_epochs, dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mepoch_loss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_inputs_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2574\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2575\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2577\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VSmBdwT1ib4",
        "outputId": "3c3cf1a1-4683-47dd-97b0-e810d712f084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_variables"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'end_token/end:0' shape=(1, 1, 128) dtype=float32, numpy=\n",
              " array([[[ 3.60508449e-03,  3.32776383e-02, -2.18666829e-02,\n",
              "           4.90608774e-02,  2.97714006e-02, -2.89873034e-02,\n",
              "           7.29605928e-02,  1.34019842e-02, -1.49647146e-02,\n",
              "           2.25972123e-02,  2.23468412e-02,  2.51673199e-02,\n",
              "          -1.55287553e-02,  2.89713088e-02,  1.69046037e-02,\n",
              "          -8.93733501e-02, -6.70382604e-02,  2.63220519e-02,\n",
              "          -4.43839096e-03, -8.42819083e-03,  2.94174049e-02,\n",
              "           1.54674873e-02,  7.45686993e-05, -3.44239436e-02,\n",
              "           9.17189696e-04,  3.02985054e-03, -6.63999021e-02,\n",
              "           1.95319857e-02,  1.15739815e-02, -3.70398094e-03,\n",
              "          -1.15530990e-01,  1.95093192e-02,  5.10204397e-02,\n",
              "           4.50718664e-02,  1.10462867e-01, -5.95373772e-02,\n",
              "          -1.29670752e-02, -1.14497924e-02,  2.71223187e-02,\n",
              "          -1.05020106e-01,  1.58477842e-03,  3.73966061e-02,\n",
              "          -1.30177019e-02,  2.06384938e-02,  1.01064043e-02,\n",
              "          -8.70753452e-02, -1.61993094e-02, -7.40021169e-02,\n",
              "          -1.47439269e-02, -2.52671279e-02, -3.31305154e-02,\n",
              "          -2.01438162e-02,  1.11033078e-02,  1.02647310e-02,\n",
              "           9.95684694e-03,  2.77688894e-02,  4.81028855e-02,\n",
              "          -3.08134016e-02, -8.00026283e-02, -1.23051321e-02,\n",
              "           4.80145635e-03, -1.07585257e-02, -8.17956682e-03,\n",
              "          -6.66807815e-02, -2.95577496e-02, -3.98252122e-02,\n",
              "           1.01769447e-01, -4.29283231e-02,  3.10525997e-03,\n",
              "           4.47282866e-02,  6.06435072e-03, -3.63498740e-02,\n",
              "           3.68764922e-02,  1.31655321e-01,  8.71009368e-04,\n",
              "           6.56318897e-03,  1.21232783e-02,  6.73300922e-02,\n",
              "          -1.62471291e-02, -4.84005474e-02,  3.69367236e-03,\n",
              "           1.16149142e-01, -3.81211080e-02,  2.73091234e-02,\n",
              "          -3.14940810e-02, -3.24403606e-02, -1.56071009e-02,\n",
              "           3.20387520e-02,  4.70485538e-02,  6.59274636e-03,\n",
              "           1.51212169e-02,  2.37235911e-02,  1.01344049e-01,\n",
              "          -1.21576581e-02,  6.46922588e-02, -8.43184348e-03,\n",
              "          -6.37413934e-02,  2.62149307e-03,  2.42380463e-02,\n",
              "          -3.60103436e-02,  1.17729716e-01,  2.38598580e-03,\n",
              "          -1.65849477e-02,  6.35955706e-02, -5.72749376e-02,\n",
              "           4.09502201e-02,  5.16378060e-02,  2.62201466e-02,\n",
              "           8.68646353e-02,  2.70610098e-02,  1.19089857e-02,\n",
              "           5.68701588e-02,  5.64332493e-02, -2.60491055e-02,\n",
              "          -3.68432328e-02,  8.86840671e-02, -5.60510755e-02,\n",
              "          -2.60566808e-02,  1.09779714e-02, -1.24001242e-02,\n",
              "           2.11928301e-02, -2.28140615e-02,  1.63097251e-02,\n",
              "           5.49589396e-02,  3.89253348e-02,  2.46617775e-02,\n",
              "           1.14706457e-01, -8.69385377e-02]]], dtype=float32)>,\n",
              " <tf.Variable 'lstm/lstm_cell/kernel:0' shape=(128, 2048) dtype=float32, numpy=\n",
              " array([[-0.01364357,  0.04541694, -0.0301047 , ...,  0.00988616,\n",
              "          0.04105353, -0.001246  ],\n",
              "        [ 0.05214559,  0.04113521, -0.03417934, ...,  0.01979422,\n",
              "          0.02134707, -0.03501596],\n",
              "        [-0.03152803, -0.0051338 ,  0.00977162, ...,  0.02044909,\n",
              "         -0.00135589,  0.00274577],\n",
              "        ...,\n",
              "        [ 0.01627442,  0.03335525, -0.04824112, ..., -0.03013639,\n",
              "         -0.05135049,  0.04394704],\n",
              "        [ 0.01835735, -0.04350417,  0.03571747, ..., -0.00612275,\n",
              "         -0.01050752, -0.01620971],\n",
              "        [-0.05070387, -0.01605262,  0.02240109, ...,  0.05089156,\n",
              "         -0.00793611, -0.00451319]], dtype=float32)>,\n",
              " <tf.Variable 'lstm/lstm_cell/recurrent_kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
              " array([[-0.02811499,  0.0518422 ,  0.03216044, ...,  0.01767048,\n",
              "          0.03548734,  0.01740513],\n",
              "        [-0.00421888, -0.00447697,  0.01928324, ...,  0.00606928,\n",
              "         -0.01795546, -0.03386224],\n",
              "        [ 0.00936297, -0.02095632, -0.03426414, ..., -0.00883122,\n",
              "         -0.00599135, -0.00848204],\n",
              "        ...,\n",
              "        [-0.03850673, -0.01373505,  0.04795888, ..., -0.02035152,\n",
              "          0.02950976,  0.00234211],\n",
              "        [ 0.0326699 ,  0.00621229, -0.03026466, ..., -0.03052214,\n",
              "          0.02074517,  0.00363244],\n",
              "        [ 0.01267857, -0.01016773,  0.00561044, ...,  0.04409415,\n",
              "          0.01698481,  0.00321949]], dtype=float32)>,\n",
              " <tf.Variable 'lstm/lstm_cell/bias:0' shape=(2048,) dtype=float32, numpy=\n",
              " array([0.00404117, 0.00287452, 0.00505245, ..., 0.00257514, 0.00319315,\n",
              "        0.00207164], dtype=float32)>,\n",
              " <tf.Variable 'decoder/embedding/embeddings:0' shape=(5744, 128) dtype=float32, numpy=\n",
              " array([[ 0.00693794,  0.02815292, -0.0380728 , ...,  0.03629461,\n",
              "          0.03742779,  0.04265673],\n",
              "        [ 0.05207553,  0.04455885, -0.00832341, ...,  0.03408431,\n",
              "         -0.00841947, -0.00862864],\n",
              "        [-0.01031681,  0.04224729,  0.04843474, ...,  0.03606573,\n",
              "          0.02477345, -0.03679099],\n",
              "        ...,\n",
              "        [ 0.01105231, -0.01052713, -0.02387687, ..., -0.02955076,\n",
              "          0.04646801,  0.02081683],\n",
              "        [ 0.04936479, -0.02184151,  0.00810836, ...,  0.01477542,\n",
              "         -0.04816312, -0.03017161],\n",
              "        [-0.00336723,  0.04957923, -0.02998201, ...,  0.02786082,\n",
              "          0.01199119,  0.0195534 ]], dtype=float32)>,\n",
              " <tf.Variable 'decoder/lstm_1/lstm_cell_1/kernel:0' shape=(128, 2048) dtype=float32, numpy=\n",
              " array([[-0.04362523, -0.01296818,  0.02802354, ...,  0.03723316,\n",
              "         -0.00901245,  0.02119285],\n",
              "        [ 0.05075628,  0.0121352 ,  0.03186796, ...,  0.00331619,\n",
              "         -0.0481561 , -0.03191583],\n",
              "        [ 0.03631905, -0.00948377,  0.02009779, ...,  0.00013846,\n",
              "          0.03273764,  0.05059401],\n",
              "        ...,\n",
              "        [ 0.0038108 ,  0.04672695,  0.0507159 , ..., -0.02698301,\n",
              "          0.00871051,  0.00631904],\n",
              "        [ 0.00103577,  0.04624332, -0.05124824, ..., -0.00685685,\n",
              "         -0.02499629, -0.03106743],\n",
              "        [-0.04198274,  0.03274906,  0.00630578, ..., -0.02805099,\n",
              "         -0.03343724, -0.04195847]], dtype=float32)>,\n",
              " <tf.Variable 'decoder/lstm_1/lstm_cell_1/recurrent_kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
              " array([[ 0.00189314, -0.0375076 , -0.0005851 , ...,  0.01640761,\n",
              "          0.02975073,  0.06022106],\n",
              "        [ 0.01083247,  0.01565547,  0.02322439, ...,  0.02850865,\n",
              "         -0.0337944 , -0.02882743],\n",
              "        [ 0.00073303, -0.03957584,  0.04783867, ...,  0.0268339 ,\n",
              "         -0.00172491,  0.02987565],\n",
              "        ...,\n",
              "        [ 0.02908914,  0.01729278,  0.02966571, ..., -0.01161243,\n",
              "          0.01023201, -0.02888903],\n",
              "        [ 0.02303105, -0.00945874, -0.00413692, ..., -0.02211248,\n",
              "         -0.02111735, -0.01551542],\n",
              "        [-0.02392052,  0.00727292, -0.01809176, ...,  0.00792696,\n",
              "         -0.00479083,  0.02799644]], dtype=float32)>,\n",
              " <tf.Variable 'decoder/lstm_1/lstm_cell_1/bias:0' shape=(2048,) dtype=float32, numpy=\n",
              " array([0.0031058 , 0.00478154, 0.002892  , ..., 0.00309271, 0.00052673,\n",
              "        0.00459623], dtype=float32)>,\n",
              " <tf.Variable 'decoder/dense/kernel:0' shape=(512, 5744) dtype=float32, numpy=\n",
              " array([[-0.02138128,  0.02602107,  0.01689323, ...,  0.00577585,\n",
              "         -0.00716856,  0.03010017],\n",
              "        [-0.0046355 ,  0.01087875,  0.00180341, ...,  0.01198965,\n",
              "         -0.00578582, -0.00166067],\n",
              "        [-0.02771896,  0.0133745 ,  0.01992657, ...,  0.01574807,\n",
              "          0.02665486,  0.00017063],\n",
              "        ...,\n",
              "        [ 0.00307828,  0.02783181, -0.01500895, ...,  0.00032096,\n",
              "          0.02444829,  0.00012541],\n",
              "        [-0.03049289,  0.0150949 , -0.01998165, ..., -0.02384917,\n",
              "         -0.02278106,  0.0316619 ],\n",
              "        [-0.0048069 ,  0.02883592, -0.03516597, ..., -0.01840258,\n",
              "          0.02337156,  0.0260558 ]], dtype=float32)>,\n",
              " <tf.Variable 'decoder/dense/bias:0' shape=(5744,) dtype=float32, numpy=\n",
              " array([ 0.00599509, -0.00593494,  0.0059998 , ..., -0.00593905,\n",
              "        -0.005934  , -0.00593588], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpacst2F1ib7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM2gvBM11ib-"
      },
      "source": [
        "## 7. Use the model to translate\n",
        "Now it's time to put your model into practice! You should run your translation for five randomly sampled English sentences from the dataset. For each sentence, the process is as follows:\n",
        "* Preprocess and embed the English sentence according to the model requirements.\n",
        "* Pass the embedded sentence through the encoder to get the encoder hidden and cell states.\n",
        "* Starting with the special  `\"<start>\"` token, use this token and the final encoder hidden and cell states to get the one-step prediction from the decoder, as well as the decoder’s updated hidden and cell states.\n",
        "* Create a loop to get the next step prediction and updated hidden and cell states from the decoder, using the most recent hidden and cell states. Terminate the loop when the `\"<end>\"` token is emitted, or when the sentence has reached a maximum length.\n",
        "* Decode the output token sequence into German text and print the English text and the model's German translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnQtE7L1icA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohleJRcJ1icD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unk60cEy1icI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}