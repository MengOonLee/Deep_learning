{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "tensor-flow-2-2",
      "graded_item_id": "2x3vn",
      "launcher_item_id": "QKXZc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ResidualMNIST.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow/Customise/SubclassCustom/ResidualMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTehttYqbiZO"
      },
      "source": [
        "# Programming Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d-2RopbiZQ"
      },
      "source": [
        "## Residual network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK03DPBRbiZR"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In this notebook, you will use the model subclassing API together with custom layers to create a residual network architecture. You will then train your custom model on the Fashion-MNIST dataset by using a custom training loop and implementing the automatic differentiation tools in Tensorflow to calculate the gradients for backpropagation.\n",
        "\n",
        "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line:\n",
        "\n",
        "`#### GRADED CELL ####`\n",
        "\n",
        "Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly. Inside these graded cells, you can use any functions or classes that are imported below, but make sure you don't use any variables that are outside the scope of the function.\n",
        "\n",
        "### How to submit\n",
        "\n",
        "Complete all the tasks you are asked for in the worksheet. When you have finished and are happy with your code, press the **Submit Assignment** button at the top of this notebook.\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "We'll start running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LURF_ZYbiZX"
      },
      "source": [
        "![Fashion-MNIST overview image](https://github.com/MengOonLee/Deep_learning/blob/master/TensorFlow/image/fashion_mnist/fashion_mnist.png?raw=1)\n",
        "\n",
        "#### The Fashion-MNIST dataset\n",
        "\n",
        "In this assignment, you will use the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). It consists of a training set of 60,000 images of fashion items with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The dataset is frequently used in machine learning research, especially as a drop-in replacement for the MNIST dataset.\n",
        "\n",
        "- H. Xiao, K. Rasul, and R. Vollgraf. \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\" arXiv:1708.07747, August 2017.\n",
        "\n",
        "Your goal is to construct a ResNet model that classifies images of fashion items into one of 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(seed=42)\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "DX23PusaZcdL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JikW1jd8biZY"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Z9Eoq4biZZ"
      },
      "source": [
        "For this programming assignment, we will take a smaller sample of the dataset to reduce the training time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets\\\n",
        "    .fashion_mnist.load_data()\n",
        "\n",
        "inputs = tf.keras.Input(shape=(28, 28), dtype=tf.float32)\n",
        "h = tf.keras.layers.Rescaling(scale=1./255)(inputs)\n",
        "outputs = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(h)\n",
        "preprocessing_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Create Dataset objects for the training and test sets\n",
        "def get_ds(x, y, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(tensors=(x, y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(y))\n",
        "    ds = ds.batch(batch_size=64)\n",
        "    ds = ds.map(map_func=lambda x, y: (preprocessing_model(x), y),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "ds_train = get_ds(x=x_train, y=y_train, shuffle=True)\n",
        "ds_test = get_ds(x=x_test, y=y_test)\n",
        "ds_test.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7t2HQviay0i",
        "outputId": "8e1c7795-de19-482b-cf59-a96da6ccdc16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.uint8, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UKYy3Q1biZk"
      },
      "source": [
        "#### Create custom layers for the residual blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqm8M28ZbiZl"
      },
      "source": [
        "You should now create a first custom layer for a residual block of your network. Using layer subclassing, build your custom layer according to the following spec:\n",
        "\n",
        "* The custom layer class should have `__init__`, `build` and `call` methods. The `__init__` method has been completed for you. It calls the base `Layer` class initializer, passing on any keyword arguments\n",
        "* The `build` method should create the layers. It will take an `input_shape` argument, and should extract the number of filters from this argument. It should create:\n",
        "    * A BatchNormalization layer: this will be the first layer in the block, so should use its `input_shape` keyword argument\n",
        "    * A Conv2D layer with the same number of filters as the layer input, a 3x3 kernel size, `'SAME'` padding, and no activation function\n",
        "    * Another BatchNormalization layer\n",
        "    * Another Conv2D layer, again with the same number of filters as the layer input, a 3x3 kernel size, `'SAME'` padding, and no activation function\n",
        "* The `call` method should then process the input through the layers:\n",
        "    * The first BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * A `tf.nn.relu` activation function\n",
        "    * The first Conv2D layer\n",
        "    * The second BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * Another `tf.nn.relu` activation function\n",
        "    * The second Conv2D layer\n",
        "    * It should then add the layer inputs to the output of the second Conv2D layer. This is the final layer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWXDT-jWbiZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe2f6cc-c240-4353-885f-157cc41f07db"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "# Complete the following class.\n",
        "# Make sure to not change the class or method names or arguments.\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.batchnorm_2 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        This method should build the layers according to the above\n",
        "        specification. Make sure to use the input_shape argument to get the\n",
        "        correct number of filters, and to set the input_shape of the first\n",
        "        layer in the block.\n",
        "        \"\"\"\n",
        "        self.batchnorm_1 = tf.keras.layers.BatchNormalization(\n",
        "            input_shape=input_shape)\n",
        "        self.conv2d_1 = tf.keras.layers.Conv2D(filters=input_shape[-1],\n",
        "            kernel_size=(3, 3), padding='same', activation=None,\n",
        "            kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "        self.conv2d_2 = tf.keras.layers.Conv2D(filters=input_shape[-1],\n",
        "            kernel_size=(3, 3), padding='same', activation=None,\n",
        "            kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        This method should contain the code for calling the layer according to\n",
        "        the above specification, using the layer objects set up\n",
        "        in the build method.\n",
        "        \"\"\"\n",
        "        h = self.batchnorm_1(inputs=inputs, training=training)\n",
        "        h = tf.nn.relu(features=h)\n",
        "        h = self.conv2d_1(inputs=h)\n",
        "        h = self.batchnorm_2(inputs=h, training=training)\n",
        "        h = tf.nn.relu(features=h)\n",
        "        h = self.conv2d_2(inputs=h)\n",
        "        return tf.keras.layers.Add()([inputs, h])\n",
        "\n",
        "# Test your custom layer - the following should create a model using your layer\n",
        "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "outputs = ResidualBlock(name='residual_block')(inputs)\n",
        "test_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "test_model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " residual_block (ResidualBl  (None, 28, 28, 1)         28        \n",
            " ock)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28 (112.00 Byte)\n",
            "Trainable params: 24 (96.00 Byte)\n",
            "Non-trainable params: 4 (16.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2PoqmxCbiZy"
      },
      "source": [
        "You should now create a second custom layer for a residual block of your network. This layer will be used to change the number of filters within the block. Using layer subclassing, build your custom layer according to the following spec:\n",
        "\n",
        "* The custom layer class should have `__init__`, `build` and `call` methods\n",
        "* The class initialiser should call the base `Layer` class initializer, passing on any keyword arguments. It should also accept a `out_filters` argument, and save it as a class attribute\n",
        "* The `build` method should create the layers. It will take an `input_shape` argument, and should extract the number of input filters from this argument. It should create:\n",
        "    * A BatchNormalization layer: this will be the first layer in the block, so should use its `input shape` keyword argument\n",
        "    * A Conv2D layer with the same number of filters as the layer input, a 3x3 kernel size, `\"SAME\"` padding, and no activation function\n",
        "    * Another BatchNormalization layer\n",
        "    * Another Conv2D layer with `out_filters` number of filters, a 3x3 kernel size, `\"SAME\"` padding, and no activation function\n",
        "    * A final Conv2D layer with `out_filters` number of filters, a 1x1 kernel size, and no activation function\n",
        "* The `call` method should then process the input through the layers:\n",
        "    * The first BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * A `tf.nn.relu` activation function\n",
        "    * The first Conv2D layer\n",
        "    * The second BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * Another `tf.nn.relu` activation function\n",
        "    * The second Conv2D layer\n",
        "    * It should then take the layer inputs, pass it through the final 1x1 Conv2D layer, and add to the output of the second Conv2D layer. This is the final layer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiK_lavabiZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc6a7e7-31c1-46e1-b424-9d3a5a8b3a43"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "# Complete the following class.\n",
        "# Make sure to not change the class or method names or arguments.\n",
        "\n",
        "class FiltersChangeResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, out_filters, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        The class initialiser should call the base class initialiser, passing\n",
        "        any keyword arguments along. It should also set the number of filters\n",
        "        as a class attribute.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.out_filters = out_filters\n",
        "        self.batchnorm_2 = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2d_2 = tf.keras.layers.Conv2D(filters=out_filters,\n",
        "            kernel_size=(3, 3), padding='same', activation=None,\n",
        "            kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "        self.conv2d_3 = tf.keras.layers.Conv2D(filters=out_filters,\n",
        "            kernel_size=(1, 1), activation=None,\n",
        "            kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        This method should build the layers according to the above\n",
        "        specification. Make sure to use the input_shape argument to get the\n",
        "        correct number of filters, and to set the input_shape of the first\n",
        "        layer in the block.\n",
        "        \"\"\"\n",
        "        self.batchnorm_1 = tf.keras.layers.BatchNormalization(\n",
        "            input_shape=input_shape)\n",
        "        self.conv2d_1 = tf.keras.layers.Conv2D(filters=input_shape[-1],\n",
        "            kernel_size=(3, 3), padding='same', activation=None,\n",
        "            kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        This method should contain the code for calling the layer according to\n",
        "        the above specification, using the layer objects set up in the build\n",
        "        method.\n",
        "        \"\"\"\n",
        "        h1 = self.batchnorm_1(inputs=inputs, training=training)\n",
        "        h1 = tf.nn.relu(features=h1)\n",
        "        h1 = self.conv2d_1(inputs=h1)\n",
        "        h1 = self.batchnorm_2(inputs=h1, training=training)\n",
        "        h1 = tf.nn.relu(features=h1)\n",
        "        h1 = self.conv2d_2(inputs=h1)\n",
        "        h2 = self.conv2d_3(inputs=inputs)\n",
        "        return tf.keras.layers.Add()([h1, h2])\n",
        "\n",
        "# Test your custom layer - the following should create a model using your layer\n",
        "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "outputs = FiltersChangeResidualBlock(name='fc_resnet_block',\n",
        "    out_filters=16)(inputs)\n",
        "test_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "test_model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " fc_resnet_block (FiltersCh  (None, 32, 32, 16)        620       \n",
            " angeResidualBlock)                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 620 (2.42 KB)\n",
            "Trainable params: 608 (2.38 KB)\n",
            "Non-trainable params: 12 (48.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMY3Ak7YbiZ6"
      },
      "source": [
        "#### Create a custom model that integrates the residual blocks\n",
        "\n",
        "You are now ready to build your ResNet model. Using model subclassing, build your model according to the following spec:\n",
        "\n",
        "* The custom model class should have `__init__` and `call` methods.\n",
        "* The class initialiser should call the base `Model` class initializer, passing on any keyword arguments. It should create the model layers:\n",
        "    * The first Conv2D layer, with 32 filters, a 7x7 kernel and stride of 2.\n",
        "    * A `ResidualBlock` layer.\n",
        "    * The second Conv2D layer, with 32 filters, a 3x3 kernel and stride of 2.\n",
        "    * A `FiltersChangeResidualBlock` layer, with 64 output filters.\n",
        "    * A Flatten layer\n",
        "    * A final Dense layer, with a 10-way softmax output\n",
        "* The `call` method should then process the input through the layers in the order given above. Ensure to pass the `training` keyword argument to the residual blocks, to ensure the correct mode of operation for the batch norm layers.\n",
        "\n",
        "In total, your neural network should have six layers (counting each residual block as one layer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53xs9JBKbiZ7"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "# Complete the following class.\n",
        "# Make sure to not change the class or method names or arguments.\n",
        "\n",
        "@tf.keras.saving.register_keras_serializable()\n",
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        The class initialiser should call the base class initialiser, passing\n",
        "        any keyword arguments along. It should also create the layers of the\n",
        "        network according to the above specification.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.conv2d_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(7, 7),\n",
        "            strides=2, kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "        self.residual_block = ResidualBlock()\n",
        "        self.conv2d_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),\n",
        "            strides=2, kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))\n",
        "        self.filters_change_residual_block = FiltersChangeResidualBlock(\n",
        "            out_filters=64)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.2)\n",
        "        self.dense = tf.keras.layers.Dense(units=10,\n",
        "            activation=tf.keras.activations.softmax)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        This method should contain the code for calling the layer according to\n",
        "        the above specification, using the layer objects set up in the\n",
        "        initialiser.\n",
        "        \"\"\"\n",
        "        h = self.conv2d_1(inputs=inputs)\n",
        "        h = self.residual_block(inputs=h,\n",
        "            training=training)\n",
        "        h = self.conv2d_2(inputs=h)\n",
        "        h = self.filters_change_residual_block(inputs=h,\n",
        "            training=training)\n",
        "        h = self.flatten(inputs=h)\n",
        "        h = self.dropout(inputs=h, training=training)\n",
        "        return self.dense(inputs=h)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlHXCYYLbiaB"
      },
      "source": [
        "#### Define the optimizer, loss function and grad function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFL3r1zZbiaH"
      },
      "source": [
        "We will use the Adam optimizer with a learning rate of 0.001, and the sparse categorical cross entropy function.\n",
        "\n",
        "You should now create the `train` function that will compute the forward and backward pass, and return the loss value and gradients that will be used in your custom training loop:\n",
        "\n",
        "* The `train` function takes a model instance, inputs, targets and the loss object above as arguments\n",
        "* The function should use a `tf.GradientTape` context to compute the forward pass and calculate the loss\n",
        "* The function should compute the gradient of the loss with respect to the model's trainable variables\n",
        "* The function should return a tuple of two elements: the loss value, and a list of gradients"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(self, data):\n",
        "    \"\"\"\n",
        "    This function should compute the loss and gradients of your model,\n",
        "    corresponding to the inputs x and targets y provided. It should return\n",
        "    the loss and accuracy.\n",
        "    \"\"\"\n",
        "    x, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = self(inputs=x, training=True)\n",
        "        loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "\n",
        "    trainable_vars = self.trainable_variables\n",
        "    grads = tape.gradient(target=loss, sources=trainable_vars)\n",
        "    self.optimizer.apply_gradients(grads_and_vars=zip(\n",
        "        grads, trainable_vars))\n",
        "\n",
        "    for m in self.metrics:\n",
        "        if m.name=='loss':\n",
        "            m.update_state(values=loss)\n",
        "        else:\n",
        "            m.update_state(y_true=y, y_pred=y_pred)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "@tf.function\n",
        "def test_step(self, data):\n",
        "    x, y = data\n",
        "\n",
        "    y_pred = self(inputs=x, training=False)\n",
        "    loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "\n",
        "    for m in self.metrics:\n",
        "        if m.name=='loss':\n",
        "            m.update_state(values=loss)\n",
        "        else:\n",
        "            m.update_state(y_true=y, y_pred=y_pred)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "CustomModel.metrics=[\n",
        "    tf.keras.metrics.Mean(name='loss'),\n",
        "    tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "]\n",
        "CustomModel.train_step = train_step\n",
        "CustomModel.test_step= test_step"
      ],
      "metadata": {
        "id": "jsOsNRNB701V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_48RRAGbiaK"
      },
      "source": [
        "#### Define the custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDKxVoDEbiaL"
      },
      "source": [
        "You should now write a custom training loop. Complete the following function, according to the spec:\n",
        "\n",
        "* The function takes the following arguments:\n",
        "    * `model`: an instance of your custom model\n",
        "    * `num_epochs`: integer number of epochs to train the model\n",
        "    * `train_dataset`: a `tf.data.Dataset` object for the training data\n",
        "    * `valid_dataset`: a `tf.data.Dataset` object for the testing data\n",
        "   \n",
        "* Your function should train the model for the given number of epochs, using the `train` above, that returns the loss and gradients for given model, inputs and targets for each training batch, and updating the model parameters using the optimizer object as created `optimizer.apply_gradients`.\n",
        "* Your function should collect the mean `loss`, a sparse categorical cross entropy object as created above and accuracy values over the epoch, and return a dataframe of the two metrics; the first for the loss values per epoch, the second for the accuracy values per epoch.\n",
        "\n",
        "You may also want to print out the loss and accuracy at each epoch during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOvFcrLpbiaM",
        "collapsed": true
      },
      "source": [
        "#### GRADED CELL ####\n",
        "# Complete the following function.\n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "start_time = time.time()\n",
        "# Create the model, optimizer\n",
        "resnet_model = CustomModel()\n",
        "resnet_model.compile(\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(name='loss'),\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        ")\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "    patience=1, factor=0.9)\n",
        "\n",
        "history = resnet_model.fit(x=ds_train, validation_data=ds_test,\n",
        "    epochs=30, callbacks=[reduce_lr], verbose=0)\n",
        "print('Training duration: %.2fs'%(time.time() - start_time))\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "df_history = pd.DataFrame(history.history)\n",
        "df_history['epoch'] = df_history.index + 1\n",
        "\n",
        "# Plot the learning curves\n",
        "fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle(t='Training Metrics')\n",
        "axes[1].set_xlabel(xlabel='epoch')\n",
        "for ax, metric in zip(axes, ['loss', 'accuracy']):\n",
        "    g = sns.lineplot(ax=ax, label='train', data=df_history,\n",
        "        x='epoch', y=f\"{metric}\")\n",
        "    g = sns.lineplot(ax=ax, label=\"val\", data=df_history,\n",
        "        x='epoch', y=f'val_{metric}')\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_ylabel(ylabel=f'{metric}')\n",
        "    ax.set_title(label=f'{metric} vs. epoch')\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "loss, acc = model.evaluate(x=ds_test, verbose=0)\n",
        "print(f'Test loss: {loss:.2f}, Test accuracy: {acc:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTV7htZDbiaX"
      },
      "source": [
        "#### Model predictions\n",
        "\n",
        "Let's see some model predictions! We will randomly select four images from the test data, and display the image and label for each.\n",
        "\n",
        "For each test image, model's prediction (the label with maximum probability) is shown, together with a plot showing the model's categorical distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o1RDRWEbiaY"
      },
      "source": [
        "inputs = preprocessing_model.input\n",
        "outputs = resnet_model(preprocessing_model(inputs))\n",
        "inference_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "inference_model.save('models/resnet_model.keras')\n",
        "inference_model = tf.keras.models.load_model('models/resnet_model.keras')\n",
        "\n",
        "# Run this cell to get model predictions on randomly selected test images\n",
        "rand_inx = np.random.choice(a=x_test.shape[0], size=4)\n",
        "x_rand = x_test[rand_inx, ...]\n",
        "y_rand = y_test[rand_inx, ...]\n",
        "preds = inference_model(inputs=x_rand)\n",
        "\n",
        "# Get dataset labels\n",
        "labels = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(16, 12))\n",
        "fig.subplots_adjust(hspace=0.5, wspace=-0.2)\n",
        "for i, (y_pred, x, y) in enumerate(\n",
        "        zip(preds, x_rand, y_rand)):\n",
        "    axes[i, 0].imshow(np.squeeze(a=x))\n",
        "    axes[i, 0].get_xaxis().set_visible(b=False)\n",
        "    axes[i, 0].get_yaxis().set_visible(b=False)\n",
        "    axes[i, 0].text(x=5., y=-2., s=f\"Class {y} ({labels[y]})\")\n",
        "    axes[i, 1].bar(x=np.arange(stop=len(y_pred)), height=y_pred)\n",
        "    axes[i, 1].set_xticks(ticks=np.arange(stop=len(y_pred)))\n",
        "    axes[i, 1].set_xticklabels(labels=labels, rotation=0)\n",
        "    pred_inx = np.argmax(a=y_pred)\n",
        "    axes[i, 1].set_title(label=f\"Categorical distribution.\\\n",
        "Model prediction: {labels[pred_inx]}\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6qxLOmbbiaa"
      },
      "source": [
        "Congratulations for completing this programming assignment! You're now ready to move on to the capstone project for this course."
      ]
    }
  ]
}