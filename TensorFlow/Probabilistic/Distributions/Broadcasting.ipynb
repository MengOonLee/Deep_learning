{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow/Probabilistic/Distributions/Broadcasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjzWUNqbL7x"
   },
   "source": [
    "# Broadcasting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g7i4tmHbL7y"
   },
   "source": [
    "This reading will introduce you to numpy's broadcasting rules and show how you can use broadcasting when specifying batches of distributions in TensorFlow, as well as with the `prob` and `log_prob` methods.\n",
    "\n",
    "Broadcasting will also be discussed and demonstrated in the following videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PHjA2snbL71"
   },
   "source": [
    "## Operations on arrays of different sizes in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZaF-keybL71"
   },
   "source": [
    "Numpy operations can be applied to arrays that are not of the same shape, but only if the shapes satisfy certain conditions.\n",
    "\n",
    "As a demonstration of this, let us add together two arrays of different shapes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhzSph1mbL71"
   },
   "source": [
    "This is the addition\n",
    "\n",
    "    [ [1.],    +  [0., 1., 2.]  \n",
    "      [2.],  \n",
    "      [3.],  \n",
    "      [4.] ]\n",
    "\n",
    "To execute it, numpy:\n",
    "1. Aligned the shapes of `a` and `b` on the last axis and prepended 1s to the shape with fewer axes:\n",
    "        a: 4 x 1     --->    a: 4 x 1\n",
    "        b:     3     --->    b: 1 x 3\n",
    "        \n",
    "\n",
    "2. Checked that the sizes of the axes matched or were equal to 1:\n",
    "        a: 4 x 1  \n",
    "        b: 1 x 3\n",
    "`a` and `b` satisfied this criterion.\n",
    "\n",
    "\n",
    "3. Stretched both arrays on their 1-valued axes so that their shapes matched, then added them together.  \n",
    "`a` was replicated 3 times in the second axis, while `b` was replicated 4 times in the first axis.\n",
    "\n",
    "This meant that the addition in the final step was\n",
    "\n",
    "    [ [1., 1., 1.],    +  [ [0., 1., 2.],  \n",
    "      [2., 2., 2.],         [0., 1., 2.],  \n",
    "      [3., 3., 3.],         [0., 1., 2.],  \n",
    "      [4., 4., 4.] ]        [0., 1., 2.] ]\n",
    "      \n",
    "Addition was then carried out element-by-element, as you can verify by referring back to the output of the code cell above.  \n",
    "This resulted in an output with shape 4 x 3.\n",
    "\n",
    "\n",
    "## Numpy's broadcasting rule\n",
    "\n",
    "Broadcasting rules describe how values should be transmitted when the inputs to an operation do not match.  \n",
    "In numpy, the broadcasting rule is very simple:\n",
    "> Prepend 1s to the smaller shape,   \n",
    "check that the axes of both arrays have sizes that are equal or 1,  \n",
    "then stretch the arrays in their size-1 axes.\n",
    "\n",
    "A crucial aspect of this rule is that it does not require the input arrays have the same number of axes.  \n",
    "Another consequence of it is that a broadcasting output will have the largest size of its inputs in each axis.  \n",
    "Take the following multiplication as an example:\n",
    "\n",
    "        a: 3 x 7 x 1  \n",
    "        b:     1 x 5  \n",
    "    a * b: 3 x 7 x 5\n",
    "\n",
    "You can see that the output shape is the maximum of the sizes in each axis.\n",
    "\n",
    "Numpy's broadcasting rule also does not require that one of the arrays has to be bigger in all axes.  \n",
    "This is seen in the following example, where `a` is smaller than `b` in its third axis but is bigger in its second axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HT64qJUbbL71"
   },
   "source": [
    "Broadcasting behaviour also points to an efficient way to compute an outer product in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXschlJBbL71"
   },
   "source": [
    "The idea of numpy stretching the arrays in their size-1 axes is useful and is functionally correct. But this is not what numpy literally does behind the scenes, since that would be an inefficient use of memory. Instead, numpy carries out the operation by looping over singleton (size-1) dimensions.\n",
    "\n",
    "To give you some practise with broadcasting, try predicting the output shapes for the following operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2omTD-MxbL71",
    "outputId": "48704123-98d7-4b75-b6cb-42d5d1196016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape: (3, 1)\n",
      "b shape: (10, 1, 1)\n",
      "c shape: (4, 1)\n",
      "a + b shape: (10, 3, 1)\n",
      "ac^T shape: (3, 4)\n",
      "a*b + c^T shape: (10, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define three arrays with different shapes\n",
    "a = np.ones(shape=(3, 1))\n",
    "print(\"a shape:\", a.shape)\n",
    "b = np.zeros(shape=(10, 1, 1))\n",
    "print(\"b shape:\", b.shape)\n",
    "c = np.ones(shape=(4, 1))\n",
    "print(\"c shape:\", c.shape)\n",
    "\n",
    "# Predict the shape before executing this cell\n",
    "# Add two arrays with different shapes\n",
    "print(\"a + b shape:\", (a + b).shape)\n",
    "# Use broadcasting to compute and outer product ac^T,\n",
    "# where a and c are column vectors\n",
    "print(\"ac^T shape:\", (a*c.T).shape)\n",
    "print(\"a*b + c^T shape:\", (a*b + c.T).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcKchEh3bL72"
   },
   "source": [
    "## Broadcasting for univariate TensorFlow Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48EH1rCAbL72"
   },
   "source": [
    "The broadcasting rule for TensorFlow is the same as that for numpy. For example, TensorFlow also allows you to specify the parameters of Distribution objects using broadcasting.\n",
    "\n",
    "What is meant by this can be understood through an example with the univariate normal distribution. Say that we wish to specify a parameter grid for six Gaussians. The parameter combinations to be used, `(loc, scale)`, are:  \n",
    "\n",
    "    (0, 1)  \n",
    "    (0, 10)  \n",
    "    (0, 100)  \n",
    "    (1, 1)  \n",
    "    (1, 10)  \n",
    "    (1, 100)\n",
    "    \n",
    "A laborious way of doing this is to explicitly pass each parameter to `tfd.Normal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njErcwT9bL72"
   },
   "source": [
    "A more succinct way to create a batch of distributions for this parameter grid is to use broadcasting.  \n",
    "Consider what would happen if we were to broadcast these arrays according the rule discussed earlier:\n",
    "    \n",
    "    loc = [ [0.],\n",
    "            [1.] ]\n",
    "    scale = [1., 10., 100.]\n",
    "    \n",
    "The shapes would be stretched according to\n",
    "\n",
    "    loc:   2 x 1 ---> 2 x 3\n",
    "    scale: 1 x 3 ---> 2 x 3\n",
    "    \n",
    "resulting in\n",
    "\n",
    "    loc = [ [0., 0., 0.],\n",
    "            [1., 1., 1.] ]\n",
    "    scale = [ [1., 10., 100.],\n",
    "              [1., 10., 100.] ]\n",
    "              \n",
    "which are compatible with the `loc` and `scale` arguments of `tfd.Normal`.  \n",
    "Sure enough, this is precisely what TensorFlow does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "GGXrFPQSbL72",
    "outputId": "1b9cd477-6406-44d0-8670-be1040e01908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: tfp.distributions.Normal(\"Normal\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n",
      "Prob shape (10, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed=42)\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Define a batch of Normal distributions with broadcasting\n",
    "dist = tfp.distributions.Normal(\n",
    "    loc=[[0], [5]], scale=[1, 2, 3])\n",
    "# Print the distribution and notice the batch and event shapes\n",
    "print(\"Distribution:\", dist)\n",
    "samples = dist.sample(sample_shape=10000)\n",
    "pdf = dist.prob(value=samples)\n",
    "\n",
    "sample = tf.random.uniform(shape=(10, 1, 1))\n",
    "prob = dist.prob(value=sample)\n",
    "print(f\"Prob shape {prob.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4J6AknubL73"
   },
   "source": [
    "In summary, TensorFlow broadcasts parameter arrays: it stretches them according to the broadcasting rule, then creates a distribution on an element-by-element basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--014D6QbL73"
   },
   "source": [
    "#### Broadcasting with `prob` and `log_prob` methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NK3ZE8gdbL73"
   },
   "source": [
    "When using `prob` and  `log_prob` with broadcasting, we follow the same principles as before. Let's make a new batch of normals as before but with means which are centered at different locations to help distinguish the results we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umwvH5s2bL73"
   },
   "source": [
    "We can feed in samples of any shape as long as it can be broadcast agasint our batch shape for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A1wnzHWbL73"
   },
   "source": [
    "`log_prob` works in the exact same way with broadcasting. We can replace `prob` with `log_prob` in any of the previous examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a5HhsiNbL74"
   },
   "source": [
    "## Broadcasting for multivariate TensorFlow distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccCXkHewbL74"
   },
   "source": [
    "Broadcasting behaviour for multivariate distributions is only a little more sophisticated than it is for univariate distributions.\n",
    "\n",
    "Recall that `MultivariateNormalDiag` has two parameter arguments: `loc` and `scale_diag`. When specifying a single distribution, these arguments are vectors of the same length:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKzkUDxXbL74"
   },
   "source": [
    "The size of the final axis of the inputs determines the event shape for each distribution in the batch.  This means that if we pass\n",
    "    \n",
    "    loc = [ [0., 0.],\n",
    "            [1., 1.] ]\n",
    "    scale_diag = [1., 0.5]\n",
    "    \n",
    "such that\n",
    "\n",
    "    loc:        2 x 2\n",
    "    scale_diag: 1 x 2\n",
    "                    ^ final dimension is interpreted as event dimension\n",
    "                ^ other dimensions are interpreted as batch dimensions  \n",
    "then a batch of two bivariate normal distributions will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I24W8kwSbL75"
   },
   "source": [
    "Knowing that, for multivariate distributions, TensorFlow\n",
    "\n",
    "- interprets the final axis of an array of parameters as the event shape,\n",
    "\n",
    "\n",
    "- and broadcasts over the remaining axes,  \n",
    "\n",
    "can you predict what the batch and event shapes will if we pass the arguments\n",
    "\n",
    "\n",
    "    loc = [ [ 1.],\n",
    "            [-1.] ] # shape (2, 1)\n",
    "    scale_diag = [ [[0.1, 0.2, 0.3]],\n",
    "                   [[1., 2., 3.]] ] # shape (2, 1, 3)\n",
    "                   \n",
    "to `MultivariateNormalDiag`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnpWwQG1bL75"
   },
   "source": [
    "Solution:\n",
    "\n",
    "Align the parameter array shapes on their last axis, prepending 1s where necessary:  \n",
    "    \n",
    "           loc: 1 x 2 x 1  \n",
    "    scale_diag: 2 x 1 x 3  \n",
    "\n",
    "The final axis has size 3, so `event_shape = (3)`. The remaining axes are broadcast over to yield  \n",
    "    \n",
    "           loc: 2 x 2 x 3  \n",
    "    scale_diag: 2 x 2 x 3  \n",
    "\n",
    "so `batch_shape = (2, 2)`.\n",
    "\n",
    "Let's see if this is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U7M1rdwZbL74",
    "outputId": "2a23c512-e6cd-42e3-b7a7-3d6e947204b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: tfp.distributions.MultivariateNormalDiag(\"MultivariateNormalDiag\", batch_shape=[2], event_shape=[2], dtype=float32)\n",
      "Prob shape (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.random.set_seed(seed=42)\n",
    "\n",
    "# Define a multivariate Gaussian distribution with broadcasting\n",
    "dist = tfp.distributions.MultivariateNormalDiag(\n",
    "    loc=[[0], [1]], scale_diag=[1, 2])\n",
    "# Print the distribution - note the event shape and batch shape\n",
    "print(\"Distribution:\", dist)\n",
    "\n",
    "sample = tf.random.uniform(shape=(10, 1, 2))\n",
    "prob = dist.prob(value=sample)\n",
    "print(f\"Prob shape {prob.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ0twEgVbL75"
   },
   "source": [
    "As we did before lets also look at broadcasting when we have batches of multivariate distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNdxO4wBbL75"
   },
   "source": [
    "And to refresh our memory of `Independent` we'll use it below to roll the rightmost batch shape into the event shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLz5HiLgbL75"
   },
   "source": [
    "Now, onto the broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXsT_WcbbL75",
    "outputId": "8af534e5-e31a-4243-ee08-f2c4624b9144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: tfp.distributions.Independent(\"IndependentNormal\", batch_shape=[2], event_shape=[2], dtype=float32)\n",
      "Prob shape (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed=42)\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a batch of Normal distributions with broadcasting\n",
    "dist = tfp.distributions.Normal(\n",
    "    loc=[[0], [1]], scale=[1, 10])\n",
    "# Create a multivariate Independent distribution\n",
    "dist = tfp.distributions.Independent(distribution=dist,\n",
    "    reinterpreted_batch_ndims=1)\n",
    "print(\"Distribution:\", dist)\n",
    "\n",
    "# Use broadcasting with the prob method\n",
    "# [S,b,e] shaped input where [b,e] can be broadcast agaisnt [B,E]\n",
    "sample = tf.random.uniform(shape=(10, 1, 1))\n",
    "prob = dist.prob(value=sample)\n",
    "print(f\"Prob shape {prob.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czZnGzvhbL75"
   },
   "source": [
    "You should now feel confident specifying batches of distributions using broadcasting. As you may have already guessed, broadcasting is especially useful when specifying grids of hyperparameters.\n",
    "\n",
    "If you don't feel entirely comfortable with broadcasting quite yet, don't worry: re-read this notebook, go through the further reading provided below, and experiment with broadcasting in both numpy and TensorFlow, and you'll be broadcasting in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSKqZbtUbL75"
   },
   "source": [
    "### Further reading and resources\n",
    "* Numpy documentation on broadcasting: https://numpy.org/devdocs/user/theory.broadcasting.html\n",
    "* https://www.tensorflow.org/xla/broadcasting"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Broadcasting_rules.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
