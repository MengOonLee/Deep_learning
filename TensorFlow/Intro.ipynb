{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TensorFlow/Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juxNknyiLVQH"
   },
   "source": [
    "# Introduction to TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZeFZkX3q0_1T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(seed=42)\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "import time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "sPGUEw-f9bRl",
    "outputId": "c105d3a0-1fc5-41ef-b09a-4d1b286d5f72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'type': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 0., 0.]], dtype=float32)>,\n",
       "  'size': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>,\n",
       "  'weight': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.3934661]], dtype=float32)>},\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = {\n",
    "    \"type\": [0, 1, 1],\n",
    "    \"size\": [\"small\", \"small\", \"medium\"],\n",
    "    \"weight\": [2.7, 1.8, 1.6]\n",
    "}\n",
    "y_train = [1, 1, 0]\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(tensors=(x_train, y_train))\n",
    "ds_train = ds_train.map(map_func=lambda x, y:\n",
    "    ({k: tf.expand_dims(input=v, axis=-1) for k, v in x.items()}, y))\\\n",
    "    .batch(batch_size=1)\n",
    "                        \n",
    "inputs = {}\n",
    "inputs[\"type\"] = tf.keras.Input(name=\"type\", shape=(), dtype=tf.int64)\n",
    "inputs[\"size\"] = tf.keras.Input(name=\"size\", shape=(), dtype=tf.string)\n",
    "inputs[\"weight\"] = tf.keras.Input(name=\"weight\", shape=(), dtype=tf.float32)\n",
    "\n",
    "outputs = {}\n",
    "outputs[\"type\"] = tf.keras.layers.CategoryEncoding(\n",
    "    num_tokens=3, output_mode=\"one_hot\")(inputs[\"type\"])\n",
    "outputs[\"size\"] = tf.keras.layers.StringLookup(\n",
    "    vocabulary=[\"small\", \"medium\", \"large\"])(inputs[\"size\"])\n",
    "normalizer = tf.keras.layers.Normalization()\n",
    "normalizer.adapt(ds_train.map(lambda x, y: x[\"weight\"]))\n",
    "outputs[\"weight\"] =  normalizer(inputs[\"weight\"])\n",
    "\n",
    "preprocessing_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "ds_train = ds_train.map(lambda x, y: (preprocessing_model(x), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "next(iter(ds_train.take(count=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718463304.148049    1597 service.cc:145] XLA service 0x7ac8f8005ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1718463304.148128    1597 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce GTX 1660, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1718463304.732951    1597 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ac9b7db5f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {}\n",
    "inputs[\"type\"] = tf.keras.Input(name=\"type\", shape=(3,), dtype=tf.float32)\n",
    "inputs[\"size\"] = tf.keras.Input(name=\"size\", shape=(), dtype=tf.int64)\n",
    "inputs[\"weight\"] = tf.keras.Input(name=\"weight\", shape=(1,), dtype=tf.float32)\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=3, output_dim=4)\n",
    "\n",
    "h = tf.keras.layers.Concatenate()([\n",
    "    inputs[\"type\"],\n",
    "    embedding(inputs[\"size\"]),\n",
    "    inputs[\"weight\"]\n",
    "])\n",
    "outputs = tf.keras.layers.Dense(units=1)(h)\n",
    "training_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "training_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "training_model.fit(x=ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 911ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5399132]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = preprocessing_model.input\n",
    "outputs = training_model(preprocessing_model(inputs))\n",
    "inference_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "inference_model.save(\"model.keras\")\n",
    "\n",
    "x_test = {\n",
    "    \"type\": [0],\n",
    "    \"size\": [\"foo\"],\n",
    "    \"weight\": [-0.7]\n",
    "}\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(tensors=x_test)\n",
    "ds_test = ds_test.map(map_func=lambda x:\n",
    "    {k: tf.expand_dims(input=v, axis=-1) for k, v in x.items()})\\\n",
    "    .batch(batch_size=1)\n",
    "\n",
    "inference_model = tf.keras.models.load_model(\"model.keras\")\n",
    "inference_model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb6gDr32Js-t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "id": "t0JC8sUA0_1U",
    "outputId": "e0cb3683-67a9-49cf-f8f1-36376e6d7598"
   },
   "outputs": [],
   "source": [
    "dataset_url = \"http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip\"\n",
    "csv_file = \"datasets/petfinder-mini/petfinder-mini.csv\"\n",
    "\n",
    "tf.keras.utils.get_file(fname=\"petfinder_mini.zip\", origin=dataset_url, extract=True,\n",
    "    cache_dir=\".\")\n",
    "df = pd.read_csv(csv_file)\n",
    "df[\"label\"] = np.where(df[\"AdoptionSpeed\"]==4, 0, 1)\n",
    "df.drop(inplace=True, columns=[\"Description\", \"AdoptionSpeed\"])\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Z3DydUf0_1W",
    "outputId": "efb24952-aaf6-4ee8-f8ea-90c2e505ebfd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_to_ds(dataframe, batch_size, shuffle=False):\n",
    "    df = dataframe.copy()\n",
    "    y = df.pop(\"label\")\n",
    "    x = {k: v.values[:, tf.newaxis] for k, v in df.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tensors=(x, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size=batch_size)\n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "    return ds\n",
    "\n",
    "df_train, df_test = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42)\n",
    "\n",
    "ds_train = df_to_ds(dataframe=df_train, batch_size=256, shuffle=True)\n",
    "ds_test = df_to_ds(dataframe=df_test, batch_size=32)\n",
    "ds_test.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-klK3EV0_1W"
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "features = []\n",
    "\n",
    "for h in [\"Fee\", \"PhotoAmt\"]:\n",
    "    col = tf.keras.Input(shape=(), name=h, dtype=tf.int64)\n",
    "    inputs.append(col)\n",
    "    x = ds_train.map(map_func=lambda x, y: x[h])\n",
    "    normalizer = tf.keras.layers.Normalization(axis=None)\n",
    "    normalizer.adapt(x)\n",
    "    feature = normalizer(col)\n",
    "    features.append(feature)\n",
    "\n",
    "for h in [\"Type\", \"Breed1\", \"Gender\", \"Color1\", \"Color2\", \"MaturitySize\", \"FurLength\",\n",
    "        \"Vaccinated\", \"Sterilized\", \"Health\"]:\n",
    "    col = tf.keras.Input(shape=(), name=h, dtype=tf.string)\n",
    "    inputs.append(col)\n",
    "    x = ds_train.map(map_func=lambda x, y: x[h])\n",
    "    index = tf.keras.layers.StringLookup(max_tokens=5)\n",
    "    index.adapt(x)\n",
    "    feature = tf.keras.layers.CategoryEncoding(\n",
    "        num_tokens=index.vocabulary_size())\n",
    "    features.append(feature)\n",
    "\n",
    "for h in [\"Age\"]:\n",
    "    col = tf.keras.Input(shape=(), name=h, dtype=tf.int64)\n",
    "    inputs.append(col)\n",
    "    x = ds_train.map(map_func=lambda x, y: x[h])\n",
    "    index = tf.keras.layers.IntegerLookup(max_tokens=5)\n",
    "    index.adapt(x)\n",
    "    feature = tf.keras.layers.CategoryEncoding(\n",
    "        num_tokens=index.vocabulary_size())\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4HJpV_10_1X",
    "outputId": "3e20b349-c60d-4acc-8061-17a9e6af938a"
   },
   "outputs": [],
   "source": [
    "outputs = tf.keras.layers.concatenate(features)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QAhEqt10_1X"
   },
   "source": [
    "### UCI Bank Marketing Dataset\n",
    "\n",
    "The `Bank Marketing` dataset is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. [here](https://archive.ics.uci.edu/dataset/222/bank+marketing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm1zoEEX0_1Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve9RWaOz0_1Y",
    "outputId": "23cc27a0-181f-45dc-b0f8-93434a97b963"
   },
   "outputs": [],
   "source": [
    "df_bank = pd.read_csv(\"data/bank/bank-full.csv\", delimiter=\";\")\n",
    "df_bank.info()\n",
    "df_bank.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMRNzeeI0_1Y",
    "outputId": "7ded5b42-3556-41fc-c6d7-514134ab0256",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_bank = tf.data.Dataset.from_tensor_slices(tensors=dict(df_bank))\n",
    "ds_bank.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBXIslfl0_1Z"
   },
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'age': tf.keras.Input(shape=(1,), dtype=tf.int64, name='age')\n",
    "}\n",
    "\n",
    "age = tf.keras.layers.Discretization(\n",
    "    bin_boundaries=[10, 20, 30, 40, 50, 60, 70, 80, 90])(inputs['age'])\n",
    "age = tf.keras.layers.CategoryEncoding(num_tokens=10, output_mode='one_hot')(age)\n",
    "\n",
    "outputs = {\n",
    "    'age': age\n",
    "}\n",
    "\n",
    "preprocessing_model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehDt85Th0_1Z",
    "outputId": "7f3ed132-5dfc-40c1-cb39-9150a5ef1881",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds01 = ds_bank.batch(1)\n",
    "ds01 = ds01.map(lambda x: preprocessing_model(x))\n",
    "next(iter(ds01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWqM6UHr0_1a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = model_selection.train_test_split(\n",
    "    df_bank, test_size=0.1, random_state=42)\n",
    "\n",
    "dict_train = dict(df_train)\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(tensors={\n",
    "    k: dict_train[k] for k in dict_train})\n",
    "\n",
    "dict_test = dict(df_test)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(tensors={\n",
    "    k: dict_test[k] for k in dict_test})\n",
    "ds_test.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCXBANLt0_1a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9Mlx1kE0_1a",
    "outputId": "2f668e63-e032-44a0-cc7c-ebba70d38535"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(tensors=(x_train, y_train))\n",
    "ds_train = ds_train.map(map_func=lambda x, y: (\n",
    "    tf.cast(x=x, dtype=tf.float32)/255., y))\n",
    "ds_train = ds_train.map(map_func=lambda x, y: (\n",
    "    tf.reshape(x, (784,)), y))\n",
    "ds_train = ds_train.shuffle(buffer_size=len(ds_train))\\\n",
    "    .batch(batch_size=batch_size)\n",
    "\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(tensors=(x_test, y_test))\n",
    "ds_test = ds_test.map(map_func=lambda x, y:\n",
    "    (tf.cast(x=x, dtype=tf.float32)/255., y))\n",
    "ds_test = ds_test.map(map_func=lambda x, y: (\n",
    "    tf.reshape(x, (784,)), y))\n",
    "ds_test = ds_test.batch(batch_size=batch_size)\n",
    "print(\"Element spec:\", ds_test.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZTQGBG_0_1a",
    "outputId": "6330c0be-b61e-40c8-9d45-b7ec52ec5945",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    h = tf.keras.layers.Dense(units=64,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(l2=1e-5))(inputs)\n",
    "    h = tf.keras.layers.Dropout(rate=0.2)(h)\n",
    "    outputs = tf.keras.layers.Dense(units=num_classes,\n",
    "        activation=tf.keras.activations.softmax)(h)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(input_shape=(784,), num_classes=10)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    logs = {}\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(inputs=x, training=True)\n",
    "        loss = model.compute_loss(y=y, y_pred=y_pred)\n",
    "\n",
    "    trainable_vars = model.trainable_variables\n",
    "    grads = tape.gradient(target=loss,\n",
    "        sources=trainable_vars)\n",
    "    model.optimizer.apply_gradients(grads_and_vars=zip(\n",
    "        grads, trainable_vars))\n",
    "\n",
    "    for metric in model.metrics:\n",
    "        if metric.name==\"loss\":\n",
    "            metric.update_state(values=loss)\n",
    "        else:\n",
    "            metric.update_state(y_true=y, y_pred=y_pred)\n",
    "\n",
    "    metrics = [m.result() for m in model.metrics]\n",
    "    logs = {f\"train_{k}\": v for k, v in metrics[1].items()}\n",
    "    logs[\"train_loss\"] = metrics[0]\n",
    "\n",
    "    return logs\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    y_pred = model(inputs=x, training=False)\n",
    "    loss = model.compute_loss(y=y, y_pred=y_pred)\n",
    "\n",
    "    for metric in model.metrics:\n",
    "        if metric.name==\"loss\":\n",
    "            metric.update_state(values=loss)\n",
    "        else:\n",
    "            metric.update_state(y_true=y, y_pred=y_pred)\n",
    "\n",
    "    metrics = [m.result() for m in model.metrics]\n",
    "    logs = {f\"test_{k}\": v for k, v in metrics[1].items()}\n",
    "    logs[\"test_loss\"] = metrics[0]\n",
    "\n",
    "    return logs\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"test_loss\",\n",
    "    patience=1)\n",
    "callbacks = tf.keras.callbacks.CallbackList(model=model,\n",
    "    callbacks=[reduce_lr], add_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKTz7LqHLVQg",
    "outputId": "808488ef-8035-458c-ca47-4d0ffb58cdae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "start_time = time.time()\n",
    "history = {\n",
    "    \"train_loss\": [], \"test_loss\": [],\n",
    "    \"train_accuracy\": [], \"test_accuracy\": []\n",
    "}\n",
    "\n",
    "logs = {}\n",
    "callbacks.on_train_begin(logs=logs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    callbacks.on_epoch_begin(epoch=epoch, logs=logs)\n",
    "\n",
    "    for x, y in ds_train:\n",
    "        logs = train_step(x, y)\n",
    "    history[\"train_loss\"].append(logs[\"train_loss\"].numpy())\n",
    "    history[\"train_accuracy\"].append(logs[\"train_accuracy\"].numpy())\n",
    "\n",
    "    for x, y in ds_test:\n",
    "        logs = test_step(x, y)\n",
    "    history[\"test_loss\"].append(logs[\"test_loss\"].numpy())\n",
    "    history[\"test_accuracy\"].append(logs[\"test_accuracy\"].numpy())\n",
    "\n",
    "    callbacks.on_epoch_end(epoch=epoch, logs=logs)\n",
    "\n",
    "callbacks.on_train_end(logs=logs)\n",
    "print(\"Time taken: %.2fs\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whW74r9O0_1b",
    "outputId": "fe714263-75b0-4805-cbc2-e355227ea21b"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "for ax, m in zip(axes, [\"loss\", \"accuracy\"]):\n",
    "    for l in [\"train\", \"test\"]:\n",
    "        g = sns.lineplot(ax=ax, label=l, x=range(num_epochs), y=history[f\"{l}_{m}\"])\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set_xlabel(xlabel=\"epoch\")\n",
    "    ax.set_ylabel(ylabel=f\"{m}\")\n",
    "    ax.set_title(label=f\"{m} vs. epoch\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
