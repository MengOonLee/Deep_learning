{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SplitsAPI.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/Deep_learning/blob/master/TFDS/SplitsAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRh3kEO0Lm3r"
      },
      "source": [
        "# Exploring the Splits API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0xBoW5GLm3s"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll start by importing TensorFlow and TensorFlow Datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install --no-cache-dir -qU pip wheel\n",
        "pip install --no-cache-dir -qU numpy pandas matplotlib seaborn scikit-learn\n",
        "pip install --no-cache-dir -qU tensorflow-cpu tensorflow-datasets\n",
        "pip check"
      ],
      "metadata": {
        "id": "aPTwhYSpeTBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTBSvHcSLBzc",
        "outputId": "654dfa29-932d-4c4c-e7ff-433264d49411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "sns.set(font='DejaVu Sans')\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Using TensorFlow Version: 2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2F0l7EZMIsE"
      },
      "source": [
        "## Exploring the Splits API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWDJcqHmtpwM"
      },
      "source": [
        "train_ds, test_ds = tfds.load('mnist:3.*.*', split=['train', 'test'])\n",
        "\n",
        "print(len(list(train_ds)))\n",
        "print(len(list(test_ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx9IEFBmLm4A"
      },
      "source": [
        "With the slicing API we can use strings to specify the slicing instructions. For example, in the cell below we will merge the training and test sets by passing the string `’train+test'` to the `split` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cvBpzUCuVBT"
      },
      "source": [
        "combined = tfds.load('mnist:3.*.*', split='train+test')\n",
        "\n",
        "print(len(list(combined)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVrfSZXnLm4C"
      },
      "source": [
        "We can also use Python style list slicers to specify the data we want. For example, we can specify that we want to take the first 10,000 records of the `train` split with the string `'train[:10000]'`, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUUGRKh9uxkG"
      },
      "source": [
        "first10k = tfds.load('mnist:3.*.*', split='train[:10000]')\n",
        "\n",
        "print(len(list(first10k)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o__wKe-ILm4E"
      },
      "source": [
        "It also allows us to specify the percentage of the data we want to use. For example, we can select the first 20\\% of the training set with the string `'train[:20%]'`, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R41Psxi9vn4E"
      },
      "source": [
        "first20p = tfds.load('mnist:3.*.*', split='train[:20%]')\n",
        "\n",
        "print(len(list(first20p)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rwmWbT7Lm4G"
      },
      "source": [
        "We can see that `first20p` contains 12,000 records, which is indeed 20\\% the total number of records in the training set. Recall that the training set contains 60,000 records. \n",
        "\n",
        "Because the slices are string-based we can use loops, like the ones shown below, to slice up the dataset and make some pretty complex splits. For example, the loops below create 10 complimentary validation and training sets (each loop returns a list with 5 data sets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnUGAufZv7GL"
      },
      "source": [
        "val_ds = tfds.load('mnist:3.*.*', split=['train[{}%:{}%]'.format(k, k+20) for k in range(0, 100, 20)])\n",
        "\n",
        "train_ds = tfds.load('mnist:3.*.*', split=['train[:{}%]+train[{}%:]'.format(k, k+20) for k in range(0, 100, 20)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvZJPgOLm4I"
      },
      "source": [
        "val_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md1pnujNLm4K"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjWD4r3ILm4M"
      },
      "source": [
        "print(len(list(val_ds)))\n",
        "print(len(list(train_ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCDR_lZJLm4O"
      },
      "source": [
        "We can also compose new datasets by using pieces from different splits. For example, we can create a new dataset from the first 10\\% of the test set and the last 80\\% of the training set, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX0wsD9IwPYR"
      },
      "source": [
        "composed_ds = tfds.load('mnist:3.*.*', split='test[:10%]+train[-80%:]')\n",
        "\n",
        "print(len(list(composed_ds)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}