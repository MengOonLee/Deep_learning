{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKn-Y_Pk9WjC"
   },
   "source": [
    "# Instruction-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 132,
    "id": "YwB8OLqiflAl"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import jsonlines\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from llama import BasicModelRunner\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, \\\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load instruction tuned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\",\n",
    "    split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 98,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 5\n",
    "# print(\"Instruction-tuned dataset:\")\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
    "# for j in top_m:\n",
    "#     print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "prompt_template_with_input = \"\"\"\\\n",
    "Below is an instruction that describes a task, \\\n",
    "paired with an input that provides further context. \\\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\\\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"\n",
    "Below is an instruction that describes a task. \\\n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrate prompts (add data to prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for j in top_m:\n",
    "    if not j[\"input\"]:\n",
    "        processed_prompt = prompt_template_without_input.format(\n",
    "            instruction=j[\"instruction\"])\n",
    "    else:\n",
    "        processed_prompt = prompt_template_with_input.format(\n",
    "            instruction=j[\"instruction\"], input=j[\"input\"])\n",
    "\n",
    "    processed_data.append({\n",
    "        \"input\": processed_prompt, \"output\": j[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "print(processed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare non-instruction-tuned vs. instruction-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_path_hf = \"lamini/alpaca\"\n",
    "dataset_hf = load_dataset(dataset_path_hf)\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not instruction-tuned output (Llama 2 Base): .\n",
      "Tell me how to train my dog to sit. I have a 10 month old puppy and I want to train him to sit. I have tried the treat method and he just sits there and looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks\n"
     ]
    }
   ],
   "source": [
    "non_instruct_model = BasicModelRunner(\n",
    "    \"meta-llama/Llama-2-7b-hf\")\n",
    "non_instruct_output = non_instruct_model(\n",
    "    \"Tell me how to train my dog to sit\")\n",
    "print(\"Not instruction-tuned output (Llama 2 Base):\",\n",
    "    non_instruct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned output (Llama 2):  on command.\n",
      "Training a dog to sit on command is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit on command:\n",
      "\n",
      "1. Choose a quiet and distraction-free area: Find a quiet area with minimal distractions where your dog can focus on you.\n",
      "2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
      "3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\n",
      "4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying \"sit\" in a calm and clear voice.\n",
      "5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say \"good sit\" and give them the treat.\n",
      "6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command \"sit\" with\n"
     ]
    }
   ],
   "source": [
    "instruct_model = BasicModelRunner(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\")\n",
    "instruct_output = instruct_model(\n",
    "    \"Tell me how to train my dog to sit\")\n",
    "print(\"Instruction-tuned output (Llama 2):\", instruct_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned output (ChatGPT): Training a dog to sit is a basic command that can be taught using positive reinforcement techniques. Here's a step-by-step guide on how to train your dog to sit:\n",
      "\n",
      "1. Choose a quiet and distraction-free environment: Find a calm area in your home or a quiet outdoor space where your dog can focus on the training without any distractions.\n",
      "\n",
      "2. Gather treats: Use small, soft, and tasty treats that your dog loves. These treats will serve as rewards during the training process.\n",
      "\n",
      "3. Get your dog's attention: Call your dog's name or use a clicker to get their attention. Make sure they are looking at you before proceeding.\n",
      "\n",
      "4. Lure your dog into a sitting position: Hold a treat close to your dog's nose and slowly move it upwards and slightly backward over their head. As their nose follows the treat, their bottom will naturally lower into a sitting position. Once they are sitting, say \"sit\" in a clear and firm voice.\n",
      "\n",
      "5. Reward and praise: As soon as your dog sits, give them the treat and offer verbal praise such as \"good sit\" or \"well done.\" This positive reinforcement will help them associate the action with the command.\n",
      "\n",
      "6. Repeat the process: Practice the sit command multiple times in short training sessions\n"
     ]
    }
   ],
   "source": [
    "chatgpt = BasicModelRunner(\"chat-gpt\")\n",
    "instruct_output_chatgpt = chatgpt(\n",
    "    \"Tell me how to train my dog to sit\")\n",
    "print(\"Instruction-tuned output (ChatGPT):\",\n",
    "    instruct_output_chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try smaller models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"EleutherAI/pythia-70m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 453
   },
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(\n",
    "        generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0]\\\n",
    "        [len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Can Lamini generate technical documentation or user manuals for software projects?', 'answer': 'Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.', 'input_ids': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15]}\n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_sample = finetuning_dataset[\"test\"][0]\n",
    "print(test_sample)\n",
    "print(inference(test_sample[\"question\"], model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to finetuned small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "instruction_model = AutoModelForCausalLM\\\n",
    "    .from_pretrained(\"lamini/lamini_docs_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
     ]
    }
   ],
   "source": [
    "print(inference(test_sample[\"question\"], \n",
    "    instruction_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 232,
    "id": "mSJMi8I4Sgrw"
   },
   "outputs": [],
   "source": [
    "# Pssst! If you were curious how to upload your \n",
    "# own dataset to Huggingface. Here is how we did it\n",
    "\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "\n",
    "# import pandas as pd\n",
    "# import datasets\n",
    "# from datasets import Dataset\n",
    "\n",
    "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
    "# finetuning_dataset.push_to_hub(dataset_path_hf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
